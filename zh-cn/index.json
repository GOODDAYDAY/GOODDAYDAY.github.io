[{"categories":["Java","Spring"],"content":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:0:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"前言 在Spring应用中处理高并发场景时,合理使用异步编程和线程池管理至关重要。本文将通过实际代码示例,深入分析Spring的默认线程池、自定义线程池以及线程复用的机制。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:1:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"为什么要使用自定义线程池? Spring Boot应用启动时会自动配置一个全局的任务执行器(TaskExecutor),默认名称为applicationTaskExecutor。然而,在生产环境中不推荐直接使用Spring的默认线程池,主要原因如下: 缺乏隔离性: 所有异步任务共享同一个线程池,不同业务模块的任务会相互影响 难以监控: 无法针对特定业务场景进行细粒度的线程池监控和调优 配置单一: 默认配置可能无法满足所有业务场景的性能需求 最佳实践: 根据业务场景自定义线程池,实现任务隔离和精细化管理。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:2:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"自定义线程池配置 以下是一个典型的自定义线程池配置示例: private static final AtomicInteger COUNT = new AtomicInteger(0); private static final Executor EXECUTOR = new ThreadPoolExecutor( 10, // 核心线程数 10, // 最大线程数 10, // 空闲线程存活时间 TimeUnit.SECONDS, new ArrayBlockingQueue\u003c\u003e(10), // 工作队列容量 r -\u003e new Thread(r, String.format(\"customer-t-%s\", COUNT.addAndGet(1))) // 自定义线程命名 ); ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:3:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"配置解析: 核心线程数 = 最大线程数 = 10: 固定大小线程池,避免频繁创建销毁线程 队列容量 = 10: 当10个线程都在工作时,最多再排队10个任务 自定义线程名: customer-t-{序号},便于日志追踪和问题定位 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:3:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"异步接口 vs 同步接口对比 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:4:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"异步接口实现 (asyncQuery1) @GetMapping(\"async/query1\") public CompletionStage\u003cString\u003e asyncQuery1() { log.info(\"async query start\"); // Tomcat线程执行 return CompletableFuture.supplyAsync(() -\u003e { log.info(\"async query sleep start\"); // customer-t线程执行 ThreadUtils.sleep(10000); // 模拟耗时操作 log.info(\"async query sleep done\"); return \"done\"; }, EXECUTOR); } 特点: 非阻塞: Tomcat线程立即释放,可以处理其他请求 高吞吐: 适合I/O密集型任务 线程切换: 请求在Tomcat线程和自定义线程池之间切换 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:4:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"同步接口实现 (syncQuery1) @GetMapping(\"sync/query1\") public String syncQuery1() throws InterruptedException { log.info(\"sync query start\"); // Tomcat线程执行 final CountDownLatch latch = new CountDownLatch(1); EXECUTOR.execute(() -\u003e { log.info(\"sync query sleep start\"); // customer-t线程执行 ThreadUtils.sleep(1000); latch.countDown(); }); latch.await(); // Tomcat线程阻塞等待 log.info(\"sync query done\"); // Tomcat线程执行 return \"done\"; } 特点: 阻塞等待: Tomcat线程被CountDownLatch阻塞,不能处理其他请求 资源浪费: 同时占用Tomcat线程和Worker线程,两个线程干了一个线程的活 本质是同步: 虽然用了自定义线程池,但Tomcat线程一直等待,完全没有发挥异步优势 适用场景: 几乎没有!不如直接在Tomcat线程执行,还能省一个Worker线程 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:4:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"线程复用的实战表现 通过压测工具发送20个并发请求,观察异步和同步接口的线程行为差异。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:5:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"异步接口并发测试 发送20个并发请求到/goody/async/query1(每个任务耗时10秒): // ============ 阶段1: 前10个请求立即被Tomcat线程接收并提交 ============ 09:53:20.896 INFO [io-50012-exec-1] async query start ← Tomcat线程快速释放 09:53:20.899 INFO [customer-t-1] async query sleep start ← Worker线程1开始执行 09:53:21.026 INFO [io-50012-exec-1] async query start ← Tomcat线程又接收新请求 09:53:21.026 INFO [customer-t-2] async query sleep start ← Worker线程2开始执行 09:53:21.186 INFO [io-50012-exec-1] async query start 09:53:21.187 INFO [customer-t-3] async query sleep start ... 09:53:22.261 INFO [io-50012-exec-1] async query start 09:53:22.261 INFO [customer-t-10] async query sleep start ← 10个线程全部占满 // ============ 阶段2: 第11-20个请求进入队列等待 ============ 09:53:22.411 INFO [io-50012-exec-1] async query start ← 第11个请求,进入队列 09:53:22.597 INFO [io-50012-exec-1] async query start ← 第12个请求,进入队列 09:53:22.732 INFO [io-50012-exec-1] async query start ← ...持续到第20个 ... 09:53:24.048 INFO [io-50012-exec-1] async query start ← 第20个请求,队列满 // ============ 阶段3: 第21个请求触发拒绝策略 ============ 09:53:24.065 ERROR [io-50012-exec-1] RejectedExecutionException: ThreadPoolExecutor@79a3d00d[Running, pool size = 10, active threads = 10, queued tasks = 10] ↑ 线程池状态: 10个线程全忙 + 10个任务排队 = 容量已满 // ============ 阶段4: 线程复用开始 - 关键现象! ============ 09:53:30.313 INFO [customer-t-1] async query sleep done ← 线程1完成第1个任务 09:53:30.313 INFO [customer-t-1] async query done 09:53:30.314 INFO [customer-t-1] async query sleep start ← 线程1立即执行第11个任务(复用!) 09:53:31.041 INFO [customer-t-2] async query sleep done ← 线程2完成第2个任务 09:53:31.041 INFO [customer-t-2] async query sleep start ← 线程2立即执行第12个任务(复用!) 09:53:31.197 INFO [customer-t-3] async query sleep done 09:53:31.197 INFO [customer-t-3] async query sleep start ← 线程3复用 // ... 所有10个线程依次复用,处理队列中的第11-20个任务 // ============ 阶段5: 第二轮任务全部完成 ============ 09:53:40.320 INFO [customer-t-1] async query sleep done ← 线程1完成第11个任务 09:53:41.048 INFO [customer-t-2] async query sleep done ← 线程2完成第12个任务 ... 核心观察点: 并发能力强: Tomcat线程(io-50012-exec-1)在2秒内接收了20个请求,平均100ms处理一个 线程固定: 始终只有customer-t-1到customer-t-10这10个Worker线程 线程复用: customer-t-1在09:53:30完成第1个任务后,立即执行第11个任务(间隔仅1ms) 拒绝策略: 超过容量(10线程+10队列)时,第21个请求被拒绝 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:5:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"同步接口串行执行 发送20个并发请求到/goody/sync/query1(每个任务耗时1秒): // ============ 串行处理: Tomcat线程被阻塞 ============ 09:54:02.401 INFO [io-50012-exec-1] sync query start ← Tomcat线程处理第1个请求 09:54:02.401 INFO [customer-t-1] sync query sleep start ← Worker线程执行 09:54:03.407 INFO [customer-t-1] sync query sleep done ← 1秒后完成 09:54:03.407 INFO [io-50012-exec-1] sync query done ← Tomcat线程才返回 09:54:03.409 INFO [io-50012-exec-1] sync query start ← 处理第2个请求 09:54:03.409 INFO [customer-t-2] sync query sleep start 09:54:04.416 INFO [customer-t-2] sync query sleep done 09:54:04.416 INFO [io-50012-exec-1] sync query done 09:54:04.418 INFO [io-50012-exec-1] sync query start ← 处理第3个请求 09:54:04.418 INFO [customer-t-3] sync query sleep start ... // ============ 线程复用也存在 ============ 09:54:12.490 INFO [io-50012-exec-1] sync query start ← 第11个请求 09:54:12.490 INFO [customer-t-1] sync query sleep start ← 线程1被复用 09:54:13.500 INFO [customer-t-1] sync query sleep done 09:54:13.500 INFO [io-50012-exec-1] sync query done 对比分析: 维度 异步接口 同步接口 Tomcat线程 快速释放,2秒接收20个请求 被阻塞,20秒才处理完20个请求 并发能力 可同时处理20个(10线程+10队列) 只能串行处理,1个接1个 Worker线程复用 ✅ 存在(customer-t-1处理第1和第11个任务) ✅ 存在(customer-t-1处理第1和第11个任务) 总耗时 ~20秒(10秒×2轮) ~20秒(1秒×20个) 线程利用率 高(Tomcat空闲,Worker忙) 低(Tomcat+Worker同时占用,干一份活) 系统吞吐 高(Tomcat线程可处理其他请求) 低(Tomcat线程被占用) 异步本质 ✅ 真正异步,释放主线程 ❌ 假异步,本质是同步等待(两线程干一份活,还更慢) 关键结论: 同步接口虽然也展示了Worker线程复用,但本质上没有利用异步优势。它反而带来了额外开销: Tomcat线程阻塞 → 无法处理其他请求 Worker线程执行 → 占用线程池资源 两个线程配合完成一个任务,不如直接在Tomcat线程执行,还能省掉线程切换开销 这种写法在生产环境中是反模式,仅用于对比演示异步的优势。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:5:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"线程复用的核心机制 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:6:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"生产者-消费者模型 Java线程池的线程复用基于生产者-消费者模型: 工作线程循环: 线程池中的Worker线程不断从BlockingQueue中获取任务 任务队列: 新任务提交到队列,空闲线程立即取出执行 复用优势: 避免频繁创建销毁线程的开销(上下文切换、内存分配) ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:6:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"与IO多路复用的相似之处 核心: 异步线程池本质上是应用层的\"多路复用\"思想,虽然实现机制不同,但解决问题的思路与IO多路复用高度相似。 相似之处 核心思想: 用少量资源处理大量请求 IO多路复用: 1个线程通过epoll/select监听N个socket连接 异步线程池: 少量Tomcat线程处理N个并发请求(通过快速释放) 非阻塞模式 IO多路复用: 主线程不阻塞在单个IO操作上,轮询等待多个IO事件就绪 异步线程池: Tomcat线程不阻塞在耗时任务上,立即返回处理下个请求 事件通知机制 IO多路复用: epoll通知哪个socket可读/可写 异步线程池: CompletableFuture通知任务完成 本质区别 维度 IO多路复用 异步线程池 复用对象 复用线程(单线程处理多IO) 复用Tomcat线程(快速释放) 适用场景 网络IO密集型 CPU/IO混合型 实现层次 操作系统层(epoll/select) 应用层(线程池调度) 典型应用 Netty, Redis, Nginx Spring WebFlux, 传统Web 设计模式 Reactor模式 生产者-消费者模式 类比理解 IO多路复用: ┌─────────────┐ │ Event Loop │ ──监听──\u003e [Socket1, Socket2, ..., SocketN] │ (1 thread) │ 哪个就绪处理哪个 └─────────────┘ 异步线程池: ┌─────────────┐ │ Tomcat线程池│ ──快速释放──\u003e [Request1, Request2, ..., RequestN] │ (200线程) │ 交给Worker池异步处理 └─────────────┘ 结论: 虽然底层机制不同,但都在解决\"如何用有限资源应对高并发\"的核心问题。异步线程池可以理解为**应用层实现的多路复用思想 **。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:6:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"总结 本文通过对比异步和同步两种接口实现,揭示了自定义线程池的重要性和线程复用的机制。关键要点: ✅ 自定义线程池实现业务隔离和精细化管理 ✅ 异步接口提升系统吞吐量,释放Tomcat线程 ✅ 线程复用避免频繁创建销毁线程的开销 ✅ 合理配置线程池参数,避免资源浪费或任务拒绝 ✅ 通过日志中的线程名可以清晰观察到线程复用过程 在实际生产环境中,还需要结合监控指标(线程池活跃度、队列长度、拒绝次数等)持续优化线程池配置。 ","date":"2025-10-02","objectID":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/:7:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. 浅谈Spring异步接口中的自定义线程池与线程复用分析","uri":"/zh-cn/2025/10/14.-spring-async-threadpool-and-thread-reuse/"},{"categories":["MySQL"],"content":"[MySQL] 3. MySQL 索引 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:0:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"什么是索引？ 索引是一种数据结构，通过额外的写入和存储空间来维护索引数据结构，从而提高数据库表上数据检索操作的速度。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:1:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"索引类型 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:2:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"按数据结构分类 哈希索引 哈希索引基于哈希表数据结构。它使用哈希函数将键映射到哈希表中的特定位置，允许非常快速的数据检索。 算法复杂度：O(1) 优点： 对等值查询（如 =）非常快。 缺点： 不适合范围查询（如 \u003c、\u003e、BETWEEN）。 可能发生哈希冲突，导致性能下降。 为什么不支持 order by 让我们看看哈希索引的算法 我们看到哈希索引是键值结构。因此值不是有序的，所以它不能支持 order by。 BTree(B-Tree) 索引 为什么不用 AVL 或红黑树 对于树索引，我们可以考虑平衡搜索树，如 AVL 树或红黑树 算法复杂度：O(log n) 显然，它们是二叉平衡搜索树，不适合数据库索引，因为： 树高度高：二叉树可能变得很高，导致搜索时间增加。 频繁重新平衡：插入和删除操作经常需要树旋转来维护平衡，这在性能方面代价很高。 磁盘 I/O 性能差：二叉树不利用空间局部性，导致低效的磁盘访问模式。 磁盘 I/O 的成本远高于内存访问。 为什么选择 BTree 因此，为了减少树高度和磁盘 I/O，BTree 自然而然地出现了。 首先，使用多路树来减少树高度。 其次，每个节点包含整个磁盘页数据以减少磁盘 I/O。 但我们仍然有一些问题： 当我们插入或删除数据时，树可能变得不平衡。 很难按顺序搜索，我们需要使用中序遍历，成本很高，才能获得排序数据。 所以，B+Tree 出现了。 B+Tree 索引 首先，B+Tree 通过只在内部节点中存储键来减少树节点大小，这允许更多键适合内存并减少树高度。 没有值意味着一个节点中有更多键 =\u003e 更少的树高度 =\u003e 更少的磁盘 I/O 其次，B+Tree 将所有实际数据存储在叶节点中，这些节点在链表中链接在一起，使范围查询更高效。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:2:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"按用途分类 主键索引 主键索引是为主键列自动创建的。如果没有主键，MySQL 会创建一个。 确保唯一性和快速访问记录，因为只有一个操作就能获得整个记录。 主键是 B+Tree 复合索引 我们知道 MySQL 支持复合索引，即多个列上的索引。 让我们看看单个叶页。 叶页首先按第一列排序，然后按第二列排序。 {列 A，列 B}：(1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2) 所以，让我们思考经典问题：（最左前缀规则） 如果我们在 (A, B) 上有复合索引，我们能将其用于查询 where B = 1 吗？ 不能，因为叶页首先按 A 排序，然后按 B 排序。所以我们不能快速找到所有 B = 1。 如果我们在 (A, B) 上有复合索引，我们能将其用于查询 where A = 1 AND B \u003e 1 吗？ 可以，因为我们可以快速找到 A = 1，然后我们得到 (1, 1), (1, 2)，这意味着 B 是有序的，所以我们可以快速找到 B \u003e 1。 如果我们在 (A, B) 上有复合索引，我们能将其用于查询 where A \u003e 1 AND B \u003e 1 吗？ 可以，但只有 A \u003e 1 被使用。因为我们将得到 (2, 1), (2, 2), (3, 1), (3, 2)，这意味着 B 不是有序的。 二级索引（非聚集索引） 二级索引也称为非聚集索引，与聚集索引（主键索引）不同。 关键区别：二级索引叶节点只存储索引列 + 主键值，不存储完整的行数据。 索引结构：B+Tree 结构，但叶节点包含对主键的引用而不是完整行数据。 覆盖索引 vs 回表 覆盖索引：当查询字段都包含在二级索引中时，无需回表。 -- 索引：idx_AB (A, B) SELECT A, B FROM table WHERE A = 1; -- ✅ 覆盖索引，无需回表 回表：当查询需要不在二级索引中的字段时，必须查找主键。 -- 索引：idx_AB (A, B)，但需要字段 C SELECT A, B, C FROM table WHERE A = 1; -- ❌ 需要为字段 C 回表 回表过程 搜索二级索引：找到匹配记录，获取主键值 查找主索引：使用主键从聚集索引获取完整行数据 返回结果：合并来自两个索引的数据 这就是为什么 SELECT * 经常需要回表，而只选择索引列可以避免回表。 全文索引 全文索引设计用于解决在大文本中查找单词的问题，使用倒排索引结构。 与 B+Tree 索引的关键区别 特性 B+Tree 索引 全文索引 数据结构 平衡树 倒排索引（单词 → 文档） 存储 叶子中的完整行数据 单词到文档的映射 查询类型 精确匹配，范围查询 文本搜索，相关性排名 复杂度 O(log n) 取决于单词频率 倒排索引机制 分词：将文本分解为单个单词 单词映射：创建单词 → 文档列表映射 搜索过程：查找包含查询单词的文档 交集：为多词查询合并结果 支持的数据类型 CHAR VARCHAR TEXT 创建表语法 -- 单列全文索引 CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), content TEXT, FULLTEXT KEY ft_content (content) ) ENGINE=InnoDB; -- 多列全文索引 CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), content TEXT, FULLTEXT KEY ft_title_content (title, content) ) ENGINE=InnoDB; 搜索模式 自然语言模式（默认） SELECT * FROM articles WHERE MATCH(title, content) AGAINST('MySQL 优化'); -- 带相关性评分 SELECT *, MATCH(title, content) AGAINST('MySQL 优化') as score FROM articles WHERE MATCH(title, content) AGAINST('MySQL 优化') ORDER BY score DESC; 布尔模式 -- 必须包含\"MySQL\"，不能包含\"Oracle\" SELECT * FROM articles WHERE MATCH(title, content) AGAINST('+MySQL -Oracle' IN BOOLEAN MODE); -- 精确短语搜索 SELECT * FROM articles WHERE MATCH(title, content) AGAINST('\"索引优化\"' IN BOOLEAN MODE); -- 通配符搜索 SELECT * FROM articles WHERE MATCH(title, content) AGAINST('优化*' IN BOOLEAN MODE); ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:2:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"高级索引优化概念 让我们分析各种 SQL 查询，以了解覆盖索引、索引条件下推（ICP）和回表行为。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:3:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"表结构和索引 CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, A BIGINT NOT NULL, B BIGINT NOT NULL, C BIGINT NOT NULL, D BIGINT NOT NULL ) ENGINE=InnoDB; -- 在列 A, B, C 上的复合索引 CREATE INDEX idx_articles_query ON articles (A, B, C); 索引结构：idx_articles_query (A, B, C)（二级索引）+ 主键 id（自动包含在二级索引中） ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:3:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"覆盖索引 覆盖索引：当所有查询列都包含在索引中时，消除回表需求。 所以，如果查询需要回表，它就不能是覆盖索引 非覆盖索引：为其他列返回而回表 SELECT * FROM articles WHERE A = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; 非覆盖索引：为其他列作为条件而回表 SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; 覆盖索引成功 SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:3:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"索引条件下推 ICP：将可以使用索引的 WHERE 条件下推到存储引擎层，减少回表。 非 ICP：只有一个条件 SELECT * FROM articles WHERE A = 1; ICP 成功 即使跳过一个索引列 SELECT * FROM articles WHERE A = 1 AND C \u003e 1; 即使有其他索引列 SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; 正常情况 SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:3:3","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"回表 二级索引只保存 id，如果有其他列，就会发生回表。 回表是我们需要避免的操作。因为它代价更高。 回表：返回更多列 SELECT * FROM articles WHERE A = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; 回表：条件包含更多列 SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; 不回表（好） SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; id 包含在二级索引叶节点中 SELECT id, A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:3:4","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"索引使用经验 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:4:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"何时使用哈希索引 只搜索一个图片。 列内容几乎随机或不相关，如 UUID。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:4:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"过滤率是索引使用的关键点 如果可重用，索引应该更少。 索引设计的要点是更高的过滤率。 经典问题：(id, begin, end) vs (id, end, begin) 优惠券表 user_id begin end ~ ~ ~ 7 1234567890000 1234567890100 7 1234567891000 1234567891100 7 1234567892000 1234567892100 7 1234567893000 1234567893100 7 1234567894000 1234567894100 7 1234567895000 1234567895100 7 1234567896000 1234567896100 7 1234567897000 1234567897100 7 1234567898000 1234567898100 ~ ~ ~ 查找活跃优惠券 让我们分析如果我们搜索下面的 SQL。 SELECT * FROM table WHERE id = 7 AND begin \u003c 1234567894000 AND end \u003e 1234567894000 索引条件下推可以用于此查询。 只有前两列索引可以使用，即 (id, begin, end) 中的 (id, begin) 或 (id, end, begin) 中的 (id, end)。 但是，对于 (id, begin) 部分，begin \u003e 1234567894000 总是过滤更少的数据，时间总是现在。 因为总有数据时间比现在小。 对于 (id, end) 部分，end \u003e 1234567894000 总是能过滤更多的数据。 所以更好的是 (id, end, begin) 经典问题：类型索引 假设有 5 种类型 几乎平均，如计算机类型 INDEX idx_computer_type(type) 在这种情况下，当我们使用 type = 1 时，只有大约 20% 的过滤率。 不平均，如支付状态 INDEX idx_payment_status(status) 在这种情况下，几乎所有支付都将是 status = final。 但是我们总是查询 status != final，它只占很少的数据，少于 1%（接近超过 99% 的过滤率）。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:4:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"数据大小 VS 索引大小 人们总是认为如果数据只是一点点，就不需要创建索引。 当数据不大时，有很多索引也没关系。 因为索引不会使用太多存储。 当数据很大时，有很多索引也没关系。 因为索引对我们快速查询很重要。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:4:3","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"不要频繁修改索引列 我们知道索引是平衡和有序的。如果我们修改索引列值，B+Tree 需要重新平衡自己，这代价很高。 ","date":"2025-09-28","objectID":"/zh-cn/2025/09/13.-mysql-index/:4:4","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL 索引","uri":"/zh-cn/2025/09/13.-mysql-index/"},{"categories":["MySQL"],"content":"[MySQL] 2. 锁机制执行分析 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:0:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"引言 在高并发环境下，数据库锁机制是确保数据一致性和完整性的关键手段。MySQL作为广泛使用的关系型数据库，提供了多种锁类型和机制来管理并发访问。然而，锁的使用不当可能导致性能瓶颈、死锁等问题，影响系统的稳定性和响应速度。 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:1:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"锁相关基本概念 锁的定义：锁是一种机制，用于控制对共享资源的访问，防止多个事务同时修改同一数据，确保数据的一致性和完整性。 锁的类型： 表级锁：锁定整个表。 共享锁（S锁）：允许多个事务同时读取数据，但不允许修改。 排他锁（X锁）：允许一个事务修改数据，其他事务既不能读取也不能修改。 意向锁（IS锁和IX锁）：用于表级别，表示事务打算在行级别上加锁。 自增锁（AUTO-INC锁）：用于处理自增列的并发插入，防止冲突。 间隙锁（Gap Lock）：锁定索引记录之间的间隙，防止幻读。 临键锁（Next-Key Lock）：结合了记录锁和间隙锁，锁定索引记录及其前面的间隙。 记录锁（Record Lock）：锁定具体的索引记录。 行级锁：锁定具体的行。 乐观锁：通过版本号或时间戳实现，适用于读多写少的场景。 悲观锁：通过显式加锁实现，适用于写多读少的场景。 死锁：多个事务互相等待对方释放锁，导致无法继续执行。 锁的兼容性：不同类型的锁之间存在兼容性规则，决定了哪些锁可以同时存在。 锁的粒度：锁定资源的范围，粒度越细，系统并发性越高，但管理开销也越大。 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:2:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"MySQL锁介绍 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"基本命令 创建测试表 -- auto-generated definition create table example_single_pk ( id bigint not null comment 'id' primary key, created timestamp default CURRENT_TIMESTAMP not null comment 'create time', updated timestamp default CURRENT_TIMESTAMP not null on update CURRENT_TIMESTAMP comment 'update time' ) comment 'example_single_pk' charset = utf8mb4; 执行命令 SELECT id, created, updated FROM example_single_pk; INSERT INTO example_single_pk (id) VALUES (1); SELECT id, created, updated FROM example_single_pk; UPDATE example_single_pk SET id = 6 WHERE id = 1; SELECT id, created, updated FROM example_single_pk; DELETE FROM example_single_pk WHERE id = 1 or id = 6; SELECT id, created, updated FROM example_single_pk; 结果 mysql\u003e SELECT id, created, updated FROM example_single_pk; Empty set (0.00 sec) mysql\u003e mysql\u003e INSERT INTO example_single_pk (id) VALUES (1); Query OK, 1 row affected (0.00 sec) mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; +----+---------------------+---------------------+ | id | created | updated | +----+---------------------+---------------------+ | 1 | 2025-09-27 11:14:40 | 2025-09-27 11:14:40 | +----+---------------------+---------------------+ 1 row in set (0.00 sec) mysql\u003e mysql\u003e UPDATE example_single_pk SET id = 6 WHERE id = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; +----+---------------------+---------------------+ | id | created | updated | +----+---------------------+---------------------+ | 6 | 2025-09-27 11:14:40 | 2025-09-27 11:14:40 | +----+---------------------+---------------------+ 1 row in set (0.00 sec) mysql\u003e mysql\u003e DELETE FROM example_single_pk WHERE id = 1 or id = 6; Query OK, 1 row affected (0.00 sec) mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; Empty set (0.00 sec) ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:1","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"按粒度分 表级锁 - READ 加锁 mysql\u003e LOCK TABLES example_single_pk READ; Query OK, 0 rows affected (0.00 sec) 此时，其他会话可以读取数据，但不能进行写操作。 解锁 mysql\u003e UNLOCK tables; Query OK, 0 rows affected (0.00 sec) 可以看出此时，其他会话可以正常读取但不能写入数据。 表级锁 - WRITE 加锁 mysql\u003e LOCK TABLES example_single_pk WRITE; query OK, 0 rows affected (0.00 sec) 解锁 mysql\u003e UNLOCK tables; Query OK, 0 rows affected (0.00 sec) 可以看出此时，其他会话既不能读取也不能写入数据。 总体来看，表级别锁的粒度较大，适用于对整个表进行批量操作的场景，但会影响并发性能。 不推荐使用。 行级锁 - SELECT … FOR SHARE 加锁 mysql\u003e start transaction; Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM example_single_pk WHERE id = 1 FOR SHARE; Empty set (0.00 sec) 此时影响的是另外会话内的锁。 其他会话可以读取数据，但不能进行写操作，不在赘述并截图。 解锁 mysql\u003e COMMIT; Query OK, 0 rows affected (0.00 sec) 解锁后可以正常加读锁和写锁。 行级锁 - SELECT … FOR UPDATE 加锁 mysql\u003e start transaction; Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM example_single_pk WHERE id = 1 FOR UPDATE; Empty set (0.00 sec) 其实此时从查询数据的角度，与 FOR SHARE没有区别，影响的是另外会话内的锁。 FOR UPDATE会锁定所选行，其他事务不能读取或修改这些行。 解锁 mysql\u003e COMMIT; Query OK, 0 rows affected (0.00 sec) 解锁后可以正常加读锁和写锁。 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:2","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"按属性分 共享锁 \u0026 排他锁 序号 锁名称 触发方式 锁类型 作用域 核心特点 1 表级共享锁（S锁） LOCK TABLES tbl_name READ 服务器层锁 整表 允许其他会话读，阻止写；需手动UNLOCK TABLES解锁。 2 表级排他锁（X锁） LOCK TABLES tbl_name WRITE 服务器层锁 整表 阻止其他会话读写；需手动UNLOCK TABLES解锁。 3 行级共享锁（S锁） SELECT … FOR SHARE（事务内） InnoDB行锁 单行 允许其他会话读该行，阻止写；事务提交/回滚后自动解锁。 4 行级排他锁（X锁） SELECT … FOR UPDATE（事务内） InnoDB行锁 单行 阻止其他会话读写该行；事务提交/回滚后自动解锁。 自增锁 (AUTO-INC Lock) 触发方式：INSERT语句操作AUTO_INCREMENT列时自动触发 特点：AUTO-INC锁，保证自增值的连续性和唯一性。与行锁不同，但是行为结果接近。 可以看到，此时自增锁之间没有冲突，两个会话都成功插入数据。 而很明显，当确定了id之后，则与行锁行为接近，第二个会话插入失败。 提交之后，可以看到数据插入失败，因为id冲突。 意向锁 (Intention Lock) 意向锁是 InnoDB 存储引擎为协调「服务器层（MySQL 主进程）表锁」与「存储引擎层行锁」的冲突，设计的服务器级表级锁。它的核心是传递“存储引擎中有行锁存在”的信号，让服务器层能快速判断表锁与行锁的兼容性，避免高成本冲突检测。以下从层级定位、设计目标、联动机制、轻量原因展开说明： 意向锁是服务器层的“行锁信号灯”——用表级锁的极小开销，传递存储引擎的行锁状态，让两层锁机制高效协同，既正确又高性能。 层级定位：服务器层的“行锁信号员” • 服务器层：MySQL 主进程负责管理服务器级锁（如 LOCK TABLES），这是独立于存储引擎的原生锁机制。 • 存储引擎层：InnoDB 负责管理行级锁（如 SELECT … FOR UPDATE 的 X 锁），控制具体数据行的访问。 • 意向锁的角色：意向锁属于服务器层的表级锁，但它不直接控制数据行——而是作为“翻译官”，将存储引擎中的行锁状态（“有事务在操作某行”）转化为服务器层能理解的“信号”（表级的 IS/IX 锁）。 核心设计目标：解决分层架构的“信息差” MySQL 的分层架构（服务器层 vs 存储引擎层）导致锁状态天然隔离： • 服务器层想加表锁时，无法直接感知存储引擎里是否有行锁（比如事务 A 已锁定某行）； • 存储引擎的行锁，也不关心服务器层是否有表锁。 若没有意向锁，服务器层加表锁时，必须遍历存储引擎的所有行（O(n) 时间复杂度），检查是否有行锁冲突——这对大表来说是灾难性的性能损耗。 意向锁的出现，将这种“遍历检查”优化为O(1) 的信号判断： • 存储引擎加行锁时，自动向服务器层注册对应的意向锁（IS=行读意图，IX=行写意图）； • 服务器层加表锁时，只需检查表级的意向锁状态，就能立刻判断是否冲突。 与行锁/表锁的联动机制 意向锁的生命周期完全依附于行锁，是行锁的“影子”： • 行锁触发意向锁：当事务对某行加行锁（S/X）时，InnoDB 引擎会自动通知服务器层，在表级加对应的意向锁（IS 或 IX）。 • 例：SELECT * FROM t WHERE id=1 FOR UPDATE（加行 X 锁）→ 服务器层加表级 IX 锁（意向排他锁）。 • 表锁检查意向锁：当事务尝试加服务器层表锁时，服务器层会检查表级的意向锁： • 若表级有 IX 锁（意向排他锁），说明存储引擎中有行锁在活动，表锁（如 LOCK TABLES … WRITE）会被阻塞； • 若表级有 IS 锁（意向共享锁），说明存储引擎中有行读锁，表读锁（LOCK TABLES … READ）可兼容，表写锁仍被阻塞。 为什么意向锁“轻量”？ 意向锁的“轻量”源于其极小的状态空间和自动同步机制： • 锁范围有限：仅锁定“整张表”的概念（表级），不涉及具体数据行，无需维护每行的锁状态（如行锁的 heap_no 或 trx_id）。 • 状态极简：仅需记录两种“意图”——IS（有事务要读行）、IX（有事务要写行），逻辑复杂度远低于行锁。 • 自动同步：由 InnoDB 引擎在加/解锁行锁时自动触发，无需手动管理，无额外的人工或系统开销。 价值总结：用最小开销换最大正确性 意向锁的本质是用表级锁的“轻量状态”，连接服务器层与存储引擎层的锁机制： • 对服务器层：快速判断表锁是否与行锁冲突，避免遍历所有行的高成本； • 对存储引擎：无需关心服务器层的表锁，专注管理行锁； • 对整体并发：既保证了数据一致性（避免表锁与行锁冲突），又维持了高并发性能。 ","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:3","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"按算法分 (InnoDB引擎) 1 记录锁 (Record Lock) 2 间隙锁 (Gap Lock) 3 临键锁 (Next-key Lock) 实操说明 操作类型 常见场景 锁类型 锁范围 隔离级别依赖 主要冲突对象 备注 SELECT 普通查询（无 FOR UPDATE/SHARE） 无锁 无 无 无 读不加锁（快照读） SELECT FOR UPDATE 等值查询（记录存在） 记录锁（LOCK_REC_NOT_GAP） 具体记录（如 id=3） RR/RC 同记录的记录锁、临键锁 锁住目标行，阻止修改/删除 SELECT FOR UPDATE 等值查询（记录不存在，RR） 间隙锁（LOCK_GAP） 相邻记录的间隙（如 (1,5)） RR 同间隙的间隙锁、插入意向锁 防其他事务插入缺失的记录（幻读） SELECT FOR UPDATE 等值查询（记录不存在，RC） 无锁 无 RC 无 RC 无间隙锁，仅读不加锁 SELECT FOR UPDATE 范围查询（如 id\u003e2，RR） 临键锁（LOCK_NEXT_KEY） 记录+前驱间隙（如 (3,3]、(3,5]） RR 同记录的记录锁、间隙锁 锁住范围所有记录及间隙（防幻读） SELECT FOR UPDATE 范围查询（如 id\u003e2，RC） 记录锁 符合条件的记录（如 id=3,5） RC 同记录的记录锁 RC 无间隙锁，仅锁存在的记录 INSERT 插入新记录（任意场景） 插入意向锁（LOCK_INSERT_INTENTION） 相邻记录的间隙（如 (1,5)） 无（始终加） 同间隙的普通间隙锁、记录锁 协调插入互斥，与普通锁冲突 DELETE 等值删除（记录存在） 记录锁（LOCK_REC_NOT_GAP） 具体记录（如 id=3） RR/RC 同记录的记录锁、临键锁 锁住目标行，阻止修改/插入 DELETE 范围删除（如 id\u003e2，RR） 临键锁（LOCK_NEXT_KEY） 记录+前驱间隙（如 (3,3]、(3,5]） RR 同记录的记录锁、间隙锁 锁住范围所有记录及间隙（防幻读） DELETE 范围删除（如 id\u003e2，RC） 记录锁 符合条件的记录（如 id=3,5） RC 同记录的记录锁 RC 无间隙锁，仅锁存在的记录 UPDATE 等值更新（记录存在） 记录锁（LOCK_REC_NOT_GAP） 具体记录（如 id=3） RR/RC 同记录的记录锁、临键锁 锁住目标行，阻止修改/插入 UPDATE 范围更新（如 id\u003e2，RR） 临键锁（LOCK_NEXT_KEY） 记录+前驱间隙（如 (3,3]、(3,5]） RR 同记录的记录锁、间隙锁 锁住范围所有记录及间隙（防幻读） UPDATE 范围更新（如 id\u003e2，RC） 记录锁 符合条件的记录（如 id=3,5） RC 同记录的记录锁 RC 无间隙锁，仅锁存在的记录 INSERT 插入使用的是插入意向锁 insert into -1 Time SESSION 1 (LEFT) SESSION 2 (RIGHT) 0.0 SESSION 2 START - Waiting for Session1… 0.3 SESSION 1 START - INSERT === Round 0: Testing SELECT \u0026 INSERT === ~ mysql\u003e SELECT * FROM example_single_pk ~ (1, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ (4, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ (5, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ 3 rows (0.03s) ~ === Round -1: Testing INSERT ID=-1 === ~ mysql\u003e start transaction ~ Query OK, 0 rows affected ~ mysql\u003e INSERT INTO example_single_pk (id) VALUES (-1) ~ Query OK, 1 row affected (0.03s) Session 1 插入 ID=-1 的记录，并保持事务未提交，持有插入意向锁。 Time SESSION 1 (LEFT) SESSION 2 (RIGHT) 0.7 mysql\u003e SELECT * FROM example_single_pk WHERE id = -1 FOR UPDATE 3.7 ERROR: SELECT timeout (3.04s) (3.04s) 4.1 mysql\u003e INSERT INTO example_single_pk (id) VALUES (-1) 7.1 ERROR: INSERT timeout (3.03s) (3.03s) Session 2 查询 ID=-1 的记录，发现记录存在，但被 Session 1 持有插入意向锁，导致查询阻塞等待锁释放，最终超时失败。 Session 2 尝试插入 ID=-1 的记录，同样因为冲突而超时失败。 Time SESSION 1 (LEFT) SESSION 2 (RIGHT) 7.4 mysql\u003e SELECT * FROM example_single_pk WHERE id = 0 FOR UPDATE 7.5 SELECT returned 0 rows (0.03s) 7.7 mysql\u003e INSERT INTO example_single_pk (id) VALUES (0) ~ INSERT success (0.02s) 8.0 mysql\u003e SELECT * FROM example_single_pk WHERE id = 1 FOR UPDATE 8.1 SELECT returned 1 rows (0.03s) ~ (1, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) 8.4 mysql\u003e INSERT INTO example_single_pk (id) VALUES (1) ~ ERROR: INSERT duplicate (0.03s) (0.03s) 8.7 mysql\u003e SELECT * FROM example_single_pk WHERE id = 2 FOR UPDATE ~ SELECT returned 0 rows (0.03s) 9.1 mysql\u003e INSERT INTO example_single_pk (id) VALUES (2) ~ INSERT success (0.03s) 9.4 mysql\u003e SELECT * FROM example_single_pk WHERE id = 3 FOR UPDATE ~ SELECT returned 0 rows (0.03s) 9.7 mysql\u003e INSERT INTO example_single_pk (id) VALUES (3) ~ INSERT success (0.03s) 10.1 mysql\u003e SELECT * FROM example_single_pk WHERE id = 4 FOR UPDATE ~ SELECT returned 1 rows (0.03s) ~ (4, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) 10.4 mysql\u003e INSERT INTO example_single_pk (id) VALUES (4) ~ ERROR: INSERT duplicate (0.03s) (0.03s) 10.8 mysql\u003e SELECT * FROM example_single_pk WHERE id = 5 FOR UPDATE ~ SELECT returned 1 rows (0.02s) ~ (5, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) 11.1 mysql\u003e INSERT INTO example_single_pk (id) VALUES (5) ~ ERROR: INSERT duplicate (0.03s) (0.03s) 11.5 mysql\u003e SELECT * FROM example_single_pk WHERE id = 6 FOR UPDATE ~ SELECT returned 0 rows (0.03s) 11.9 mysql\u003e INSERT INTO example_single_pk (id) VALUES (6) ~ INSERT success (0.06s) 12.3 mysql\u003e SELECT * FROM example_single_pk WHERE id = 7 FOR UPDATE ~ SELECT returned 0 rows (0.04s) 12.6 mysql\u003e INSERT INTO example_single_pk (id) VALUES (7) ~ INSERT success (0.03s) 12.7 mysql\u003e rollback ~ Query OK, 0 rows affected 后续无问题 insert into 0 Time SESSIO","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:4","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["MySQL"],"content":"附录 python脚本，用于数据获取 #!/usr/bin/env python3 \"\"\" MySQL Lock Testing - Timeline Analysis Version \"\"\" import pymysql import threading import time from datetime import datetime from collections import defaultdict DB_CONFIG = { 'host': 'localhost', 'user': 'haotian', 'password': 'qwe123qwe123', 'database': 'toy', 'autocommit': False } # Test Configuration - Modify as needed TEST_CONFIG = { # Session1 Configuration 'session1_operation': 'select_for_update_range', # 'insert', 'select_for_update', 'select_for_update_range' 'insert_start': -1, # Operation start ID 'insert_end': 7, # Operation end ID (exclusive) 'range_size': 2, # Range size (for select_for_update_range only) # Session2 Test Configuration 'test_ids': [-1, 0, 1, 2, 3, 4, 5, 6, 7], # ID list to test 'lock_`timeout`': 3 # Lock wait `timeout` in seconds } # Global log collector timeline_log = [] log_lock = threading.Lock() def log_event(session, event_type, sql, result=None, error=None, duration=None): \"\"\"Record event to timeline\"\"\" with log_lock: timestamp = datetime.now() timeline_log.append({ 'timestamp': timestamp, 'session': session, 'type': event_type, # 'sql', 'info', 'error' 'sql': sql, 'result': result, 'error': error, 'duration': duration }) # Real-time progress display time_str = timestamp.strftime(\"%H:%M:%S.%f\")[:-3] if event_type == 'sql': progress_text = f\"mysql\u003e {sql}\" if duration: progress_text += f\" ({duration:.2f}s)\" elif event_type == 'error': progress_text = f\"ERROR: {error}\" if duration: progress_text += f\" ({duration:.2f}s)\" else: progress_text = result or sql print(f\"[{time_str}] [{session}] {progress_text}\") def session1(): \"\"\"Session1: Loop operations (INSERT or SELECT FOR UPDATE)\"\"\" conn = pymysql.connect(**DB_CONFIG) cursor = conn.cursor() operation_type = TEST_CONFIG['session1_operation'] log_event('S1', 'info', '', f'SESSION 1 START - {operation_type.upper()}') # Check table status start = time.time() cursor.execute(\"SELECT * FROM example_single_pk\") results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', 'SELECT * FROM example_single_pk', results, duration=duration) # Loop operations try: for target_id in range(TEST_CONFIG['insert_start'], TEST_CONFIG['insert_end']): log_event('S1', 'info', '', f'=== Round {target_id}: Testing {operation_type.upper()} ID={target_id} ===') try: # Start transaction log_event('S1', 'sql', 'start transaction', 'Query OK, 0 rows affected') cursor.execute(\"START TRANSACTION\") if operation_type == 'insert': # INSERT operation sql = f\"INSERT INTO example_single_pk (id) VALUES ({target_id})\" start = time.time() try: cursor.execute(sql) duration = time.time() - start log_event('S1', 'sql', sql, 'Query OK, 1 row affected', duration=duration) except pymysql.err.IntegrityError as e: duration = time.time() - start log_event('S1', 'error', sql, None, str(e), duration) elif operation_type == 'select_for_update': # SELECT FOR UPDATE operation sql = f\"SELECT * FROM example_single_pk WHERE id = {target_id} FOR UPDATE\" start = time.time() try: cursor.execute(sql) results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', sql, None) log_event('S1', 'info', '', f'Query returned {len(results)} rows ({duration:.2f}s)') if results: for row in results: formatted_row = [] for item in row: if hasattr(item, 'strftime'): formatted_row.append(item.strftime('%Y-%m-%d %H:%M:%S')) else: formatted_row.append(item) log_event('S1', 'info', '', str(tuple(formatted_row))) except Exception as e: duration = time.time() - start log_event('S1', 'error', sql, None, str(e), duration) elif operation_type == 'select_for_update_range': # SELECT FOR UPDATE range operation range_size = TEST_CONFIG['range_size'] end_id = target_id + range_size sql = f\"SELECT * FROM example_single_pk WHERE id \u003e= {target_id} AND id \u003c {end_id} FOR UPDATE\" start = time.time() try: cursor.execute(sql) results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', sql, None) log_event('S1', 'info', '', f'Range query re","date":"2025-09-27","objectID":"/zh-cn/2025/09/12.-mysql-lock/:3:5","tags":["MySQL","Lock"],"title":"[MySQL] 2. 锁机制执行分析","uri":"/zh-cn/2025/09/12.-mysql-lock/"},{"categories":["Github","Efficiency"],"content":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:0:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"引言 Value-Checker-Java本质上是一个可自定义的AOP切入点框架。它允许开发者在方法执行前插入自定义的验证逻辑，而这些验证逻辑可以是任意复杂的业务规则。 但是，如果仅仅是提供一个AOP切入点，那意义并不大。Value-Checker-Java的核心价值在于它提供了线程安全的上下文管理机制。如果没有这个上下文管理，在第一个验证器中查询的数据就无法在后续的验证器中使用，每个验证器都必须重新查询数据，这样就失去了验证链的意义。 正是因为有了ValueCheckerReentrantThreadLocal这个线程安全的上下文管理器，多个验证器才能够共享数据，形成真正有意义的验证链条。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:1:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Github value-checker-java-8 value-checker-java-17 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:2:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"基本使用 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:3:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"验证器配置 // 来自TargetService.java @ValueCheckers(checkers = { @ValueCheckers.ValueChecker(method = \"verify\", keys = {\"#id\", \"#name\"}, handler = SampleCheckerHandlerImpl.class), @ValueCheckers.ValueChecker(method = \"verify\", keys = \"#id\", handler = SampleCheckerHandlerImpl.class), @ValueCheckers.ValueChecker(method = \"verify\", keys = \"#name\", handler = SampleCheckerHandlerImpl.class) }) public void checker(Long id, String name) { // 会按顺序执行3个验证器 } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:3:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"验证器实现 // 来自SampleCheckerHandlerImpl.java @Service public class SampleCheckerHandlerImpl implements IValueCheckerHandler { public static final Long CORRECT_ID = 2L; public static final String CORRECT_NAME = \"correctName\"; public void verify(Long id, String name) { if (!CORRECT_ID.equals(id) || !CORRECT_NAME.equals(name)) { throw new ValueIllegalException(\"error\"); } } public void verify(Long id) { if (!CORRECT_ID.equals(id)) { throw new ValueIllegalException(\"error\"); } } public void verify(String name) { if (!CORRECT_NAME.equals(name)) { throw new ValueIllegalException(\"error\"); } } } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:3:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"关键技术实现 注解设计 @ValueCheckers采用了嵌套注解的设计模式： @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface ValueCheckers { ValueChecker[] checkers(); @interface ValueChecker { Class\u003c? extends IValueCheckerHandler\u003e handler(); String method() default \"verify\"; String[] keys() default \"\"; } } 设计亮点： 数组配置：支持多个验证器的组合 类型安全：Handler必须实现IValueCheckerHandler接口 灵活方法映射：可指定任意的验证方法名 参数化配置：通过keys数组传递验证所需参数 SpEL表达式引擎 SeplUtil类提供了强大的参数提取能力： public static Object[] getValue(ProceedingJoinPoint point, String[] keys) { MethodSignature methodSignature = (MethodSignature) point.getSignature(); String[] params = methodSignature.getParameterNames(); Object[] args = point.getArgs(); EvaluationContext context = new StandardEvaluationContext(); for (int len = 0; len \u003c params.length; len++) { context.setVariable(params[len], args[len]); } Object[] values = new Object[keys.length]; for (int i = 0; i \u003c keys.length; i++) { Expression expression = SPEL_PARSER.parseExpression(keys[i]); values[i] = expression.getValue(context, Object.class); } return values; } 技术特点： 动态参数映射：运行时获取方法参数名 表达式解析：支持复杂的SpEL表达式 类型安全：自动处理类型转换 性能优化：复用SpEL解析器实例 智能方法调用机制 ValueCheckerAspect中的方法调用机制具有以下特性： private void methodInvoke(Object instance, String method, Object[] paras) { // 生成方法签名缓存键 final String parasName = objectTypeName(paras); final String objectMethodName = String.format(OBJECT_METHOD_FORMAT, instanceClass.getSimpleName(), method, parasName); // 优先使用缓存的方法 if (OBJECT_METHOD_MAP.containsKey(objectMethodName)) { final Method pointMethod = OBJECT_METHOD_MAP.get(objectMethodName); pointMethod.invoke(instance, paras); return; } // 首次调用时进行方法匹配和缓存 for (Method subMethod : instanceClass.getMethods()) { // 方法名匹配 + 参数长度匹配 + 参数类型匹配 if (subMethod.getName().equals(method) \u0026\u0026 subMethod.getParameterTypes().length == paras.length \u0026\u0026 parasName.equals(methodTypeName(subMethod.getParameterTypes()))) { OBJECT_METHOD_MAP.put(objectMethodName, subMethod); subMethod.invoke(instance, paras); return; } } } 核心优势： 性能优化：方法反射结果缓存，避免重复查找 精确匹配：支持方法重载的准确识别 类型安全：严格的参数类型匹配 可重入ThreadLocal设计 ValueCheckerReentrantThreadLocal是框架的创新设计： public static void init() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter) { VALUE_CHECKER_THREAD_LOCAL.set(new ConcurrentHashMap\u003c\u003e()); VALUE_CHECKER_THREAD_LOCAL_COUNTER.set(new AtomicInteger()); return; } counter.addAndGet(1); } public static void clear() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter || counter.get() \u003c= 0) { VALUE_CHECKER_THREAD_LOCAL.remove(); VALUE_CHECKER_THREAD_LOCAL_COUNTER.remove(); return; } counter.addAndGet(-1); } 设计精髓： 引用计数：使用AtomicInteger实现可重入计数 线程安全：ConcurrentHashMap保证并发安全 自动清理：最外层调用结束时自动清理资源 嵌套支持：完美支持验证器的嵌套调用 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:3:3","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"ThreadLocal上下文管理 这是框架可以使用的核心之一。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:4:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"数据存储和获取 // 存储数据到ThreadLocal public void verifyPutThreadValue(String name) { ValueCheckerReentrantThreadLocal.getOrDefault(String.class, name); if (!ValueCheckerReentrantThreadLocal.getOrDefault(String.class, \"\").equals(name)) { throw new ValueIllegalException(\"error\"); } } // 从ThreadLocal获取数据 public void verifyGetRightThreadValue(String name) { if (!ValueCheckerReentrantThreadLocal.getOrDefault(String.class, name).equals(name)) { throw new ValueIllegalException(\"error\"); } } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:4:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"为什么重要 没有ThreadLocal： public void validateUser(Long userId) { User user = userRepository.findById(userId); // 第1次查询 } public void validateUserPermission(Long userId) { User user = userRepository.findById(userId); // 第2次查询，重复！ } 有了ThreadLocal： public void validateUser(Long userId) { User user = userRepository.findById(userId); // 只查询1次 ValueCheckerReentrantThreadLocal.put(user); } public void validateUserPermission(Long userId) { User user = ValueCheckerReentrantThreadLocal.get(User.class, () -\u003e null); // 直接获取 } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:4:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"可重入性支持 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:5:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"测试场景 // 来自TargetService.java @ValueCheckers(checkers = { @ValueCheckers.ValueChecker(method = \"verifyPutThreadValue\", keys = \"#name\", handler = SampleCheckerHandlerImpl.class) }) public void checkerReentrant(String name) { // 第1层AOP：存储数据到ThreadLocal this.targetService.checkerGetThreadValue(name); // 第2层AOP this.targetService.checkerGetWrongThreadValue(\"\"); // 第3层AOP } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:5:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"可重入计数器 // ValueCheckerReentrantThreadLocal.java public static void init() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter) { // 首次调用：初始化ThreadLocal VALUE_CHECKER_THREAD_LOCAL.set(new ConcurrentHashMap\u003c\u003e()); VALUE_CHECKER_THREAD_LOCAL_COUNTER.set(new AtomicInteger()); } else { // 嵌套调用：计数器+1 counter.addAndGet(1); } } public static void clear() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter || counter.get() \u003c= 0) { // 最外层调用：真正清理ThreadLocal VALUE_CHECKER_THREAD_LOCAL.remove(); VALUE_CHECKER_THREAD_LOCAL_COUNTER.remove(); } else { // 内层调用：计数器-1 counter.addAndGet(-1); } } 举例： // ValueCheckerAspectTest.java - 测试注释说明了整个流程 // aop1 - 1.1 counter = 0 init ThreadLocal // aop1 - 1.2 counter = 0 set RIGHT_VALUE to ThreadLocal // aop2 - 2.1 counter = 1 init ThreadLocal // aop2 - 2.2 counter = 1 try to set RIGHT_VALUE to ThreadLocal (success) // aop2 - 2.3 counter = 0 clear ThreadLocal (if not reentrant, RIGHT_VALUE will be clear) // aop3 - 3.1 counter = 1 init ThreadLocal // aop3 - 3.2 counter = 1 try to set WRONG_VALUE to ThreadLocal (fail) // aop3 - 3.3 counter = 0 clear ThreadLocal // aop1 - 1.3 counter = null clear ThreadLocal 如果没有可重入性支持，第二层和第三层AOP调用会清空第一层存储的ThreadLocal数据，导致验证失败。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:5:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"核心架构 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:6:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"AOP切面的try-finally结构 // ValueCheckerAspect.java - 关键的try-finally实现 @Around(\"handleValueCheckerPoint() \u0026\u0026 @annotation(valueCheckers)\") public Object around(ProceedingJoinPoint point, ValueCheckers valueCheckers) throws Throwable { try { // init ThreadLocal, if init in sub ValueChecker, ThreadLocal will counter++ ValueCheckerReentrantThreadLocal.init(); for (ValueCheckers.ValueChecker checker : valueCheckers.checkers()) { valueCheck(checker, point); } return point.proceed(); } finally { // clear ThreadLocal, if clear in sub ValueChecker, ThreadLocal will counter-- ValueCheckerReentrantThreadLocal.clear(); } } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:6:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"执行流程 @ValueCheckers注解方法 ↓ ValueCheckerAspect拦截 ↓ try { init ThreadLocal (可重入计数) ↓ 遍历checkers数组 ↓ SpEL提取参数 → 反射调用Handler ↓ 验证失败抛异常 / 验证成功继续 ↓ 所有验证通过执行原方法 } finally { clear ThreadLocal (可重入计数) } try-finally的关键作用： 保证资源清理：无论验证成功还是失败，都会清理ThreadLocal 可重入计数管理：通过计数器确保嵌套调用时ThreadLocal正确管理 防止内存泄漏：确保ThreadLocal在方法结束时被正确清理 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:6:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"性能优化设计 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:7:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"方法缓存机制 // ValueCheckerAspect.java - 基于性能考虑的方法缓存 private static final ConcurrentHashMap\u003cString, Method\u003e OBJECT_METHOD_MAP = new ConcurrentHashMap\u003c\u003e(); private void methodInvoke(Object instance, String method, Object[] paras) { // 生成缓存键 final String objectMethodName = String.format(OBJECT_METHOD_FORMAT, instanceClass.getSimpleName(), method, objectTypeName(paras)); // 优先使用缓存 if (OBJECT_METHOD_MAP.containsKey(objectMethodName)) { final Method pointMethod = OBJECT_METHOD_MAP.get(objectMethodName); pointMethod.invoke(instance, paras); return; } // 首次调用：遍历方法并缓存 for (Method subMethod : instanceClass.getMethods()) { if (subMethod.getName().equals(method) \u0026\u0026 subMethod.getParameterTypes().length == paras.length \u0026\u0026 objectTypeName(paras).equals(methodTypeName(subMethod.getParameterTypes()))) { OBJECT_METHOD_MAP.put(objectMethodName, subMethod); // 缓存结果 subMethod.invoke(instance, paras); return; } } } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:7:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"性能考虑点 反射开销优化： 首次调用遍历所有方法进行匹配 后续调用直接从ConcurrentHashMap获取Method对象 避免重复的反射查找操作 SpEL表达式性能： 复用SpEL解析器实例：private static final ExpressionParser SPEL_PARSER 运行时参数映射，支持复杂表达式但有性能成本 ThreadLocal开销： ThreadLocal操作本身轻量 可重入计数器使用AtomicInteger，线程安全且高效 验证链执行： 多个验证器串行执行 总耗时 = 各验证器耗时之和 通过数据共享减少重复查询 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:7:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Java 8 vs Java 17 核心差异：SpEL参数名获取 Java 17需要额外配置： \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cparameters\u003etrue\u003c/parameters\u003e \u003c!-- 保留参数名 --\u003e \u003c/configuration\u003e \u003c/plugin\u003e 依赖版本： Java 8：Spring Boot 2.5.13 Java 17：Spring Boot 3.5.5 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:8:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"总结 Value-Checker-Java解决的核心问题：验证器间数据共享。 本质：可自定义AOP + ThreadLocal上下文 价值：避免重复查询，让验证链有意义 关键：可重入ThreadLocal支持嵌套调用 场景：多步验证需要共享查询结果的业务 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/11.-value-checker/:9:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java：可自定义的AOP验证框架","uri":"/zh-cn/2025/09/11.-value-checker/"},{"categories":["Github","Efficiency"],"content":"[Github] 3. Basic-Check：参数验证框架 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:0:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"引言 在日常的Java开发中，方法参数验证是一个常见且重要的需求。传统的参数验证通常需要在每个方法中编写大量的if-else判断代码，不仅冗余繁琐，还容易遗漏。Basic-Check-Java正是为了解决这一痛点而诞生的轻量级参数验证框架。 本文将深入介绍Basic-Check-Java的设计理念、核心特性以及实际应用，帮助开发者快速掌握这个实用的工具。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:1:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Github basic-check-java-8 basic-check-java-17 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:2:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"设计理念与核心特性 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:3:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"设计理念 Basic-Check-Java基于以下核心理念设计： 简洁性：通过注解声明式编程，减少样板代码 灵活性：支持多种返回策略，适应不同业务场景 无侵入性：基于AOP实现，对业务代码零侵入 可扩展性：支持自定义验证规则和处理逻辑 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:3:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"核心特性 丰富的参数验证注解 Basic-Check-Java提供了六种常用的参数验证注解： @CheckNull：验证参数不为null @CheckString：验证字符串参数非空白 @CheckLong：验证Long类型参数大于-1 @CheckCollection：验证集合类型参数非空 @CheckMap：验证Map类型参数非空 @CheckObject：使用Bean Validation验证对象参数 灵活的返回策略 通过@BasicCheck注解的returnType属性，支持三种验证失败时的处理策略： EXCEPTION（默认）：抛出IllegalArgumentException异常 EMPTY：根据方法返回类型自动返回空值（空集合、空Map、Optional.empty()等） NULL：直接返回null 基于Spring AOP的无侵入式实现 框架采用AspectJ注解和Spring AOP技术，通过切面编程在方法执行前进行参数验证，对业务代码完全无侵入。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:3:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"技术架构深入分析 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:4:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"核心架构图 @BasicCheck注解方法 ↓ NotNullAndPositiveAspect切面拦截 ↓ 遍历方法参数及其注解 ↓ 根据注解类型执行相应验证逻辑 ↓ 验证失败 → 根据returnType返回相应结果 验证成功 → 继续执行原方法 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:4:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"关键技术实现 注解设计 以@BasicCheck为例，展示了优雅的注解设计： @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface BasicCheck { ReturnType returnType() default ReturnType.EXCEPTION; enum ReturnType { EMPTY, // 返回空集合/映射/可选值 NULL, // 返回null EXCEPTION // 抛出异常 } } 关键设计要点： @Target(ElementType.METHOD)：只能作用于方法级别 @Retention(RetentionPolicy.RUNTIME)：运行时保留注解信息 提供了枚举类型的配置选项，保证类型安全 AOP切面实现 NotNullAndPositiveAspect是框架的核心组件，承担了参数验证的主要逻辑： @Around(\"handleBasicCheckPoint() \u0026\u0026 @annotation(basicCheck)\") public Object around(ProceedingJoinPoint point, BasicCheck basicCheck) throws Throwable { final Object[] args = point.getArgs(); final MethodSignature signature = (MethodSignature) point.getSignature(); final Method method = signature.getMethod(); final Parameter[] parameters = method.getParameters(); // 遍历所有参数进行验证 for (int i = 0; i \u003c parameters.length; i++) { // 根据不同注解类型执行相应验证逻辑 if (parameters[i].isAnnotationPresent(CheckNull.class) \u0026\u0026 null == args[i]) { return this.getReturnObj(basicCheck, method); } // ... 其他验证逻辑 } return point.proceed(); } 实现亮点： 参数与注解映射：通过反射获取方法参数和对应的注解信息 验证逻辑解耦：每种验证类型独立处理，便于维护和扩展 智能返回处理：根据方法返回类型和配置策略智能生成返回值 智能返回值生成 框架的一个巧妙设计是根据方法返回类型自动生成合适的空值： private Object getReturnObj(BasicCheck annotation, Method method) { if (annotation.returnType() == BasicCheck.ReturnType.EMPTY) { Class\u003c?\u003e returnType = method.getReturnType(); if (returnType == List.class) return Collections.emptyList(); if (returnType == Set.class) return Collections.emptySet(); if (returnType == Map.class) return Collections.emptyMap(); if (returnType == Optional.class) return Optional.empty(); } // ... 其他处理逻辑 } 这种设计避免了开发者手动处理不同返回类型的复杂性。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:4:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"实际应用案例 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:5:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"基础使用示例 简单参数验证 @Service public class UserService { @BasicCheck public void createUser(@CheckNull String username, @CheckString String email, @CheckLong Long age) { // 业务逻辑，无需手动验证参数 userRepository.save(new User(username, email, age)); } } 集合参数验证 @BasicCheck public void batchCreateUsers(@CheckCollection List\u003cUser\u003e users, @CheckMap Map\u003cString, Object\u003e config) { // 确保users集合和config映射都不为空 users.forEach(user -\u003e userRepository.save(user)); } 返回空值策略 @BasicCheck(returnType = BasicCheck.ReturnType.EMPTY) public List\u003cUser\u003e searchUsers(@CheckString String keyword) { // 如果keyword为空白字符串，自动返回空List return userRepository.findByKeyword(keyword); } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:5:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"高级应用场景 复杂对象验证 // 定义验证DTO @Data @Builder public class UserCreateRequest { @NotNull @Min(1) private Integer id; @NotBlank private String name; @Valid @NotEmpty private List\u003c@Valid ContactInfo\u003e contacts; } // 使用@CheckObject验证复杂对象 @BasicCheck public void createUserWithDetails(@CheckObject UserCreateRequest request) { // 框架会自动使用Bean Validation验证整个对象树 userService.createUser(request); } 混合验证策略 @BasicCheck(returnType = BasicCheck.ReturnType.NULL) public void processOrder(@CheckLong Long orderId, @CheckString String customerId, @CheckCollection List\u003cOrderItem\u003e items, @CheckObject OrderConfig config) { // 任一参数验证失败都返回null，适合void方法 orderProcessor.process(orderId, customerId, items, config); } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:5:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"性能考量与最佳实践 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:6:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"性能分析 反射开销：框架使用反射获取方法参数信息，建议在高并发场景下进行性能测试 AOP开销：Spring AOP基于代理模式，会有轻微的性能开销 Bean Validation：对象验证会有一定的性能成本，但通常可以接受 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:6:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"最佳实践 合理选择验证策略 // 推荐：查询方法使用EMPTY策略 @BasicCheck(returnType = BasicCheck.ReturnType.EMPTY) public List\u003cUser\u003e findUsers(@CheckString String keyword) { return userRepository.findByKeyword(keyword); } // 推荐：创建/更新方法使用EXCEPTION策略 @BasicCheck public void updateUser(@CheckLong Long id, @CheckObject UserUpdateRequest request) { userRepository.update(id, request); } 验证注解的组合使用 // 组合使用多个验证注解 @BasicCheck public void processUserData(@CheckNull @CheckString String username, @CheckNull @CheckLong Long userId, @CheckNull @CheckObject UserData data) { // 既检查null又检查业务规则 } 自定义验证对象的设计 @Data public class ProductRequest { @NotNull(message = \"产品ID不能为空\") @Min(value = 1, message = \"产品ID必须大于0\") private Long productId; @NotBlank(message = \"产品名称不能为空\") @Size(max = 100, message = \"产品名称长度不能超过100\") private String productName; @Valid // 启用嵌套验证 @NotNull(message = \"产品配置不能为空\") private ProductConfig config; } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:6:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Java 8 vs Java 17版本差异 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:7:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"核心差异：Validation API的命名空间变更 这是两个版本最重要的区别： Java 8版本使用传统的javax命名空间： import javax.validation.Validation; import javax.validation.Validator; Java 17版本使用新的jakarta命名空间： import jakarta.validation.Validation; import jakarta.validation.Validator; ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:7:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"完整依赖对比 Java 8版本依赖： Spring Boot 2.4.4 Lombok 1.18.18 javax.validation:validation-api:2.0.1.Final org.hibernate:hibernate-validator:6.0.1.Final org.glassfish:javax.el:3.0.1-b09 Java 17版本依赖： Spring Boot 3.5.5 Lombok 1.18.38 org.hibernate.validator:hibernate-validator:8.0.1.Final org.glassfish:jakarta.el:4.0.2 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:7:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"扩展与定制 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:8:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"自定义验证注解 框架采用开放式设计，支持添加自定义验证注解： // 1. 定义自定义验证注解 @Target(ElementType.PARAMETER) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface CheckEmail { } // 2. 在NotNullAndPositiveAspect中添加验证逻辑 if (parameters[i].isAnnotationPresent(CheckEmail.class) \u0026\u0026 !isValidEmail((String) args[i])) { return this.getReturnObj(basicCheck, method); } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:8:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"自定义返回策略 可以扩展ReturnType枚举，添加更多的返回策略： public enum ReturnType { EMPTY, NULL, EXCEPTION, CUSTOM_DEFAULT // 自定义默认值策略 } ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:8:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"总结 Basic-Check-Java是一个设计精巧、功能实用的参数验证框架。它通过以下特点解决了Java开发中参数验证的痛点： ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:9:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"核心优势 开发效率高：减少80%的参数验证样板代码 使用简单：仅需添加注解即可享受完整的验证功能 功能全面：支持基本类型、集合、复杂对象的多层次验证 灵活可配：多种返回策略适应不同业务场景 无侵入性：基于AOP实现，对现有代码零影响 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:9:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Github","Efficiency"],"content":"适用场景 Web应用：Controller层参数验证 服务层：Service方法参数校验 工具类：通用工具方法的参数检查 API接口：第三方接口调用前的参数预检 Basic-Check-Java体现了优秀框架的设计原则：简单易用、功能完整、性能可靠、易于扩展。它不仅能够显著提升开发效率，还能帮助开发者写出更加健壮和优雅的代码。 无论是新项目还是既有项目，Basic-Check-Java都能够快速集成并发挥价值，是Java开发者工具箱中不可缺少的利器。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/10.-basic-check/:9:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check：参数验证框架","uri":"/zh-cn/2025/09/10.-basic-check/"},{"categories":["Cluster","Algorithm"],"content":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:0:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"引言 RAFT（Raft Consensus Algorithm）是一种分布式共识算法，旨在解决分布式系统中多个节点对数据状态达成一致的问题。 相比于著名的Paxos算法，RAFT的设计理念是\"可理解性\"（understandability），通过清晰的角色划分和简洁的状态转换，让开发者更容易理解和实现。 本文将通过11个详细的图例，展示RAFT算法从单节点到多节点集群的完整演进过程，包括正常运行、故障处理、网络分区和冲突解决等关键场景。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:1:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"RAFT核心概念 在深入分析之前，让我们先了解RAFT的几个核心概念： 节点状态(Node States)： Leader（领导者）：负责处理客户端请求，向其他节点复制日志条目 Follower（跟随者）：被动接收Leader的日志复制请求 Candidate（候选者）：Leader选举过程中的临时状态 关键数据结构： Log（日志）：存储操作命令的有序序列 Term（任期）：单调递增的逻辑时钟，用于检测过期信息 CommitIndex（提交索引）：已知被提交的最高日志条目索引 ApplyIndex/LastApplied（应用索引）：已应用到状态机的最高日志条目索引 State（状态机）：实际的业务数据状态 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:2:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段一：单节点启动（图1） Node1启动为Leader，初始状态： - Log=[]（空日志） - Term=0（初始任期） - CommitIndex=0, ApplyIndex=0（无已提交/应用的条目） - State={}（空状态机） 单节点集群中，节点自动成为Leader，因为它构成了\"多数派\"（1 \u003e 1/2）。这是RAFT算法的一个重要特性：任何时刻最多只能有一个Leader。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:3:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段二：处理客户端请求（图2） 客户端请求：x=1 处理流程： 1. Leader将\"x=1\"追加到日志（Log=[x=1]） 2. 更新Term=1（在某些实现中，term在接收客户端请求时更新） 3. 单节点立即提交（CommitIndex=1） 4. 应用到状态机（ApplyIndex=1, State={x:1}） 这展示了RAFT的基本工作流程：日志追加 → 复制 → 提交 → 应用。在单节点场景下，这个过程会立即完成。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:4:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段三：节点加入集群（图3） Node2和Node3以Follower身份加入： - 新节点初始状态：Term=0, 空日志，空状态机 - 通过\"join\"操作发现已存在的Leader 新节点加入时处于Follower状态，需要从Leader同步历史数据。这体现了RAFT的动态成员变更能力。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:5:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段四：日志同步（图4） Leader同步过程： 1. Node1向Node2和Node3发送AppendEntries RPC 2. 包含历史日志条目\"x=1\" 3. Followers更新自己的状态： - Term=1（同步Leader的任期） - Log=[x=1]（复制日志条目） - CommitIndex=1, ApplyIndex=1（提交并应用） - State={x:1}（状态机同步） 这个阶段展示了RAFT的核心机制：日志复制。Leader确保所有Followers的日志与自己保持一致。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:6:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段五：集群扩展（图5） 继续添加Node4和Node5： - 新节点通过\"join \u0026 rpc sync\"一步完成加入和同步 - 最终形成5节点集群，所有节点状态一致 集群可以动态扩展，新节点加入时会自动完成历史数据的同步。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:7:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段六：批量操作与故障（图6-图6结果） 图6展示问题场景： Leader处理多个客户端请求： - x=2, y=1, y=2, update term - Node1状态：Term=2, CommitIndex=4, State={x:2, y:2} 图6结果展示恢复状态： Leader继续工作： - Node1成功同步到Node2, Node4, Node5 - Node3故障，保持旧状态（x=1） - 集群在多数节点正常的情况下继续提供服务 这体现了RAFT的容错性：只要多数节点正常，集群就能继续工作。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:8:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段七：选举失败场景（图7） 网络分区发生： - Node1被隔离（单独分区） - Node3尝试选举但失败： - 更新Term=2，变为Candidate - 但无法获得多数票（日志过于陈旧） 分区状态： - 分区1：Node1（孤立的旧Leader） - 分区2：Node2,Node3,Node4,Node5（无新Leader） 这展示了RAFT处理网络分区的机制：日志较新的节点更有可能成为新Leader。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:9:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段八：分区中的选举（图8） Node2发起选举： - Term=3，成为Candidate - 向Node4、Node5、Node3发送RequestVote RPC - 即便Node3日志落后，但是仍可以投票给Node2 - 同时，被隔离的Node1继续接收客户端请求\"x=9\" 被分区的旧Leader仍然处理请求，但这些操作不会被提交，因为无法获得多数确认。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:10:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段九：新Leader确立但存在分区（图9） Node2成为新Leader： - 获得足够选票，Term=3 - 开始向其他节点同步 - Node1和Node3状态不同步： - Node1：有未提交的\"x=9\"，Term=2 - Node3：仍然是旧数据，Term=1 这证明了RAFT的分区容忍性：即使部分节点不可达，多数派仍能正常工作。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:11:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段十：冲突检测（图10） Node2作为新Leader开始工作： - 向Node4和Node5同步（成功） - Node1和Node3恢复 - 集群在可用的多数节点上继续运行 这个状态展示了脑裂问题：系统中存在新旧两个Leader的状态。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:12:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"阶段十一：分区愈合与冲突解决（图11） 网络分区修复后的最终状态： - 所有节点重新连接 - Node2仍为Leader，Term=3 - 冲突解决过程： 1. Node1发现更高的term，降级为Follower 2. Node1的未提交操作\"x=9\"被丢弃（日志截断） 3. Node3同步到最新状态 4. 所有节点最终一致：State={x:2, y:2} 数据丢失：x=9永久丢失，因为它从未被多数确认 这是RAFT处理日志冲突的经典场景： 更高term的Leader是权威的 未被多数确认的操作会被丢弃 最终所有节点达到强一致性 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:13:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"RAFT算法的关键特性分析 通过这11个图例，我们可以总结RAFT的重要特性： 1. 强一致性（Strong Consistency） 所有节点最终达到相同状态 通过多数派提交确保数据可靠性 2. 分区容忍性（Partition Tolerance） 网络分区时，多数派分区继续服务 少数派分区无法处理写操作 3. 领导选举（Leader Election） 任何时刻最多一个Leader 通过term和日志比较确保最新节点当选 4. 日志复制（Log Replication） Leader负责向所有Followers复制日志 保证日志的顺序性和一致性 5. 故障恢复（Failure Recovery） 节点故障后重新加入能自动同步 冲突日志会被正确覆盖 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:14:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"实际应用考虑 性能特点： 写操作需要多数确认，延迟较高 读操作可以从Leader或Follower进行 网络分区会影响可用性 适用场景： 配置管理系统（如etcd） 分布式数据库（如TiKV） 分布式锁服务 注意事项： 奇数节点集群更好（避免脑裂） 网络质量对性能影响很大 需要考虑成员变更的安全性 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:15:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"结论 RAFT算法通过清晰的角色划分和简单的规则，优雅地解决了分布式共识问题。本文通过11个渐进式图例，完整展示了从单节点到复杂故障场景的处理过程。理解这些场景对于设计和实现可靠的分布式系统具有重要意义。 RAFT的成功在于其可理解性：相比Paxos的复杂证明，RAFT用直观的概念（任期、选举、日志复制）让开发者能够真正理解并正确实现分布式一致性系统。 ","date":"2025-09-26","objectID":"/zh-cn/2025/09/9.-understanding_raft_algorithm/:16:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. 浅谈RAFT算法：从单节点到分布式共识的完整演进","uri":"/zh-cn/2025/09/9.-understanding_raft_algorithm/"},{"categories":["Github","MyBatis","plugins"],"content":"[Github] 2. MyBatis Generator 自定义插件 🔗 项目地址: mybatis-generator-custome-plugins 为 MyBatis Generator 设计的强大自定义插件集合，专门针对 MySQL 数据库特性，提供 DTO 层生成、Service 层自动生成等功能。 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:0:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🚀 功能概览 本插件集合包含以下6个自定义插件： 插件名称 功能描述 核心特性 InsertIgnoreIntoPlugin MySQL INSERT IGNORE语句支持 批量插入忽略重复记录 InsertOnDuplicateKeyPlugin MySQL ON DUPLICATE KEY UPDATE支持 插入冲突时自动更新 ReplaceIntoPlugin MySQL REPLACE INTO语句支持 替换插入操作 DtoGeneratorPlugin DTO层代码生成 Lombok注解，Entity转换方法 ServiceGeneratorPlugin Service层代码生成 接口+实现，完整CRUD操作 CustomerMapperPlugin 自定义Mapper生成 扩展原生Mapper功能 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:1:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"📦 依赖分析 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:2:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"核心依赖 \u003c!-- MyBatis Generator核心依赖 --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.generator\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-core\u003c/artifactId\u003e \u003cversion\u003e1.4.2\u003c/version\u003e \u003c/dependency\u003e \u003c!-- MyBatis Dynamic SQL支持 --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.dynamic-sql\u003c/groupId\u003e \u003cartifactId\u003emybatis-dynamic-sql\u003c/artifactId\u003e \u003cversion\u003e1.5.2\u003c/version\u003e \u003c/dependency\u003e \u003c!-- MyBatis Spring Boot集成 --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.spring.boot\u003c/groupId\u003e \u003cartifactId\u003emybatis-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e3.0.5\u003c/version\u003e \u003c/dependency\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:2:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Maven插件配置 \u003cplugin\u003e \u003cgroupId\u003eorg.mybatis.generator\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e1.4.2\u003c/version\u003e \u003cconfiguration\u003e \u003cverbose\u003etrue\u003c/verbose\u003e \u003coverwrite\u003etrue\u003c/overwrite\u003e \u003c/configuration\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.goody.utils\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-custome-plugins\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/plugin\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:2:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🔧 插件详述 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:3:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"MySQL扩展插件 InsertIgnoreIntoPlugin 功能: 为Mapper添加INSERT IGNORE语句支持 生成方法: insertIgnoreCustom(), insertIgnoreBatchCustom() 应用场景: 批量插入时忽略主键冲突记录 InsertOnDuplicateKeyPlugin 功能: 为Mapper添加ON DUPLICATE KEY UPDATE语句支持 生成方法: insertOnDuplicateKeyCustom(), insertOnDuplicateKeyBatchCustom() 应用场景: 插入时遇到重复键则更新记录 ReplaceIntoPlugin 功能: 为Mapper添加REPLACE INTO语句支持 生成方法: replaceIntoCustom(), replaceIntoBatchCustom() 应用场景: 存在则替换，不存在则插入 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:3:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"DTO层生成插件 DtoGeneratorPlugin 功能: 自动生成DTO类 特性: Lombok注解支持(@Data, @Builder, @AllArgsConstructor, @NoArgsConstructor) 自动生成fromEntity()和toEntity()转换方法 包结构: *.model.dto ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:3:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Service层生成插件 ServiceGeneratorPlugin 功能: 自动生成Service接口和实现类 特性: 完整CRUD操作方法 支持单主键和联合主键 Spring注解支持(@Service, @Autowired) 包结构: *.service.interfaces 和 *.service.impl ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:3:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"自定义Mapper插件 CustomerMapperPlugin 功能: 生成扩展Mapper接口 包结构: *.dao.customer ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:3:4","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"💻 使用方法 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:4:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"步骤1: 添加依赖 将插件添加到你的项目中： \u003cdependency\u003e \u003cgroupId\u003ecom.goody.utils\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-custome-plugins\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:4:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"步骤2: 配置generatorConfig.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"https://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"\u003e \u003cgeneratorConfiguration\u003e \u003cclassPathEntry location=\"${user.home}/.m2/repository/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar\"/\u003e \u003ccontext id=\"dao\" targetRuntime=\"MyBatis3DynamicSql\"\u003e \u003cproperty name=\"autoDelimitKeywords\" value=\"true\"/\u003e \u003cproperty name=\"beginningDelimiter\" value=\"`\"/\u003e \u003cproperty name=\"endingDelimiter\" value=\"`\"/\u003e \u003c!-- 标准插件 --\u003e \u003cplugin type=\"org.mybatis.generator.plugins.SerializablePlugin\" /\u003e \u003cplugin type=\"org.mybatis.generator.plugins.EqualsHashCodePlugin\" /\u003e \u003cplugin type=\"org.mybatis.generator.plugins.ToStringPlugin\" /\u003e \u003cplugin type=\"org.mybatis.generator.plugins.FluentBuilderMethodsPlugin\" /\u003e \u003c!-- 自定义插件 --\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.InsertIgnoreIntoPlugin\" /\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.InsertOnDuplicateKeyPlugin\" /\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.ReplaceIntoPlugin\" /\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.DtoGeneratorPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.ServiceGeneratorPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.CustomerMapperPlugin\"/\u003e \u003ccommentGenerator\u003e \u003cproperty name=\"addRemarkComments\" value=\"true\" /\u003e \u003cproperty name=\"suppressDate\" value=\"true\"/\u003e \u003c/commentGenerator\u003e \u003cjdbcConnection driverClass=\"com.mysql.cj.jdbc.Driver\" connectionURL=\"jdbc:mysql://127.0.0.1:3306/your_database\" userId=\"your_username\" password=\"your_password\"\u003e \u003cproperty name=\"useSSL\" value=\"false\" /\u003e \u003cproperty name=\"serverTimezone\" value=\"Asia/Shanghai\" /\u003e \u003cproperty name=\"nullCatalogMeansCurrent\" value=\"true\" /\u003e \u003c/jdbcConnection\u003e \u003cjavaTypeResolver \u003e \u003cproperty name=\"forceBigDecimals\" value=\"true\" /\u003e \u003cproperty name=\"useJSR310Types\" value=\"true\" /\u003e \u003c/javaTypeResolver\u003e \u003cjavaModelGenerator targetPackage=\"com.yourpackage.model.entity\" targetProject=\"src/main/java\"\u003e \u003cproperty name=\"enableSubPackages\" value=\"true\" /\u003e \u003cproperty name=\"trimStrings\" value=\"true\" /\u003e \u003c/javaModelGenerator\u003e \u003cjavaClientGenerator type=\"ANNOTATEDMAPPER\" targetPackage=\"com.yourpackage.model.dao\" targetProject=\"src/main/java\"\u003e \u003cproperty name=\"enableSubPackages\" value=\"true\" /\u003e \u003c/javaClientGenerator\u003e \u003c!-- 配置要生成的表 --\u003e \u003ctable schema=\"your_schema\" tableName=\"your_table\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e \u003c/context\u003e \u003c/generatorConfiguration\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:4:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"步骤3: 执行代码生成 mvn mybatis-generator:generate ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:4:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"📝 配置示例 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:5:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"单主键表配置 \u003ctable schema=\"toy\" tableName=\"example_single_pk\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:5:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"联合主键表配置 \u003ctable schema=\"toy\" tableName=\"example_double_pk\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:5:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🏗️ 生成代码分析 插件会为每个表生成完整的代码结构： ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:6:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"文件结构 src/main/java/ ├── com/yourpackage/model/ │ ├── entity/ # Entity实体类 │ │ ├── Example.java │ │ └── ExampleDoublePk.java │ ├── dao/ # 标准Mapper接口 │ │ ├── ExampleMapper.java │ │ ├── ExampleDynamicSqlSupport.java │ │ └── customer/ # 自定义Mapper接口 │ │ └── CustomerExampleMapper.java │ └── dto/ # DTO类 │ ├── ExampleDTO.java │ └── ExampleDoublePkDTO.java ├── service/ │ ├── interfaces/ # Service接口 │ │ ├── IExampleService.java │ │ └── IExampleDoublePkService.java │ └── impl/ # Service实现 │ ├── ExampleServiceImpl.java │ └── ExampleDoublePkServiceImpl.java ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:6:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"核心代码片段分析 MySQL扩展方法 (Mapper层) @Mapper public interface ExampleMapper { // 标准生成的方法... // INSERT IGNORE支持 @Insert({\"\u003cscript\u003e\" + \" INSERT IGNORE INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \"\u003c/script\u003e\"}) void insertIgnoreCustom(@Param(\"item\") Example record); // 批量INSERT IGNORE @Insert({\"\u003cscript\u003e\" + \" INSERT IGNORE INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" \u003cforeach collection='items' item='item' separator=','\u003e\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \" \u003c/foreach\u003e\" + \"\u003c/script\u003e\"}) void insertIgnoreBatchCustom(@Param(\"items\") Collection\u003cExample\u003e records); // ON DUPLICATE KEY UPDATE支持 @Insert({\"\u003cscript\u003e\" + \" INSERT INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \" AS r\" + \" ON DUPLICATE KEY UPDATE\" + \" name = r.name, updated = r.updated\" + \"\u003c/script\u003e\"}) void insertOnDuplicateKeyCustom(@Param(\"item\") Example record); } DTO类 (数据传输层) @Data @AllArgsConstructor @NoArgsConstructor @Builder public class ExampleDTO { private Long id; private String name; private LocalDateTime created; private LocalDateTime updated; // Entity转DTO public static ExampleDTO fromEntity(Example entity) { if (entity == null) return null; return ExampleDTO.builder() .id(entity.getId()) .name(entity.getName()) .created(entity.getCreated()) .updated(entity.getUpdated()) .build(); } // DTO转Entity public Example toEntity() { return new Example() .withId(this.id) .withName(this.name) .withCreated(this.created) .withUpdated(this.updated); } } Service接口 (服务层接口) 单主键Service接口: public interface IExampleService { int save(ExampleDTO dto); int saveBatch(List\u003cExampleDTO\u003e dtoList); int update(ExampleDTO dto); int deleteById(Long id); // 单参数 ExampleDTO findById(Long id); // 单参数 List\u003cExampleDTO\u003e findAll(); } 联合主键Service接口 (插件已修复): public interface IExampleDoublePkService { int save(ExampleDoublePkDTO dto); int saveBatch(List\u003cExampleDoublePkDTO\u003e dtoList); int update(ExampleDoublePkDTO dto); int deleteById(Long id, Long id2); // 多参数支持 ExampleDoublePkDTO findById(Long id, Long id2); // 多参数支持 List\u003cExampleDoublePkDTO\u003e findAll(); } Service实现 (服务层实现) 联合主键处理的核心改进: @Service public class ExampleDoublePkServiceImpl implements IExampleDoublePkService { @Autowired private ExampleDoublePkMapper exampleDoublePkMapper; @Override public int deleteById(Long id, Long id2) { if (id == null || id2 == null) { return 0; } // 正确调用联合主键方法 return exampleDoublePkMapper.deleteByPrimaryKey(id, id2); } @Override public ExampleDoublePkDTO findById(Long id, Long id2) { if (id == null || id2 == null) { return null; } // 正确调用联合主键方法 ExampleDoublePk entity = exampleDoublePkMapper .selectByPrimaryKey(id, id2).orElse(null); return entity != null ? ExampleDoublePkDTO.fromEntity(entity) : null; } } ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:6:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"关键特性分析 联合主键支持 问题: 之前版本错误地尝试使用单一主键类型 解决: 自动检测主键列数量，为联合主键生成多参数方法 实现: List\u003cIntrospectedColumn\u003e primaryKeyColumns MySQL特有功能 INSERT IGNORE: 忽略重复键错误，继续插入其他记录 ON DUPLICATE KEY UPDATE: 插入冲突时自动更新指定字段 REPLACE INTO: MySQL的替换插入操作 自动化程度 依赖注入: 自动生成Spring @Autowired注解 空值检查: 自动生成参数空值验证 转换方法: DTO与Entity之间的自动转换 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:6:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🏗️ 项目结构 mybatis-generator-custome-plugins/ ├── src/main/java/com/goody/utils/mybatis/plugin/ │ ├── CustomerMapperPlugin.java │ ├── DtoGeneratorPlugin.java │ ├── InsertIgnoreIntoPlugin.java │ ├── InsertOnDuplicateKeyPlugin.java │ ├── ReplaceIntoPlugin.java │ └── ServiceGeneratorPlugin.java ├── src/main/resources/ │ └── generatorConfig.xml ├── src/test/ # 生成代码示例 └── pom.xml ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:7:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🎯 使用建议 开发流程: 先设计数据库表结构，然后使用插件生成完整的分层代码 联合主键: 插件已完美支持联合主键，无需手动修改生成代码 扩展性: 可以继承生成的Service接口，添加复杂业务逻辑 MySQL优化: 合理使用INSERT IGNORE和ON DUPLICATE KEY功能提升性能 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:8:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🔍 源码深度分析 此部分为开发者提供插件实现细节的深入洞察，适合希望理解或扩展插件功能的开发者阅读。 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"插件架构概览 所有插件都继承 PluginAdapter 并遵循MyBatis Generator的插件生命周期： public abstract class PluginAdapter implements Plugin { // 验证阶段 public boolean validate(List\u003cString\u003e warnings); // 代码生成钩子 public boolean clientGenerated(Interface interfaze, IntrospectedTable introspectedTable); public List\u003cGeneratedJavaFile\u003e contextGenerateAdditionalJavaFiles(IntrospectedTable introspectedTable); } ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"InsertIgnoreIntoPlugin 实现分析 核心方法生成策略 插件使用精巧的方法动态生成SQL模板： private Method insertIgnoreIntoOne(Interface interfaze, IntrospectedTable introspectedTable) { // 使用Stream API进行动态列映射 final String columnNames = introspectedTable.getAllColumns() .stream() .map(column -\u003e String.format(\"`%s`\", column.getActualColumnName())) .collect(Collectors.joining(\", \")); // MyBatis语法的参数绑定 final String columnValueNames = introspectedTable.getAllColumns() .stream() .map(column -\u003e String.format(\"#{item.%s}\", column.getJavaProperty())) .collect(Collectors.joining(\", \")); // 基于模板的注解生成 String insertIgnore = String.format(\"@Insert({\" + \"\\\"\u003cscript\u003e\\\" +\\n\" + \" \\\" INSERT IGNORE INTO %s\\\" +\\n\" + \" \\\" (%s)\\\" +\\n\" + \" \\\" VALUES\\\" +\\n\" + \" \\\"(%s)\\\" +\\n\" + \" \\\"\u003c/script\u003e\\\"\" + \"})\", tableName, columnNames, columnValueNames); } 关键技术创新 基于Stream的字段转换: 高效地将数据库模式转换为Java方法参数 动态SQL模板生成: 创建带适当转义的参数化SQL 批量处理支持: 为批量操作自动生成 \u003cforeach\u003e 循环 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"DtoGeneratorPlugin 实现分析 包名解析算法 private TopLevelClass generateDtoClass(IntrospectedTable introspectedTable) { // 智能包名转换 String entityFullType = introspectedTable.getBaseRecordType(); String entityPackage = entityFullType.substring(0, entityFullType.lastIndexOf('.')); String dtoFullPackage = entityPackage.replace(\".entity\", \".\" + DTO_PACKAGE); // 类型安全的类实例化 FullyQualifiedJavaType dtoType = new FullyQualifiedJavaType(dtoFullPackage + \".\" + dtoClassName); TopLevelClass dtoClass = new TopLevelClass(dtoType); } 带元数据保留的字段生成 // 带注释保留的列到字段转换 for (IntrospectedColumn column : allColumns) { Field field = new Field(column.getJavaProperty(), column.getFullyQualifiedJavaType()); field.setVisibility(JavaVisibility.PRIVATE); // 保留数据库列注释 if (column.getRemarks() != null \u0026\u0026 !column.getRemarks().trim().isEmpty()) { field.addJavaDocLine(\"/**\"); field.addJavaDocLine(\" * \" + column.getRemarks()); field.addJavaDocLine(\" */\"); } dtoClass.addField(field); } ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"ServiceGeneratorPlugin 实现分析 联合主键解析 最精妙的功能是智能联合主键处理： // 动态主键分析 List\u003cIntrospectedColumn\u003e primaryKeyColumns = introspectedTable.getPrimaryKeyColumns(); // 自适应方法签名生成 for (IntrospectedColumn column : primaryKeyColumns) { String paramName = column.getJavaProperty(); method.addParameter(new Parameter(column.getFullyQualifiedJavaType(), paramName)); } // 动态方法调用构建 StringBuilder methodCall = new StringBuilder(\"return \" + mapperFieldName + \".deleteByPrimaryKey(\"); for (int i = 0; i \u003c primaryKeyColumns.size(); i++) { if (i \u003e 0) methodCall.append(\", \"); methodCall.append(primaryKeyColumns.get(i).getJavaProperty()); } methodCall.append(\");\"); Service实现模式 // 空值安全的参数验证 for (IntrospectedColumn column : primaryKeyColumns) { method.addBodyLine(\"if (\" + column.getJavaProperty() + \" == null) {\"); method.addBodyLine(\" return 0;\"); method.addBodyLine(\"}\"); } ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:4","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"高级模式和技术 类型安全强化 // 泛型类型保留 FullyQualifiedJavaType listType = new FullyQualifiedJavaType(\"List\u003c\" + dtoClassName + \"\u003e\"); Parameter batchParameter = new Parameter(listType, \"dtoList\"); // 导入解析 serviceInterface.addImportedType(new FullyQualifiedJavaType(\"java.util.List\")); SQL注入防护 所有生成的SQL都使用参数化查询： // 安全：参数化查询 \"INSERT IGNORE INTO \" + tableName + \" VALUES (#{item.id}, #{item.name})\" // 不安全：字符串拼接（从不使用） \"INSERT IGNORE INTO \" + tableName + \" VALUES (\" + item.getId() + \", '\" + item.getName() + \"')\" 内存高效生成 // 大型代码库的延迟求值模式 public List\u003cGeneratedJavaFile\u003e contextGenerateAdditionalJavaFiles(IntrospectedTable introspectedTable) { List\u003cGeneratedJavaFile\u003e files = new ArrayList\u003c\u003e(); // 仅在需要时生成 if (shouldGenerateService(introspectedTable)) { files.add(createServiceInterface(introspectedTable)); files.add(createServiceImplementation(introspectedTable)); } return files; } ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:5","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"自定义开发扩展点 自定义插件模板 public class CustomPlugin extends PluginAdapter { @Override public boolean validate(List\u003cString\u003e warnings) { // 插件验证逻辑 return true; } @Override public boolean clientGenerated(Interface interfaze, IntrospectedTable introspectedTable) { // 向现有Mapper接口添加方法 interfaze.addMethod(createCustomMethod(introspectedTable)); return super.clientGenerated(interfaze, introspectedTable); } private Method createCustomMethod(IntrospectedTable table) { // 自定义方法生成逻辑 Method method = new Method(\"customMethod\"); method.setVisibility(JavaVisibility.PUBLIC); method.setAbstract(true); return method; } } 配置驱动行为 @Override public boolean validate(List\u003cString\u003e warnings) { // 读取插件属性 String enableFeature = getProperties().getProperty(\"enableCustomFeature\"); if (\"false\".equals(enableFeature)) { // 跳过插件执行 return false; } return true; } 这种深入的源码分析揭示了每个插件背后的精妙工程，展示了先进的Java代码生成技术和MyBatis Generator的可扩展性框架。 本技术深度解析旨在为MyBatis Generator自定义插件的架构和实现提供全面的洞察。 ","date":"2025-09-24","objectID":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/:9:6","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator 自定义插件","uri":"/zh-cn/2025/09/8.-mybatis-generator-custom-plugins/"},{"categories":["Customs","IDE"],"content":"[Customs] 1. IntelliJ IDEA 配置与推荐插件 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:0:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"介绍 作为目前最强大的Java IDE之一，IntelliJ IDEA可以通过合理的配置和精心选择的插件得到显著增强。本指南提供了一套精选的必备插件和配置技巧，将彻底改变您的开发体验。 我们的目标是提供即用型配置和插件，立即提升开发效率、代码质量和开发工作流程。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:1:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"必备插件 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🔧 开发工具 CamelCase (3.0.12) 用途: 变量名和字符串的快速大小写转换工具。 功能特性: 在camelCase、PascalCase、snake_case和SCREAMING_SNAKE_CASE之间转换 快捷键: Shift + Alt + U 重构和代码一致性的必备工具 支持多选进行批量转换 使用场景: 将数据库列名转换为Java字段名 适配不同编码标准之间的命名约定 代码评审期间快速文本格式化 Maven Helper (4.23.222.2964.0) 用途: 增强的Maven项目管理和依赖分析。 核心功能: 可视化依赖树和冲突解决 轻松排除传递依赖 快速Maven目标执行 带搜索功能的依赖分析器 优势: 快速解决依赖冲突 理解项目依赖结构 通过识别未使用的依赖优化构建性能 RestfulToolkit-fix (2.0.8) 用途: RESTful API开发辅助工具。 功能特性: 快速导航到REST端点 从控制器方法生成HTTP请求 API文档集成 IDE内的请求/响应测试 工作流程增强: 从URL跳转到控制器方法 无需外部工具即可测试API端点 与代码同步维护API文档 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:1","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🎨 视觉增强 Atom Material Icons 用途: 美观的文件和文件夹图标，改善视觉组织效果。 功能特性: 现代、色彩丰富的图标集 特定语言的文件图标 框架和库识别 可自定义图标主题 Pokemon Progress 用途: 有趣的宝可梦主题进度条。 功能特性: 用宝可梦角色替换枯燥的进度条 提供多种宝可梦主题 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:2","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🛠️ 生产力工具 Grep Console 用途: 高级控制台输出过滤和高亮显示。 功能特性: 使用正则表达式模式实时日志过滤 日志级别和模式的彩色编码 保存和重用过滤器配置 具有不同过滤器的多个控制台标签 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:3","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🌐 API开发 Apipost 用途: 在IntelliJ IDEA内进行API测试和文档编写。 功能特性: 创建和执行HTTP请求 使用建议: 仅用于临时测试 复杂API测试仍建议使用Postman ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:4","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"📊 图表与设计工具 PlantUML 用途: 使用基于文本的语法创建专业UML图表。 核心功能: 具有即时更新的实时图表预览 全面的图表类型：序列图、类图、活动图、用例图、组件图、部署图、状态图等 导出为多种格式(PNG、SVG、PDF、LaTeX) 与代码文档和注释集成 版本控制友好(基于文本的源码) Excalidraw 用途: 手绘风格图表，用于头脑风暴和创意设计。 核心功能: 直观的拖放界面 手绘美学效果，图表更易接受 实时协作白板功能 丰富的形状和元素库 导出为多种格式(PNG、SVG、JSON) ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:2:5","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"必备IDE技巧与配置 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:3:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🚀 自动化设置 提交时代码格式化 配置: 在提交时启用自动代码格式化 设置路径: VCS → Git → 启用\"重新格式化代码\"和\"优化导入\" 局部变量Final增强 配置: 局部变量的自动final修饰符 设置: Editor → Inspections → Java → Code Style → Local variable or parameter can be final ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:3:1","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"⌨️ 生产力快捷键 高级光标操作 基本快捷键: Alt + Click: 在点击位置添加光标 Alt + Shift + Click: 创建矩形选择 Ctrl + Alt + Shift + J: 选择所有出现的地方 Alt + J: 选择下一个出现的地方 工作流程增强: 多行编辑效率 批量文本替换 同时修改代码 自定义动态模板 热门自定义模板: .str → String.valueOf($VAR$) .not → !$VAR$ .nn → if ($VAR$ != null) .null → if ($VAR$ == null) 设置路径: Editor → Live Templates → 创建新模板组 生产力优势: 更快的常用代码模式 减少输入和语法错误 团队间一致的编码模式 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:3:2","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🎯 代码质量设置 导入优化 配置: 防止通配符导入 设置路径: Editor → Code Style → Java → Imports 将\"使用’*‘导入的类数量\"设置为999 将\"使用静态’*‘导入的名称数量\"设置为999 优势: 明确的导入声明 避免命名冲突 更小的JAR文件大小 更好的IDE性能 文件头模板 配置: 带有作者和日期的自动文件头 模板示例: /** * TODO: 添加类描述 * * @author ${USER} * @version 1.0, ${DATE} * @since 1.0.0 */ 设置路径: Editor → File and Code Templates → Includes 行尾一致性 配置: 确保文件以换行符结尾 设置路径: Editor → General → On Save → \"确保每个保存的文件都以换行符结尾\" 优势: 跨操作系统的一致文件结尾 与命令行工具更好的兼容性 更清晰的git差异显示 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/7.-customs-idea/:3:3","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA 配置与推荐插件","uri":"/zh-cn/2025/09/7.-customs-idea/"},{"categories":["Java"],"content":"[Java] 2. Unit Test 基本用法 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:0:0","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"Mocikto基本用法 在单元测试里,很多测试(除Util类)都需要mock掉一些服务来保证只测试当前想测的内容. 具体使用时,需要先mock一个对象,然后再mock此对象的方法,然后就可以使用mock的方法去测想测的逻辑了. ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:1:0","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"Mock对象 首先,需要在Test类里声明需要mock的接口/实现类. 如 @MockBean private IOssService ossService; 有时候,也需要直接手动mock一个东西出来,比如,当需要mock掉redis的操作时,可以 RSet\u003cLong\u003e redisSet = Mockito.mock(RSet.class); 注意此操作不要去mock基本类型,如int,long等. 还有一种方式是使用@SpyBean. 此处先略过,后面的部分会介绍. ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:1:1","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"Mock方法 假定有一个接口 public interface IUserService { Long add(UserDTO dto); void remove(Long userId); Optional\u003cUserDTO\u003e find(String username); Optional\u003cUserDTO\u003e find(Long userId); } 在已经mock掉userService的情况下 @MockBean private IUserService userService; 我们可以mock掉add这个方法,使得任意参数传过来都直接返回100. Mockito.doReturn(100L).when(userService).add(any()); 也可以这样 Mockito.when(userService.add(any())).thenReturn(100L); 但是, 对同一service, 这两种方式不要混用, 否则有时会出现第二种mock无效的情况. 当你发现mock的方法无效,或者有些莫名其妙的错误时,请统一使用一种mock的方式. 如果还不行,请切换到另一种方式. 当你需要mock掉void的方法时,可以 Mockito.doNothing().when(userService).remove(anyLong()); 如果你需要模拟出错的情况,可以 Mockito.doThrow(...).when(userService).remove(anyLong()); 当你需要对一些特殊数据mock,如userId = 1,有用户, userId = 2, 没有用户,你可以这样mock: Mockito.doReturn(Optional.of(UserDTO.builder().build())).when(userService).find(1L); Mockito.doReturn(Optional.empty()).when(userService).find(2L); mock还有更泛用的方式,如果你想要在userId \u003c 10的时候返回数据,其他情况不返回: Mockito.doAnswer(invocation -\u003e { Long userId = (Long) invocation.getArguments()[0]; if (userId \u003c 10L) { return UserDTO.builder().id(userId).build(); } return null; }).when(userService).find(anyLong()); 或者 Mockito.when(userService.find(anyLong())).thenAnswer(invocation -\u003e { Long userId = (Long) invocation.getArguments()[0]; if (userId \u003c 10L) { return UserDTO.builder().build(); } return null; }); 需要注意的是,doAnswer也可以mock掉void返回值,假如我现在要mock掉redis的操作: // mock redis的get和set操作. RAtomicLong mockValue = Mockito.mock(RAtomicLong.class); doAnswer(invocation -\u003e { Long newValue = (Long) (invocation.getArguments()[0]); doReturn(true).when(mockValue).isExists(); doReturn(newValue).when(mockValue).get(); return null; }).when(mockValue).set(anyLong()); 这个例子里,在调用redis的set时,我们把get也给mock掉了,而且返回的是他刚刚set的值. 这样就可以模拟redis的操作. 当你mock一些有重载的方法时,会有出错的情况,比如 Mockito.when(userService.find(any())).thenReturn(Optional.empty()); 此时的find(any())可以把两个方法都匹配上, 这个时候就会有问题. 解决方法就是 Mockito.when(userService.find(anyLong())).thenReturn(Optional.empty()); Mockito.when(userService.find(anyString())).thenReturn(Optional.empty()); 在使用any()时一定要注意 不要用any()去代表long,int这种值,否则会有NPE. 可具体用any(class)去对应具体的输入,如any(LocalDateTime.class)即可代表任意LocalDateTime的参数. 在使用mock时,你还可以doCallRealMethod()或者thenCallRealMethod()调用原来的实现,但我个人并不推荐这么搞,一般情况都会有各种各样的问题. 具体解决方案会在后面写. ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:1:2","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试结果 一般在你mock了所有调用的方法,然后使用assert工具验证了输出时,程序基本就是按照预想的方式在跑了. public interface IUserBizService { UserDTO reg(UserDTO dto); } 比如你正在测试这个reg方法,Mockito工具可以验证是否真的调用了一次userService.add()方法: Mockito.verify(userService, times(1)).add(any()); 然而有些时候,你还需要检查一些其他的东西,比如你mock掉的方法的调用参数. 这个时候就需要用到Mockito的工具了 ArgumentCaptor\u003cUserDTO\u003e captor = ArgumentCaptor.forClass(UserDTO.class); Mockito.verify(userService, times(1)).add(captor.capture()); List\u003cUserDTO\u003e users = captor.getAllValues(); // 此处即可验证users的内容 assertEquals(......); 还有一种简单写法 Mockito.verify(userService, times(1)).add((UserDTO) argThat(u -\u003e { assertEquals(userId, t.getId()); ... return true; })); ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:1:3","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"SpyBean示例 在上面的例子里,都是使用的MockBean,这种mock的方式用处最广. 但有些时候,测试需要深入到实现类里,修改一些逻辑,然后再测,这时候就需要用到SpyBean了. MockBean相当于你完完全全的mock了一个类,这个类里所有的方法都是你mock的,都没办法直接调用,必须先mock SpyBean相当于你拿了个真的实现出来,然后你可以只mock其中的一部分方法. 举个例子,比如在IUserService的实现里,有这样一段 public class UserServiceImpl implements IUserService { @Override public void remove(Long userId) { Optional\u003cUserDTO\u003e dto = this.find(userId); // 注意这一行 if (!dto.isPresent()) { throw new Error(404); } ... } } 此时remove方法调用了find方法,如果你想测试这个remove方法, mock掉整个实现类显然是不可行的, 而如果你不mock, 这个find的方法返回值就不可控. 如果你直接往数据库里放数据,那又不是完全意义上的单元测试,因为你使用了外部的服务. 此时就该使用SpyBean了. 你可以只mock掉find,然后直接调用remove,就可以执行测试. @SpyBean private IUserService userService; 同样的,还有一些不是很好测试的逻辑,如 public void doJob() { while(true) { if (xxx) { break; } doWork(); sleep(); } } public void sleep() { try { Thread.sleep(2000); } catch (...) { ... } } 你需要测试这个while循环在按照你的思路运行,但这个sleep方法会真的sleep,这会让整个测试变得巨麻烦. 这时候用SpyBean就很好解决. doNothing().when(xxxService).sleep(); xxxService.doJob(); verify(xxxService, times(1)).sleep(); verify(xxxService, times(1)).doWork(); 同理,当你需要测试一些和时间相关的操作,这部分逻辑很关键,但和当前时间紧密相关, 这时候也需要用SpyBean. public void doJob() { LocalDateTime now = getNow(); ... } public LocalDateTime getNow() { return DateUtil.now(); } mock操作: doReturn(LocalDateTime.of(2021, 10, 1)).when(xxxService).getNow(); xxxService.doJob(); ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:1:4","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试用例规范 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:0","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试用例命名规则 待测试方法startTask，则可能存在以下测试用例 startTask_noPerm startTask_banned startTask_succeed startTask_limited … javaguide.html#s5.2.3-method-names ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:1","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试用例注释规则 1 测试场景说明 2 预期结果，实际结果 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:2","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试范围规则 使用Mockito隔离测试边界 使用Postman做集成测试 控制层也要有测试用例 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:3","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"代码覆盖率 最大化代码覆盖率 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:4","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java"],"content":"测试用例规范 构建测试数据 使用测试数据构建mock方法 执行方法 验证mock结果 要点 单测用例要体现单元的概念，在构建测试数据时应注重构建数据之间足够单元和隔离 简而言之，单测代码也需要足够优雅，可扩展性高，后续发生业务修改时才会更好地进行测试的扩展 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/6.-unit-test/:2:5","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test 基本用法","uri":"/zh-cn/2025/09/6.-unit-test/"},{"categories":["Java","Spring"],"content":"[Spring] 1. spring web CompletionStage 浅谈 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:0:0","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"介绍 spring-web里对异步的支持做的很好，可以通过异步返回的形式，做许多优化 提高吞吐量 精细调控各业务执行线程池 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:1:0","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"样例说明 /** * async interface controller * * @author Goody * @version 1.0, 2024/9/19 */ @RestController @RequestMapping(\"/goody\") @RequiredArgsConstructor @Slf4j public class GoodyAsyncController { private static final AtomicInteger COUNT = new AtomicInteger(0); private static final Executor EXECUTOR = new ThreadPoolExecutor( 10, 10, 10, TimeUnit.SECONDS, new ArrayBlockingQueue\u003c\u003e(10), r -\u003e new Thread(r, String.format(\"customer-t-%s\", COUNT.addAndGet(1))) ); @GetMapping(\"async/query1\") public CompletionStage\u003cString\u003e asyncQuery1() { log.info(\"async query start\"); return CompletableFuture.supplyAsync(() -\u003e { log.info(\"async query sleep start\"); ThreadUtils.sleep(1000); log.info(\"async query sleep done\"); log.info(\"async query done\"); return \"done\"; }, EXECUTOR); } @GetMapping(\"sync/query1\") public String syncQuery1() throws InterruptedException { log.info(\"sync query start\"); final CountDownLatch latch = new CountDownLatch(1); EXECUTOR.execute(() -\u003e { log.info(\"sync query sleep start\"); ThreadUtils.sleep(1000); log.info(\"sync query sleep done\"); latch.countDown(); }); latch.await(); log.info(\"sync query done\"); return \"done\"; } } 定义了一个自定义的线程池，用于异步状态下使用 这里一个同步，一个异步，可以看下具体的请求情况 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:2:0","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"单次请求 请求异步接口 curl –location ‘127.0.0.1:50012/goody/async/query1’ 2024-09-19 15:56:43.408 INFO 24912 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 15:56:43.411 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 15:56:44.417 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 15:56:44.417 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query done 请求同步接口 curl –location ‘127.0.0.1:50012/goody/sync/query1’ 2024-09-19 16:03:00.916 INFO 25780 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : sync query start 2024-09-19 16:03:00.917 INFO 25780 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : sync query sleep start 2024-09-19 16:03:01.924 INFO 25780 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : sync query sleep done 2024-09-19 16:03:01.924 INFO 25780 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : sync query done 分析 其实从一个单独请求的例子里，看出来的差别不大。不过我们先一步一步分析 从异步接口里来看，从CompletableFuture接手之后，全部都是自定义线程池处理，全部交给spring-web框架进行拆包的处理 从同步接口里来看，从CompletableFuture接手之后，spring-web的线程进行了等待，其实可以推理出来此时是同步等待的。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:2:1","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"10并发请求 java程序已经设置web线程=1，自定义业务线程=10 请求脚本 import threading import requests import datetime def get_current_time(): current_time = datetime.datetime.now() return current_time.strftime(\"%Y-%m-%d %H:%M:%S\") url = \"http://127.0.0.1:50012/goody/async/query1\" num_threads = 10 def send_request(): response = requests.get(url) print(f\"{get_current_time()} Request finished with status code: {response.status_code}\") threads = [] for _ in range(num_threads): thread = threading.Thread(target=send_request) threads.append(thread) thread.start() # 等待所有线程完成 for t in threads: t.join() 请求异步接口 java输出 2024-09-19 16:11:19.983 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.986 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.991 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.992 INFO 11712 --- [ customer-t-2] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.992 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.993 INFO 11712 --- [ customer-t-3] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.993 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.994 INFO 11712 --- [ customer-t-4] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.994 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.995 INFO 11712 --- [ customer-t-5] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.995 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.996 INFO 11712 --- [ customer-t-6] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.997 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.997 INFO 11712 --- [ customer-t-7] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.997 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.998 INFO 11712 --- [ customer-t-8] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.998 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.999 INFO 11712 --- [ customer-t-9] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.999 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:20.000 INFO 11712 --- [ customer-t-10] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:20.989 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:20.989 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-2] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-8] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-6] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-10] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-9] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-7] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-9] c.g.u.j.controller.Goo","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:2:2","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"源码分析 protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // =========================================== // Determine handler for the current request. // =========================================== mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { noHandlerFound(processedRequest, response); return; } // =========================================== // Determine handler adapter for the current request. // =========================================== HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) \u0026\u0026 isGet) { return; } } // =========================================== // 前置处理 // =========================================== if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // =========================================== // Actually invoke the handler. // asyncManager.isConcurrentHandlingStarted() = false // org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter#handleInternal // org.springframework.web.servlet.mvc.method.annotation.DeferredResultMethodReturnValueHandler // =========================================== mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); // =========================================== // asyncManager.isConcurrentHandlingStarted() = true // =========================================== // =========================================== // 如果是异步返回，这里就会是true，下文的后置处理不会执行 // =========================================== if (asyncManager.isConcurrentHandlingStarted()) { return; } applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion if (mappedHandler != null) { // =========================================== // 实际增加的就是以下的后置处理，用于唤醒后，把上下文恢复 // org.springframework.boot.actuate.metrics.web.servlet.LongTaskTimingHandlerInterceptor // org.springframework.web.servlet.handler.ConversionServiceExposingInterceptor // org.springframework.web.servlet.resource.ResourceUrlProviderExposingInterceptor // =========================================== mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } } 这里是非常spring的写法 前置装载若干处理器。 执行前置处理器 开始执行 执行后置处理器 不过不同的是，里面的一个处理器DeferredResultMethodReturnValueHandler会判断结果是否是异步。当是异步的时候，会直接短路下文后置处理器逻辑。 @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, Mo","date":"2025-09-21","objectID":"/zh-cn/2025/09/5.-spring-web-completionstage/:3:0","tags":["Java","Spring","web"],"title":"[Spring] 1. spring web CompletionStage 浅谈","uri":"/zh-cn/2025/09/5.-spring-web-completionstage/"},{"categories":["Java"],"content":"[Java] 1. Lombok ","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:0:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["Java"],"content":"简介 Lombok是一款Java库，它可以自动为Java类生成一些重复性的代码，如 getter、setter、equals 和 hashCode 等方法。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:1:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["Java"],"content":"原理 Lombok 的工作原理是基于注解处理器 （Annotation Processor）和Java Compiler API 。 当 Lombok 被编译器发现时，它会使用注解处理器来修改 Java 代码。在这个过程中，Lombok 会检查类中的特定注解，并根据这些注解生成相应的代码，如 getter、setter 等方法。 具体来说，Lombok 使用的是 Apache BCEL（Bytecode Engineering Library）库来直接操作 Java 字节码，而不是通过反射或运行时操作。这样一来，Lombok 可以在编译期就进行修改，从而提高性能和效率。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:2:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["Java"],"content":"注解 自定义一个纯净类Node，编译后如下 public class Node { private Long item1; private String item2; } public class Node { private Long item1; private String item2; public Node() { } } 解释一下，Java编译器会自定给一个无参构造器，因为任何类不能没有构造器 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:3:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["Java"],"content":"实体类 @Data \u0026 @EqualsAndHashCode \u0026 @Getter \u0026 @Setter \u0026 @ToString @Data public class Node { private Long item1; private String item2; } 我们最常用的@Data，其实是一些基本注解的集合，比如@Getter、@Setter、@EqualsAndHashCode、@ToString public class Node { private Long item1; private String item2; public Node() { } public Long getItem1() { return this.item1; } public String getItem2() { return this.item2; } public void setItem1(Long item1) { this.item1 = item1; } public void setItem2(String item2) { this.item2 = item2; } public boolean equals(Object o) { if (o == this) { return true; } else if (!(o instanceof Node)) { return false; } else { Node other = (Node)o; if (!other.canEqual(this)) { return false; } else { Object this$item1 = this.getItem1(); Object other$item1 = other.getItem1(); if (this$item1 == null) { if (other$item1 != null) { return false; } } else if (!this$item1.equals(other$item1)) { return false; } Object this$item2 = this.getItem2(); Object other$item2 = other.getItem2(); if (this$item2 == null) { if (other$item2 != null) { return false; } } else if (!this$item2.equals(other$item2)) { return false; } return true; } } } protected boolean canEqual(Object other) { return other instanceof Node; } public int hashCode() { int PRIME = true; int result = 1; Object $item1 = this.getItem1(); result = result * 59 + ($item1 == null ? 43 : $item1.hashCode()); Object $item2 = this.getItem2(); result = result * 59 + ($item2 == null ? 43 : $item2.hashCode()); return result; } public String toString() { return \"Node(item1=\" + this.getItem1() + \", item2=\" + this.getItem2() + \")\"; } } 这里介绍一些基本的进阶用法 @Getter @Setter public class Node { @Getter(AccessLevel.PRIVATE) private Long item1; @Setter(AccessLevel.PRIVATE) private String item2; } 可以看到，设置对应的PRIVATE权限的方法变成了private。这样有利于做一些权限的集中 public class Node { private Long item1; private String item2; public Node() { } public String getItem2() { return this.item2; } public void setItem1(Long item1) { this.item1 = item1; } private Long getItem1() { return this.item1; } private void setItem2(String item2) { this.item2 = item2; } } @AllArgsConstructor \u0026 @NoArgsConstructor \u0026 @RequiredArgsConstructor @NoArgsConstructor @AllArgsConstructor public class Node { private Long item1; private String item2; } 很明显会生成无参构造器，全参构造器。必须参构造器，会与无参构造器产生冲突，所以另外展示。 public class Node { private Long item1; private String item2; public Node() { } public Node(Long item1, String item2) { this.item1 = item1; this.item2 = item2; } } 必须参构造器 @RequiredArgsConstructor @AllArgsConstructor public class Node { private final Long item1; private String item2; } public class Node { private final Long item1; private String item2; public Node(Long item1) { this.item1 = item1; } public Node(Long item1, String item2) { this.item1 = item1; this.item2 = item2; } } 这里最经典的组合如下 @Service @Slf4j @RequiredArgsConstructor public class Node { private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; } 输出如下 @Service public class Node { private static final Logger log = LoggerFactory.getLogger(Node.class); private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; public Node(Long item1, Long item2, Long item3) { this.item1 = item1; this.item2 = item2; this.item3 = item3; } } 可以看到，这里原本可以初始化的bean，依然可以初始化，有冲突或者需要lazy的，依然会遵循lazy 进阶一下 @Service @Slf4j @RequiredArgsConstructor(onConstructor_ = @JsonCreator) public class Node { private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; } 此时输出的构造器，就会带上对应的注解。这里的语法也是Java自带的 另外有个点要提一下，Json序列化和反序列化的原理是先构造对象，再set对象，所以必须要有无参构造和setter。 @Service public class Node { private static final Logger log = LoggerFactory.getLogger(Node.class); private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; @JsonCreator public Node(Long item1, Long item2, Long item3) { this.item1 = item1; this.item2 = item2; this.it","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:3:1","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["Java"],"content":"如何自定义 lombok的原理是通过Java自己带的编译API，自己进行的增强，所以我们按照这个思路也去增强即可。 核心是继承并实现类javax.annotation.processing.AbstractProcessor 然后通过该类的process方法，修改内部的JavacTrees相关变量即可。 我是已经写了一个通用图的基类，简单介绍下 public abstract class BaseProcessor\u003cT extends Annotation\u003e extends AbstractProcessor { /** annotation type */ protected Class\u003cT\u003e clazz; /** javac trees */ protected JavacTrees trees; /** AST */ protected TreeMaker treeMaker; /** mark name */ protected Names names; /** log */ protected Messager messager; /** filer */ protected Filer filer; /** the jcTrees generated by annotation to add */ protected List\u003cJCTree\u003e annotationJCTrees; @Override public synchronized void init(ProcessingEnvironment processingEnv) { super.init(processingEnv); // transfer type T to Class final Type superclass = getClass().getGenericSuperclass(); if (superclass instanceof ParameterizedType) { this.clazz = (Class\u003cT\u003e) ((ParameterizedType) superclass).getActualTypeArguments()[0]; } else { this.clazz = null; } this.trees = JavacTrees.instance(processingEnv); this.messager = processingEnv.getMessager(); this.filer = processingEnv.getFiler(); final Context context = ((JavacProcessingEnvironment) processingEnv).getContext(); this.treeMaker = TreeMaker.instance(context); this.names = Names.instance(context); // init list annotationJCTrees = List.nil(); } /** * {@inheritDoc} */ @Override public final boolean process(Set\u003c? extends TypeElement\u003e annotations, RoundEnvironment roundEnv) { roundEnv.getElementsAnnotatedWith(this.clazz) .stream() .map(element -\u003e trees.getTree(element)) // NOTE(goody): 2022/5/5 // tree is the class input. Modify the `JCTree` to modify the method or argus // `visitClassDef` runs after than `visitAnnotation`, so method `visitAnnotation` can add `annotationJCTrees` to // `annotationJCTrees`. `visitClassDef` will prepend all .forEach(tree -\u003e tree.accept(new TreeTranslator() { @Override public void visitClassDef(JCTree.JCClassDecl jcClassDecl) { // NOTE(goody): 2022/5/4 https://stackoverflow.com/questions/46874126/java-lang-assertionerror-thrown-by-compiler-when-adding-generated-method-with-pa // setMethod var is a new Object from jcVariable, the pos should be reset to jcClass treeMaker.at(jcClassDecl.pos); // generate the new method or variable or something else final List\u003cJCTree\u003e jcTrees = generate(jcClassDecl); jcClassDecl.defs = jcClassDecl.defs.prependList(jcTrees); // add all elements in `annotationJCTrees` jcClassDecl.defs = jcClassDecl.defs.prependList(annotationJCTrees); super.visitClassDef(jcClassDecl); } @Override public void visitMethodDef(JCTree.JCMethodDecl jcMethodDecl) { if (isModify(jcMethodDecl)) { super.visitMethodDef(modifyDecl(jcMethodDecl)); } else { super.visitMethodDef(jcMethodDecl); } } @Override public void visitVarDef(JCTree.JCVariableDecl jcVariableDecl) { if (isModify(jcVariableDecl)) { super.visitVarDef(modifyDecl(jcVariableDecl)); } else { super.visitVarDef(jcVariableDecl); } } @Override public void visitAnnotation(JCTree.JCAnnotation jcAnnotation) { super.visitAnnotation(jcAnnotation); final JCTree.JCAssign[] jcAssigns = jcAnnotation.getArguments() .stream() .filter(argu -\u003e argu.getKind().equals(Tree.Kind.ASSIGNMENT)) .map(argu -\u003e (JCTree.JCAssign) argu) .toArray(JCTree.JCAssign[]::new); if (jcAssigns.length \u003e 0) { annotationGenerateJCTree(handleJCAssign(List.from(jcAssigns))); } } })); return true; } /** * subclass should implement this method to add method or variable or others * * @param jcClassDecl jcClassDecl * @return new JCTree list */ private List\u003cJCTree\u003e generate(JCTree.JCClassDecl jcClassDecl) { final JCTree[] trees = generate() .toArray(JCTree[]::new); // method Trees final JCTree[] methodTrees = jcClassDecl.defs .stream() .filter(k -\u003e k.getKind().equals(Tree.Kind.METHOD)) .map(tree -\u003e (JCTree.JCMethodDecl) tree) .flatMap(jcMethodDecl -\u003e handleDecl(jcMethodDecl)) .toArray(JCTree[]::new); // variable trees final JCTree[] variableTrees = jcClassDecl.defs .stream() .filter(k -\u003e k.getKind().equals(Tree.Kind.VARIABLE)) .map(tree -\u003e (JCTr","date":"2025-09-21","objectID":"/zh-cn/2025/09/4.-lombok/:4:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/zh-cn/2025/09/4.-lombok/"},{"categories":["MySQL"],"content":"[MySQL] 1. 浅谈 MySQL 快速查询 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:0:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"讲在开头 在最开始先举几个我们常用的在平时学习、业务上最常见的优化措施 单位时间内更多的事情 快排使用二分的思想，单次循环内对多个数组进行排序 总量查更少的信息 KMP 算法对主串进行预处理，做到减少匹配次数。这就是逻辑的力量 搜索树通过二分的思想，每次过滤剩余数据的一半来提高效率。这就是数据结构的力量 对于 MySQL 的快速查询，最为关键的核心就是查询数据量少，越少越快。整篇文章均围绕该句进行展开。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:1:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"没有索引是否一定会慢？ 定义插入数据函数 DROP TABLE IF EXISTS user; CREATE TABLE user ( id bigint(20) NOT NULL COMMENT '用户id', biz_id bigint(20) NOT NULL COMMENT '业务id', message text COMMENT '业务信息', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT '用户' CHARSET = utf8mb4; DELIMITER $$ CREATE PROCEDURE insertUserData(IN start_id int, IN end_id int, IN bizId int) BEGIN DECLARE i int DEFAULT start_id; WHILE i \u003c= end_id DO INSERT INTO user (id, biz_id, message) VALUES (i, bizId, SUBSTRING(MD5(RAND()), 1, 8)); SET i = i + 1; END WHILE; END $$ DELIMITER ; 准备数据若干，bizId 为 1 的只有两条，第一条和最后一条数据 -- 插入第一条业务id1数据 CALL insertUserData(1, 1, 1); -- 插入一百万万条其他数据 CALL insertUserData(2, 1000000, 2); -- 插入第二条业务id1数据 CALL insertUserData(1000001, 1000001, 1); 其实看数据分布，我想大家已经知道我想表达的，即便不使用索引。我们这里也是有快速查询的场景。 查询 limit1 EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ALL null null null null 997030 10 Using where SELECT * FROM user WHERE biz_id = 1 LIMIT 1; [2023-08-27 10:53:43] 1 row retrieved starting from 1 in 97 ms (execution: 5 ms, fetching: 92 ms) SELECT * FROM user WHERE biz_id = 1 LIMIT 2; [2023-08-27 10:53:48] 2 rows retrieved starting from 1 in 1 s 199 ms (execution: 1 s 172 ms, fetching: 27 ms) EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ALL null null null null 997030 10 Using where 从实际出发，我们可以看到，limit1 和 limit2 的执行时间相差巨大。 这里先分享一个技巧，我们可以使用 chatgpt 来高效整理分析 explain。 (文心一言) 这个查询计划显示了一个对user表的完整扫描，没有使用索引，并且WHERE子句用于筛选结果。 如果表很大或者查询的性能低，这可能会导致性能问题。 优化可能需要考虑使用合适的索引来提高查询性能。 我们可以看到，limit1 和 limi2 中，执行计划是几乎完全一致的。这个可以肯定，两次虽然时间相差巨大，但是对于 MySQL 来说执行逻辑相同。那为什么会相差巨大呢？就需要我们来具体分析。 在 limit1 的场景中，基于全表扫描，在第一条数据立即返回。等于仅查询了一条数据。 在 limit2 的场景中，第二条数据在百万条之后，所以根据执行计划，必须要扫描到全表才可以完整找到数据。 综上，没有索引就一定慢么？不一定的，索引仅仅是减少了数据查询量，但数据量本就极少的情况。是不会更慢的。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:2:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"索引 explain 里使用 where，根据 chatgpt 的回复，以及自己的分析。我们很自然地能想到索引来进行查询的优化。使用 B+树索引进行尝试。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:3:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"B+树索引 https://www.cs.usfca.edu/~galles/visualization/Algorithms.html CREATE INDEX idx_user_biz_id ON user (biz_id); DROP INDEX idx_user_biz_id ON user; EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 2 100 null SELECT * FROM user WHERE biz_id = 1 LIMIT 1 [2023-08-27 21:09:37] 1 row retrieved starting from 1 in 48 ms (execution: 4 ms, fetching: 44 ms) EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 2 100 null SELECT * FROM user WHERE biz_id = 1 LIMIT 2 [2023-08-27 21:09:04] 2 rows retrieved starting from 1 in 48 ms (execution: 4 ms, fetching: 44 ms) 可以看到，增加索引之后，查询时间极大优化。执行计划从 where 进化为了使用 const (文心一言) 在你的例子中，这个查询对名为\"user\"的表进行查询 使用了名为\"idx_user_biz_id\"的索引，索引长度为8字节 比较列是常量（const），系统估算需要扫描2行数据，返回的行占总行的100%，没有其他额外信息。 为什么是 B+树 平衡二叉树 在平衡二叉树中，我们可以思考一下查询的过程。 假设要查询 5~16 的所有数据。需要查询六个节点，且查出来的顺序非天然顺序排序，需要中序遍历的算法实现才可以。所以我们可以看到，平衡树是查找单个节点的天然数据结构，对于范围查询的支持相对较差 第一个待优化点，如果要细致理解，需要深入到磁盘预读取的优化。我认为理解必要不大，可以简单理解为 IO 次数过多 平衡树，逻辑较近的地方，可能物理距离较远，导致在磁盘旋转操作中 IO 时间较长 第二个待优化点，在 IO 读取数据时，会有过多的无用数据。因为读取数据回来必须大于单个节点数据的大小。 另外平衡二叉树，保持平衡需要旋转，在内存操作中比较难以高效实现 B 树 针对上述优化点，我们可以很自然地将二叉往多叉的方向优化，这样优化了单个节点过多无用数据的问题。因为单个节点较大的时候，IO 次数也降了下来 这样 B 树的思想可以很自然地出现。通过扩大节点形成多叉结构 但其实仔细想来，中序遍历算法的实现依然需要。对于范围查询仅优化在单个节点上，当范围跨度较大的时候，性能依然有可优化空间 B+树 B+树首先更加优化了单个节点的存储效率(没有了 Value)。这样单个节点可以支持的叉更多了 同时最底层的叶子节点通过指针组合成为链表，从而优化遍历逻辑。 注: B+树的最底层是包含上层所有节点的 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:3:1","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"B+树联合索引 B+树的部分已经了解了，可以简单降之前的 B+树理解为聚簇索引，Value 在最底层的叶子节点 那么我们开始理解联合索引 最左匹配原则 具体原则，不需要讲述，理解即可 B+树索引为什么是排序的？ 在上图中，我们一级索引是顺序的，这个很好理解，当一级索引固定时，其下的二级索引也是顺序的。 我的理解是，B+树先搜索一级索引。当一级索引确定后，B+树转化为二级索引的 B+树。依次类推 注：联合索引在一颗树上 B+树索引为什么失效？ 首先我们可以理解一级索引为 1 的二级索引顺序和一级索引为 2 的二级索引顺序是没有必然先后顺序的。只是都具有相同的单调性而已 顺着我之前的索引数变化的方向去理解，当一级索引成为一个范围之后，二级索引不止一颗了，我们没有办法在多颗树上进行相同的范围单次查询。只能多次组合 什么是回表？ 默认大家了解聚簇索引和二级索引 回表通俗地说就是，二级索引走完玩不下去了，比如之前的表 注: Type 表示访问类型的优化顺序，从最好到最差的顺序如下：system-\u003econst-\u003eeq_ref-\u003eref-\u003eref_or_null-\u003eindex_merge-\u003eunique_subquery-\u003eindex_subquery-\u003erange-\u003eindex-\u003eall。一般来说，我们希望达到 ref 和 eq_ref 级别，范围查找需要达到 range 级别。 DROP TABLE IF EXISTS user; CREATE TABLE user ( id bigint(20) NOT NULL COMMENT '用户id', biz_id bigint(20) NOT NULL COMMENT '业务id', message text COMMENT '业务信息', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT '用户' CHARSET = utf8mb4; CREATE INDEX idx_user_biz_id ON user (biz_id); 完全使用索引 EXPLAIN SELECT id FROM `user` WHERE biz_id = 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 100 Using index (chatgpt) 查询语句涉及的表是\"user\"表。 表的访问方式为\"ref\"，表示使用了索引进行查询。 使用的索引为\"idx_user_biz_id\"，并且查询使用了该索引。 使用的索引长度为8个字节。 查询的参考值为\"const\"，表示查询使用了一个常量值进行筛选。 查询返回的行数为498380行，过滤率为100%。 \"Using index\"表示查询使用了覆盖索引，即查询结果可以直接从索引中获取，不需要再去访问表数据。 回表一次 EXPLAIN SELECT * FROM `user` WHERE biz_id = 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 100 null (chatgpt) 该查询是一个简单的SELECT查询，没有子查询或联接。 查询引用了一个名为user的表。 查询使用了名为idx_user_biz_id的索引。 索引的长度为8。 查询使用了一个常量值进行参考。 查询扫描了498380行，并根据WHERE条件过滤了100行。 查询没有其他特殊情况。 回表一次(回表数据较多) EXPLAIN SELECT * FROM `user` WHERE biz_id = 2 and message = ''; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 10 Using where (chatgpt) 给定的查询是一个简单的SELECT查询，涉及到名为\"user\"的表。 查询使用了名为\"idx_user_biz_id\"的索引，并且使用了WHERE子句。 查询使用了普通的二级索引进行相等匹配。 查询返回了498380行的结果。 索引下推 \u0026 覆盖索引 DROP INDEX idx_user_biz_id ON user; CREATE INDEX idx_user_biz_id_msg ON user (biz_id, message(8)); EXPLAIN SELECT id FROM user WHERE biz_id = 2 and message = '99dd8a31'; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id_msg idx_user_biz_id_msg 43 const,const 1 100 Using where EXPLAIN SELECT * FROM user WHERE biz_id = 2 and message = '99dd8a31'; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id_msg idx_user_biz_id_msg 43 const,const 1 100 Using where (chatgpt) type 字段表示查询的访问方式，从最优到最差的顺序依次是：system, const, eq_ref, ref, ref_or_null, index_merge, unique_subquery, index_subquery, range, index, all。 possible_keys 列出可能有助于查询的索引，如果为空则表示没有可用的索引。 key 列出实际选择使用的索引，如果为 NULL 则表示没有使用索引。 Extra 列出额外的信息，例如 Using where 表示使用了 WHERE 条件过滤。 EXPLAIN SELECT id FROM user WHERE biz_id \u003e 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null range idx_user_biz_id_msg idx_user_biz_id_msg 8 null 1 100 Using where; Using index EXPLAIN SELECT * FROM user WHERE biz_id \u003e 2 AND message = ''; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null range idx_user_biz_id_msg idx_user_biz_id_msg 8 null 1 10 Using index condition; Using where EXPLAIN SELECT id FROM user WHERE biz_id \u003e 2 ORDER BY id DESC ; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null range idx_user_biz_id_msg idx_user_biz_id_msg 8 null 1 100 Using where; Using index; Using filesort EXPLAIN SELECT * FROM user WHERE biz_id \u003e 2 AND message = '' ORDER BY id DESC ; id select_type table partitions type possible_keys key key_len ref rows fil","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:3:2","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"IN 还是 EXISTS？ DROP TABLE IF EXISTS task; CREATE TABLE task ( id bigint(20) NOT NULL COMMENT '用户id', biz_id bigint(20) NOT NULL COMMENT '业务id', message text COMMENT '业务信息', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT '订单' CHARSET = utf8mb4; CREATE INDEX idx_task_biz_id ON task(biz_id); DELIMITER $$ CREATE PROCEDURE insertTaskData(IN start_id int, IN end_id int, IN bizId int) BEGIN DECLARE i int DEFAULT start_id; WHILE i \u003c= end_id DO INSERT INTO task (id, biz_id, message) VALUES (i, bizId, SUBSTRING(MD5(RAND()), 1, 8)); SET i = i + 1; END WHILE; END $$ DELIMITER ; -- 插入第一条业务id1数据 CALL insertTaskData(1, 1, 1); -- 插入一百万万条其他数据 CALL insertTaskData(2, 10, 2); -- 插入第二条业务id1数据 CALL insertTaskData(11, 11, 1); CALL insertTaskData(12, 12, 3); CALL insertTaskData(13, 13, 4); CALL insertTaskData(14, 14, 5); CALL insertTaskData(15, 15, 6); CALL insertTaskData(16, 16, 7); ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:4:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"分析 小表驱动大表的原则 EXPLAIN SELECT COUNT(*) FROM task LEFT JOIN user u ON task.biz_id = u.biz_id; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index null idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE u null ref idx_user_biz_id_msg idx_user_biz_id_msg 8 test.task.biz_id 996761 100 Using index SELECT COUNT(*) FROM task LEFT JOIN user u ON task.biz_id = u.biz_id; -- [2023-08-30 10:04:52] 1 row retrieved starting from 1 in 2 s 784 ms (execution: 2 s 766 ms, fetching: 18 ms) EXPLAIN SELECT COUNT(*) FROM user u LEFT JOIN task t ON u.biz_id = t.biz_id; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u null index null idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE t null ref idx_task_biz_id idx_task_biz_id 8 test.u.biz_id 5 100 Using index SELECT COUNT(*) FROM user u LEFT JOIN task t ON u.biz_id = t.biz_id; -- [2023-08-30 10:04:57] 1 row retrieved starting from 1 in 4 s 527 ms (execution: 4 s 504 ms, fetching: 23 ms) EXPLAIN SELECT COUNT(*) FROM task WHERE biz_id IN ( SELECT biz_id FROM user); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.task.biz_id 1 100 null 2 MATERIALIZED user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index SELECT COUNT(*) FROM task WHERE biz_id IN (SELECT biz_id FROM user); -- [2023-08-30 10:04:35] 1 row retrieved starting from 1 in 443 ms (execution: 399 ms, fetching: 44 ms) EXPLAIN SELECT COUNT(*) FROM user WHERE biz_id IN (SELECT biz_id FROM task); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.user.biz_id 1 100 null 2 MATERIALIZED task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index SELECT COUNT(*) FROM user WHERE biz_id IN (SELECT biz_id FROM task); -- [2023-08-30 10:04:58] 1 row retrieved starting from 1 in 545 ms (execution: 507 ms, fetching: 38 ms) EXPLAIN SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.task.biz_id 1 100 null 2 MATERIALIZED user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index EXPLAIN format=tree SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); -- -\u003e Aggregate: count(0) (cost=3189638.65 rows=1) -- -\u003e Nested loop inner join (cost=1594821.05 rows=15948176) -- -\u003e Index scan on task using idx_task_biz_id (cost=1.85 rows=16) -- -\u003e Single-row index lookup on \u003csubquery2\u003e using \u003cauto_distinct_key\u003e (biz_id=task.biz_id) -- -\u003e Materialize with deduplication (cost=202826.45..202826.45 rows=996761) -- -\u003e Index scan on user using idx_user_biz_id_msg (cost=103150.35 rows=996761) SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); -- [2023-08-30 10:04:59] 1 row retrieved starting from 1 in 459 ms (execution: 421 ms, fetching: 38 ms) EXPLAIN SELECT COUNT(*) FROM user WHERE EXISTS(SELECT biz_id FROM task WHERE user.biz_id = task.biz_id); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.user.biz_id 1 100 null 2 MATERIALIZED task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index ","date":"2025-09-21","objectID":"/zh-cn/2025/09/3.-mysql-fast-query-insights/:4:1","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. 浅谈 MySQL 快速查询","uri":"/zh-cn/2025/09/3.-mysql-fast-query-insights/"},{"categories":["network"],"content":"[Network] 1. 文件传输优化分享 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:0:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"背景(敏感数据已脱敏) 由于种种要求，需要将数据上传到海外的 oss 上进行存储。所以开发了一个代理服务维护数据并进行加密等操作。期间内部发现数据上传下载非常慢，经过一系列排查，最终定位到问题根源，并给出解决方案。现将排查过程进行分享。 当然了，前提之一是内网打通，通过专线网络接入，才能做到理论上的物理极限。使用复杂漫长的公共网络既不适合文件安全，也不适合大文件长时间传输。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:1:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"服务本身问题 最初怀疑是数据落盘导致的太慢。因为上传必须落盘，防止文件过大。下载直接流式传输，非常合理。唯一的改进是上传进行流式加密和传输，但是当前问题不大。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:2:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"现象 使用编写的脚本上传 1M 的加密数据，耗时接近 2s import requests requests.post(f\"{url}/upload/files\", files={ \"data\": ('', upload_data, \"application/json\"), \"file\": transfer_data }) $ python oss.py --file_input=./1M.data --region=us --model=3 --range=5 encrypted_upload upload ./1M.data, encrypt cost 4.714599609375, upload cost 1788.95849609375 upload ./1M.data, encrypt cost 10.140625, upload cost 1945.90087890625 upload ./1M.data, encrypt cost 9.924560546875, upload cost 1756.984130859375 upload ./1M.data, encrypt cost 8.694580078125, upload cost 1930.31201171875 upload ./1M.data, encrypt cost 8.279296875, upload cost 1739.38623046875 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:3:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"抓包 与运维进行沟通，运维怀疑是网络问题，进行抓包一探究竟。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:4:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"抓包演示 ping 包 $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:21:19.255718 IP public2.alidns.com.domain \u003e domain1.36590: 43190 1/0/1 A x.x.x.x1 (88) 16:21:19.256404 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 1, length 64 16:21:19.456754 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 1, length 64 16:21:20.257688 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 2, length 64 16:21:20.458076 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 2, length 64 16:21:21.259088 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 3, length 64 16:21:21.459506 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 3, length 64 16:21:22.260538 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 4, length 64 16:21:22.460976 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 4, length 64 $ ping domain1 PING domain1 (x.x.x.x1) 56(84) bytes of data. 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=1 ttl=58 time=200 ms 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=2 ttl=58 time=200 ms 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=3 ttl=58 time=200 ms ^C --- domain1 ping statistics --- 4 packets transmitted, 3 received, 25% packet loss, time 3004ms rtt min/avg/max/mdev = 200.395/200.419/200.456/0.517 ms 三次握手 16:54:06.286416 IP domain1.33666 \u003e x.x.x.x1.http: Flags [S], seq 2682796272, win 64240, options [mss 1460,sackOK,TS val 2595135963 ecr 0,nop,wscale 7], length 0 16:54:06.486797 IP x.x.x.x1.http \u003e domain1.33666: Flags [S.], seq 2198055866, ack 2682796273, win 62643, options [mss 1460,sackOK,TS val 2062390218 ecr 2595135963,nop,wscale 7], length 0 16:54:06.486840 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 2595136163 ecr 2062390218], length 0 四次挥手 16:54:28.356723 IP domain1.54028 \u003e x.x.x.x1.http: Flags [F.], seq 1746, ack 215, win 501, options [nop,nop,TS val 2595158034 ecr 2062412087], length 0 16:54:28.557169 IP x.x.x.x1.http \u003e domain1.54028: Flags [F.], seq 215, ack 1747, win 477, options [nop,nop,TS val 2062412289 ecr 2595158034], length 0 16:54:28.557222 IP domain1.54028 \u003e x.x.x.x1.http: Flags [.], ack 216, win 501, options [nop,nop,TS val 2595158234 ecr 2062412289], length 0 tcpdump Flags Tcpdump flags 是指示 TCP 连接状态或动作的标志。它们通常在 tcpdump 输出中用方括号表示。Tcpdump 输出中有多种 flags，输出也可能包含多个 TCP flags 的组合 1。一些常见的 flags 有： S (SYN): 这个 flag 用于在两个主机之间建立连接。它在三次握手的第一个包中设置。 . (No flag): 这意味着在包中没有设置任何 flag。它通常用于数据传输或确认包。 P (PUSH): 这个 flag 用于表示发送方希望尽快发送数据，而不等待缓冲区填满。 F (FIN): 这个 flag 用于终止两个主机之间的连接。它在四次挥手的最后一个包中设置。 R (RST): 这个 flag 用于重置处于无效状态或遇到错误的连接。它也用于拒绝不想要的连接尝试。 W (ECN CWR): 这个 flag 用于表示发送方已经根据网络的显式拥塞通知 (ECN) 减小了其拥塞窗口大小。 E (ECN-Echo): 这个 flag 用于表示接收方已经收到了一个带有 ECN 位的包，这意味着网络中存在拥塞。 例如，一个带有 flags [S.] 的包意味着它是一个 SYN 包，是建立 TCP 连接的第一步。一个带有 flags [P.] 的包意味着它是一个 PUSH 包，包含了发送方想要快速传送的数据。一个带有 flags [F.] 的包意味着它是一个 FIN 包，是关闭 TCP 连接的最后一步 2。 为什么 tcpdump 四次挥手只有三个包 Tcpdump 四次挥手只有三个包的原因可能有以下几种： 一种可能是被动关闭方（收到 FIN 的一方）在回复 ACK 的同时，也发送了自己的 FIN，将第二次和第三次挥手合并为一个报文，节省了一个包 1。这种情况下，被动关闭方已经没有数据要发送了，所以可以直接进入 LAST_ACK 状态，等待主动关闭方的最后一个 ACK。 另一种可能是主动关闭方（发送 FIN 的一方）在收到被动关闭方的 FIN 后，没有及时回复 ACK，而是在一段时间后才发送 ACK，并且在 ACK 中设置了 RST 标志，表示强制重置连接 2。这种情况下，主动关闭方可能遇到了异常或超时，所以不再等待 2MSL 的时间，而是直接进入 CLOSE 状态。 还有一种可能是 tcpdump 没有抓到所有的包，因为网络延迟或丢包的原因，导致某个挥手的报文没有被捕获到 3。这种情况下，可以尝试重新抓包或增加抓包的时间范围，看是否能够看到完整的四次挥手过程。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:4:1","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"实际数据 $ python oss.py --file_input=./1K.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1K.data, encrypt cost 1.530029296875, upload cost 408.5546875 $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:54:06.286416 IP domain1.33666 \u003e x.x.x.x1.http: Flags [S], seq 2682796272, win 64240, options [mss 1460,sackOK,TS val 2595135963 ecr 0,nop,wscale 7], length 0 16:54:06.486797 IP x.x.x.x1.http \u003e domain1.33666: Flags [S.], seq 2198055866, ack 2682796273, win 62643, options [mss 1460,sackOK,TS val 2062390218 ecr 2595135963,nop,wscale 7], length 0 16:54:06.486840 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 2595136163 ecr 2062390218], length 0 16:54:06.486930 IP domain1.33666 \u003e x.x.x.x1.http: Flags [P.], seq 1:292, ack 1, win 502, options [nop,nop,TS val 2595136164 ecr 2062390218], length 291: HTTP: POST /upload/files HTTP/1.1 16:54:06.486960 IP domain1.33666 \u003e x.x.x.x1.http: Flags [P.], seq 292:1746, ack 1, win 502, options [nop,nop,TS val 2595136164 ecr 2062390218], length 1454: HTTP 16:54:06.687234 IP x.x.x.x1.http \u003e domain1.33666: Flags [.], ack 292, win 488, options [nop,nop,TS val 2062390419 ecr 2595136164], length 0 16:54:06.687279 IP x.x.x.x1.http \u003e domain1.33666: Flags [.], ack 1746, win 477, options [nop,nop,TS val 2062390419 ecr 2595136164], length 0 16:54:06.690277 IP x.x.x.x1.http \u003e domain1.33666: Flags [P.], seq 1:215, ack 1746, win 477, options [nop,nop,TS val 2062390422 ecr 2595136164], length 214: HTTP: HTTP/1.1 200 OK 16:54:06.690314 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 215, win 501, options [nop,nop,TS val 2595136367 ecr 2062390422], length 0 16:54:06.692023 IP domain1.33666 \u003e x.x.x.x1.http: Flags [F.], seq 1746, ack 215, win 501, options [nop,nop,TS val 2595136369 ecr 2062390422], length 0 16:54:06.892401 IP x.x.x.x1.http \u003e domain1.33666: Flags [F.], seq 215, ack 1747, win 477, options [nop,nop,TS val 2062390624 ecr 2595136369], length 0 16:54:06.892448 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 216, win 501, options [nop,nop,TS val 2595136569 ecr 2062390624], length 0 实际是上传 1M 数据，进行分析，此处简化。 因为时间跳跃增长全部发生在服务端返回的数据包。此时问题已经很明了了，由于深圳和美东现实的物理距离，200ms 的来回已经做到了极限。所以其实是合理的。 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:4:2","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"灵魂拷问 此时，一个灵魂拷问出现了，为什么之前走公网反而更快？ 与兄弟部门同事沟通，模拟他们的代码，使用 aws 的 sdk 进行测试 import boto3 from boto3.s3.transfer import TransferConfig def download(): s3_client = client(access_key, access_secret, host) GB = 1024 ** 3 config = TransferConfig(multipart_threshold=2 * GB, max_concurrency=10, use_threads=True) s3_client.download_file(Bucket=\"bucket\", Key=\"name-100.jpg\", Filename=\"name-100.jpg\", Config=config) if __name__ == '__main__': download() # ... download() 结果 2.359457492828369 2.34989070892334 2.4120875199635825 2.3953704833984375 2.382766008377075 2.3793430725733438 2.3801622731345042 2.374732166528702 2.393121269014147 2.387941288948059 2.3849898034876045 2.3809364239374795 2.382789208338811 2.379830701010568 2.3768802642822267 2.3746740520000458 2.374574675279505 2.3716080056296454 可以看出，使用 aws-sdk，反而更慢。这就更奇怪了，为什么没有复现出兄弟部门的结果？ 首先看到 config 里的max_concurrency=10给了我一定的迷惑，client 肯定是自己支持线程池的，但是看起来没有用到，因为每次初始化了新的 client 优化 所以复用 client 进行测试 def download(): s3_client = client(access_key, access_secret, host) GB = 1024 ** 3 config = TransferConfig(multipart_threshold=2 * GB, max_concurrency=10, use_threads=True) now = time.time() count = 0 while count \u003c 20: s3_client.download_file(Bucket=\"bucket\", Key=\"name-100.jpg\", Filename=\"name-100.jpg\", Config=config) count += 1 print((time.time() - now) / count) 结果 download 2.465491533279419 1.5669758319854736 1.2221351464589436 1.0315884947776794 0.9212518692016601 0.8434466520945231 0.7922392232077462 0.7573718726634979 0.7251839107937283 0.6981703996658325 0.6772929538380016 0.6588474710782369 0.6429501130030706 0.6297299180712018 0.6190152009328206 0.6086597740650177 0.5995960656334373 0.5917102760738797 0.585765048077232 0.5791293740272522 根据结果发现，只是以为默认框架全复用连接。修改为复用连接后，效果极佳。 而且正在接近理论极限值 200ms(无限带宽，一次交互) ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:5:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"初步结论 首先，关于如何加速传输，已经有了最直接的结论。复用连接即可，下文贴出 1M 文件的对比 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:6:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"复用连接与不复用连接对比 $ python oss.py --file_input=./1M.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1M.data, encrypt cost 4.924560546875, upload cost 1919.100341796875 http://domain1 upload ./1M.data, encrypt cost 4.593017578125, upload cost 1715.593994140625 http://domain1 upload ./1M.data, encrypt cost 10.076171875, upload cost 2253.67333984375 http://domain1 upload ./1M.data, encrypt cost 12.694091796875, upload cost 1714.197021484375 http://domain1 upload ./1M.data, encrypt cost 12.3076171875, upload cost 2152.773193359375 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:6:1","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"继续拷问 本来，一切的一切，到这里都结束了，复用连接，效率极其提高。与服务端无关，仅与客户端有关，客户端交给兄弟部门进行改造即可。 直到兄弟部门反馈了一个问题——为什么服务端会断开连接？ 我通过网上查询和翻源码的默认值，发现 服务端默认支持 8192 连接 默认客户端连接 30m 超时或永不超时 明显都与事实不符合 然后兄弟部门开始监听连接状态，发现连接状态快速进入了 CLOSE_WAIT 的状态。明显是服务端接收到了 FIN 包。为了说明该情况，通过抓包，实际证明了兄弟部门代码问题导致发送了 FIN 包。 由于我非常认可通过抓包和监听连接状态的方式来查找问题，我打算复现一下之前的情况来介绍一下抓包工具和方法。但是复现的过程中，我开始了灵魂拷问 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:7:0","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"为什么效率优化这么大？ 按照朴素的思想，复用连接，应该是省略了三次握手和四次挥手，根据上述的了解，应该仅优化 400ms 左右才对。然而事实并非如此，而是秒级别优化，这是为什么？ # 朴素思想中的不复用连接传包 1. [S][P][P][P][P][F] 2. [S][P][P][P][P][F] 3. [S][P][P][P][P][F] # 朴素思想中的复用连接传包 1. [S][P][P][P][P] 2. [P][P][P][P] 3. [P][P][P][P][F] $ python oss-muti.py --file_input=./1M.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1M.data, encrypt cost 5.02880859375, upload cost 2589.014892578125 http://domain1 upload ./1M.data, encrypt cost 10.720947265625, upload cost 562.706787109375 http://domain1 upload ./1M.data, encrypt cost 11.202392578125, upload cost 370.651611328125 http://domain1 upload ./1M.data, encrypt cost 10.948486328125, upload cost 372.409423828125 http://domain1 upload ./1M.data, encrypt cost 11.99560546875, upload cost 371.28759765625 所以再次抓包进行深究 ","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:7:1","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["network"],"content":"100K 数据对比(省略返回包) $ python oss.py --file_input=./100K.data --file_output=./100K.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./100K.data, encrypt cost 1.81884765625, upload cost 1017.35791015625 http://domain1 upload ./100K.data, encrypt cost 1.159912109375, upload cost 1021.509521484375 http://domain1 upload ./100K.data, encrypt cost 1.11669921875, upload cost 1016.612548828125 http://domain1 upload ./100K.data, encrypt cost 1.128662109375, upload cost 1016.171875 http://domain1 upload ./100K.data, encrypt cost 0.9912109375, upload cost 1016.228759765625 $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 17:16:03.069540 IP domain1.53580 \u003e x.x.x.x1.http: Flags [S], seq 4211566581, win 64240, options [mss 1460,sackOK,TS val 2596452757 ecr 0,nop,wscale 7], length 0 17:16:03.270682 IP x.x.x.x1.http \u003e domain1.53580: Flags [S.], seq 1741768869, ack 4211566582, win 62643, options [mss 1460,sackOK,TS val 2063707002 ecr 2596452757,nop,wscale 7], length 0 17:16:03.270850 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 1:294, ack 1, win 502, options [nop,nop,TS val 2596452958 ecr 2063707002], length 293: HTTP: POST /upload/files HTTP/1.1 17:16:03.680467 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 72694:74142, ack 1, win 502, options [nop,nop,TS val 2596453367 ecr 2063707405], length 1448: HTTP 17:16:03.874400 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 101758:103124, ack 1, win 502, options [nop,nop,TS val 2596453561 ecr 2063707606], length 1366: HTTP 17:16:04.082005 IP x.x.x.x1.http \u003e domain1.53580: Flags [P.], seq 1:215, ack 103124, win 442, options [nop,nop,TS val 2063707813 ecr 2596453561], length 214: HTTP: HTTP/1.1 200 OK 17:16:04.083769 IP domain1.53580 \u003e x.x.x.x1.http: Flags [F.], seq 103124, ack 215, win 501, options [nop,nop,TS val 2596453771 ecr 2063707813], length 0 17:16:04.090059 IP domain1.44338 \u003e x.x.x.x1.http: Flags [S], seq 3876376673, win 64240, options [mss 1460,sackOK,TS val 2596453777 ecr 0,nop,wscale 7], length 0 17:16:04.284937 IP x.x.x.x1.http \u003e domain1.53580: Flags [F.], seq 215, ack 103125, win 442, options [nop,nop,TS val 2063708016 ecr 2596453771], length 0 17:16:04.291110 IP x.x.x.x1.http \u003e domain1.44338: Flags [S.], seq 27078140, ack 3876376674, win 62643, options [mss 1460,sackOK,TS val 2063708023 ecr 2596453777,nop,wscale 7], length 0 17:16:04.291270 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 1:294, ack 1, win 502, options [nop,nop,TS val 2596453978 ecr 2063708023], length 293: HTTP: POST /upload/files HTTP/1.1 17:16:04.693394 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 42286:43734, ack 1, win 502, options [nop,nop,TS val 2596454380 ecr 2063708425], length 1448: HTTP 17:16:04.720945 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 72694:74142, ack 1, win 502, options [nop,nop,TS val 2596454408 ecr 2063708425], length 1448: HTTP 17:16:04.894505 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 101838:103124, ack 1, win 502, options [nop,nop,TS val 2596454582 ecr 2063708626], length 1286: HTTP 17:16:05.105003 IP x.x.x.x1.http \u003e domain1.44338: Flags [P.], seq 1:215, ack 103124, win 442, options [nop,nop,TS val 2063708837 ecr 2596454582], length 214: HTTP: HTTP/1.1 200 OK 17:16:05.106641 IP domain1.44338 \u003e x.x.x.x1.http: Flags [F.], seq 103124, ack 215, win 501, options [nop,nop,TS val 2596454794 ecr 2063708837], length 0 17:16:05.112610 IP domain1.44340 \u003e x.x.x.x1.http: Flags [S], seq 1962726172, win 64240, options [mss 1460,sackOK,TS val 2596454800 ecr 0,nop,wscale 7], length 0 17:16:05.307713 IP x.x.x.x1.http \u003e domain1.44338: Flags [F.], seq 215, ack 103125, win 442, options [nop,nop,TS val 2063709039 ecr 2596454794], length 0 17:16:05.313623 IP x.x.x.x1.http \u003e domain1.44340: Flags [S.], seq 2582074627, ack 1962726173, win 62643, options [mss 1460,sackOK,TS val 2063709045 ecr 2596454800,nop,wscale 7], length 0 1","date":"2025-09-21","objectID":"/zh-cn/2025/09/2.-file-transfer-optimization/:7:2","tags":["network","tcpdump"],"title":"[Network] 1. 文件传输优化分享","uri":"/zh-cn/2025/09/2.-file-transfer-optimization/"},{"categories":["github","deploy"],"content":"[Github] 1. 部署 GitHub 博客站点 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:0:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"简介 GitHub Pages 是一个静态网站托管服务，它直接从 GitHub 仓库中获取 HTML、CSS 和 JavaScript 文件，可选地通过构建过程运行这些文件，然后发布网站。 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:1:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"准备工作 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:2:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"创建仓库 创建一个名为 your_github_username.github.io 的仓库，其中 your_github_username 是你的 GitHub 用户名。例如，如果你的 GitHub 用户名是 octocat，那么仓库名应该是 octocat.github.io。 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:2:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"安装 Hugo 从 Hugo 官方发布页面 下载最新版本的 Hugo ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:2:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"创建博客站点 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:3:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"初始化 Hugo 站点 # 创建目录 mkdir your_github_username.github.io # 进入目录 cd your_github_username.github.io # 初始化站点 hugo new site . # git 初始化，确保这是一个 git 仓库 git init ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:3:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"添加主题 # 添加主题，这里我们使用 LoveIt 主题 git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt # 现在 git 在 main 分支上，这个分支不稳定，我们需要切换到最新的稳定版本 cd themes/LoveIt git checkout v0.3.0 cd ../.. # 现在，你的目录中应该有一个 .gitmodules 文件。如果没有，你需要先运行 `git init` # 复制示例站点配置文件到根目录 cp themes/LoveIt/exampleSite/hugo.toml . ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:3:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"修改配置文件 修改配置文件 hugo.toml 基础 URL baseURL = \"https://gooddayday.github.io\" 主题目录 # themes directory # 主题目录 themesDir = \"./themes\" 网站标题 # website title # 网站标题 title = \"GoodyHao's Blog\" 网站图片 # website images for Open Graph and Twitter Cards # 网站图片, 用于 Open Graph 和 Twitter Cards images = [\"/logo.jpg\"] 网站图标 将图标文件放在 static 目录中 Git 仓库 修改 gitRepo 为你的公共 git 仓库 URL # public git repo url only then enableGitInfo is true # 公共 git 仓库路径，仅在 enableGitInfo 设为 true 时有效 gitRepo = \"https://github.com/GOODDAYDAY/GOODDAYDAY.github.io\" ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:3:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"GitHub 部署 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:4:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"创建工作流文件 创建文件 .github/workflows/deploy.yaml，并添加以下内容： name: Deploy Hugo to GitHub Pages on: push: # 触发条件：推送代码到master分支 branches: - master jobs: build-and-deploy: runs-on: ubuntu-latest # 使用Ubuntu环境 steps: # 1. 检出仓库代码（递归拉取主题submodule） - uses: actions/checkout@v4 with: submodules: true # 2. 安装Hugo（使用extended版本，支持SASS） - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' # 或指定版本（如'0.147.2'） extended: true # 3. 缓存依赖（加快后续构建速度） - uses: actions/cache@v3 with: path: | resources/_gen public key: ${{ runner.os }}-hugo-${{ hashFiles('**/go.sum') }} restore-keys: | ${{ runner.os }}-hugo- # 4. 构建Hugo站点（开启压缩） - name: Build Hugo site run: hugo --minify # 5. 部署到GitHub Pages（自动推送public目录到gh-pages分支） - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v4 with: github_token: ${{ secrets.GITHUB_TOKEN }} # GitHub自动提供的Token（无需手动创建） publish_dir: ./public # 指向Hugo生成的静态文件目录 force_orphan: true # 强制创建新提交（避免分支历史混乱） GitHub 仓库设置 -\u003e Pages -\u003e Source -\u003e 选择 gh-pages 分支和 / (root) 文件夹 -\u003e 保存 如果 gh-pages 分支不存在，需要先将代码推送到 GitHub 需要设置 token 生成具有 repo 和 workflow 权限的新 token 将 token 添加到 GitHub secrets，名称为 TOKEN_GITHUB ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:4:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"推送代码到 GitHub # 添加所有文件 git add . # 提交 git commit -m \"first commit\" # 推送到 GitHub git push -u origin master ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:4:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"检查工作流 在 GitHub Actions 中检查工作流 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:4:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"访问博客站点 使用 https://your_github_username.github.io 访问博客站点，例如，https://gooddayday.github.io ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:5:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"其他 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"添加新文章 # 创建新文章 hugo new posts/first-post.md # 编辑文章 vim content/posts/first-post.md # 编辑后，需要将文章设置为已发布 # 设置 draft = false # 然后提交并推送到 GitHub git add . git commit -m \"add first post\" git push 如果你想在文章中添加图片，需要将图片放在 static 目录中，例如 static/images/first-post-image.png，然后你可以在文章中使用 /images/first-post-image.png 访问图片。 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"gitignore 在根目录中创建 .gitignore 文件，并添加以下内容： public/* 我们不需要将 public 目录推送到 GitHub，因为它会在工作流中由 Hugo 生成。 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"标签和分类生成 标签和分类会由 Hugo 自动生成，无需手动创建。 但如果没有显示下面的 index.html，你需要添加模板。 只需将 themes/LoveIt/layouts/taxonomy/list.html 复制到不同路径，并重命名为 layouts/taxonomy/tag.html 和 layouts/taxonomy/category.html 然后，运行 Hugo 服务器检查结果是否有像下图一样的 index.html： ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"使用 giscus 设置评论系统 默认情况下，LoveIt 主题使用 Valine 评论系统，但我们推荐使用基于 GitHub Discussions 的 Giscus。Giscus 是免费的、稳定的，并将评论数据存储在你自己的 GitHub 仓库中。 禁用其他评论系统 首先，确保在 hugo.toml 中禁用其他评论系统： # 禁用 Valine [params.page.comment.valine] enable = false # 禁用 Disqus [params.page.comment.disqus] enable = false # 禁用 Gitalk [params.page.comment.gitalk] enable = false 启用 GitHub Discussions 转到你的 GitHub 仓库设置：https://github.com/your-username/your-username.github.io/settings 导航到 Features 部分 勾选 Discussions 复选框以启用它 配置 giscus 访问 giscus.app 生成配置 填写仓库字段：your-username/your-username.github.io 点击 The giscus app is installed, otherwise visitors will not be able to comment and react. 并将 giscus 安装到你的仓库。 更新 hugo.toml 将 giscus 配置添加到你的 hugo.toml： [params.page.comment.giscus] enable = true repo = \"your-username/your-username.github.io\" repoId = \"your-repo-id-from-giscus\" category = \"General\" # 或你选择的分类 categoryId = \"your-category-id-from-giscus\" lang = \"\" # 空值表示自动检测 mapping = \"pathname\" reactionsEnabled = \"1\" emitMetadata = \"0\" inputPosition = \"bottom\" lazyLoading = false lightTheme = \"light\" darkTheme = \"dark\" 像 repoId 和 categoryId 这样的数据可以在你之前生成的 giscus 配置中找到。 ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:4","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"语言切换设置 随着 AI 的发展，将博客内容翻译成不同语言变得容易。这里我们以英语和中文为例。 首先，我们需要在 content 目录中创建两个目录：en 和 zh，然后将对应语言的内容放在各自的目录中。 不同语言文件的名称应该相同，例如，content/en/posts/1.deploy-github-blog-site.md 和 content/zh/posts/1.deploy-github-blog-site.md 如果不一样，Hugo 会将它们视为不同的文章，并在不同的语言列表中显示它们。 然后，我们需要修改 hugo.toml 文件以启用多语言支持： # determines default content language [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] # 设置默认的语言 [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] defaultContentLanguage = \"en\" # whether to include default language in URL path # 是否在URL路径中包含默认语言 (设为true让所有语言都有前缀，设为false让默认语言无前缀) defaultContentLanguageInSubdir = true .... # 是否包括中日韩文字 hasCJKLanguage = false hasCJKLanguage = true ... # Multilingual # 多语言 [languages] [languages.en] weight = 1 languageCode = \"en\" languageName = \"English\" hasCJKLanguage = false copyright = \"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\" contentDir = \"content\" contentDir = \"content/en\" ... [languages.zh-cn] weight = 2 languageCode = \"zh-CN\" languageName = \"简体中文\" hasCJKLanguage = true copyright = \"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\" contentDir = \"content/zh\" ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:5","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"语言切换显示自定义 默认情况下，语言切换按钮显示在网站的右上角。如果你想切换语言，需要点击两次，人们可能不会注意到还有另一种语言。 所以，对我来说，我想在标题菜单中显示语言切换按钮，这样人们可以轻松找到它并切换语言。 要实现这一点，我们需要将 themes/LoveIt/layouts/partials/header.html 文件复制到 layouts/partials/header.html。 然后，我们需要修改 layouts/partials/header.html 文件，在标题菜单中添加语言切换按钮。 修改前： {{- if hugo.IsMultilingual -}} \u003ca href=\"javascript:void(0);\" class=\"menu-item language\" title=\"{{ T \"selectLanguage\" }}\"\u003e \u003ci class=\"fa fa-globe fa-fw\" aria-hidden=\"true\"\u003e\u003c/i\u003e \u003cselect class=\"language-select\" id=\"language-select-desktop\" onchange=\"location = this.value;\"\u003e {{- if eq .Kind \"404\" -}} {{- /* https://github.com/dillonzq/LoveIt/issues/378 */ -}} {{- range .Sites -}} {{- $link := printf \"%v/404.html\" .LanguagePrefix -}} \u003coption value=\"{{ $link }}\"{{ if eq . $.Site }} selected{{ end }}\u003e {{- .Language.LanguageName -}} \u003c/option\u003e {{- end -}} {{- else -}} {{- range .AllTranslations -}} \u003coption value=\"{{ .RelPermalink }}\"{{ if eq .Lang $.Lang }} selected{{ end }}\u003e {{- .Language.LanguageName -}} \u003c/option\u003e {{- end -}} {{- end -}} \u003c/select\u003e \u003c/a\u003e {{- end -}} 修改后： {{- /* 直接切换语言按钮 */ -}} {{- if hugo.IsMultilingual -}} {{- if eq .Kind \"404\" -}} {{- /* https://github.com/dillonzq/LoveIt/issues/378 */ -}} {{- range .Sites -}} {{- if ne . $.Site -}} \u003ca class=\"menu-item\" href=\"{{ printf \"%v/404.html\" .LanguagePrefix }}\" title=\"{{ .Language.LanguageName }}\"\u003e {{- if eq .Language.LanguageCode \"zh-CN\" -}} 中文 {{- else -}} English {{- end -}} \u003c/a\u003e {{- end -}} {{- end -}} {{- else -}} {{- range .AllTranslations -}} {{- if ne .Lang $.Lang -}} \u003ca class=\"menu-item\" href=\"{{ .RelPermalink }}\" title=\"{{ .Language.LanguageName }}\"\u003e {{- if eq .Language.LanguageCode \"zh-CN\" -}} 中文 {{- else -}} English {{- end -}} \u003c/a\u003e {{- end -}} {{- end -}} {{- end -}} {{- end -}} 结果显示如下： ","date":"2025-09-10","objectID":"/zh-cn/2025/09/1.-deploy-github-blog-site/:6:6","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. 部署 GitHub 博客站点","uri":"/zh-cn/2025/09/1.-deploy-github-blog-site/"}]