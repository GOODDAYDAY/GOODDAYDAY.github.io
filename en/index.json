[{"categories":["Cluster","Redis"],"content":"Overview The common three cluster modes are Redis(Split Cluster), MySQL(Master-Slave), Kafka(RAFT). This blog will introduce from the single node to the cluster mode of Redis. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:1:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Evolution History: From Single Node to Cluster ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Single Node Era This is the common monolithic architecture of Redis. Problems Single Node Single point of failure (SPOF) T Memory limited to single machine T Write throughput bottleneck T ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Master-Slave Replication As to reliability, it is easy to think adding some nodes as Master-Slave. Problems Single Node Master-Slave Single point of failure (SPOF) T Solved Memory limited to single machine T T Write throughput bottleneck T T No automatic failover \\ T We can see that, Master-Slave mode just solve the SPOF. And even do not have the failover, so only Master-Slave is not something reliable. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Sentinel Mode For failover, we can easily think RAFT or other algorithms for keeping it reliable. But in Sentinel Mode, it follows other way like ZK which hosts another server for routing. Problems Single Node Master-Slave Sentinel Single point of failure (SPOF) T Solved Solved Memory limited to single machine T T T Write throughput bottleneck T T T No automatic failover \\ T Solved One core of cluster is horizontal scale for improving the throughput. It is clearly that sentinel mode is not support. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Redis Cluster: The Official Answer We know Redis has 16384 slots for saving and loading. So it is naturally that we can choose one not common but easy way for cluster. Split these 16384 slots for many different Master-Slave Clusters. Problems Single Node Master-Slave Sentinel Cluster Single point of failure (SPOF) T Solved Solved Solved Memory limited to single machine T T T Solved Write throughput bottleneck T T T Solved No automatic failover \\ T Solved Solved In this way, we can do many interesting things. Like split some slots for hot cache, some slots for cold cache. Like 0~99 slots are used for hot cache and this cluster can be assembled by 1 Master + 7 Slave. Like 100~199 slots are used for clod cache and this cluster can be assembled by 1 Master + 1 Slave. Other normal data stored in common cluster based 1 Master + 2 Slave. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:4","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Sentinel vs Cluster Head-to-Head Comparison Aspect Sentinel Mode Cluster Mode (even single-shard) Automatic Failover Yes (via external Sentinel processes) Yes (built-in, no extra processes) Deployment Complexity Need 3+ Sentinel processes + Redis nodes Just Redis nodes Client SDK Simple SDK Smart Client (slightly heavier) Multi-DB (SELECT) Supported (SELECT 0-15) Only DB 0 Multi-Key Operations Full support Need hash tags for cross-slot Future Scalability Must migrate to Cluster Just add nodes Network Overhead Sentinel heartbeats Gossip protocol (similar overhead) I should say that in my mind, if the Cluster Mode is assembled by only one Master and two Slave which hold the whole 16384 slots. This Cluster is better than Sentinel in every aspect except DB isolation. Cluster Mode Problems The core problem of Cluster Mode is the keys can be in different clusters. Lua Scripts Another problem is Lua scripts may failure while operating different keys which are in different clusters. But we can easily solve it by CRC16 algorithm. Pub / Sub Before Redis 7.0: Village Loudspeaker Mode Why broadcast? To support “dumb clients”: Client connects to random Node C, sends SUBSCRIBE news Node C doesn’t know who else subscribed to news on other nodes When someone publishes on Node A, Node A must broadcast to ALL nodes Only then can each node deliver the message to its local subscribers Cost: O(N) network messages per PUBLISH. In a 100-node cluster, every PUBLISH triggers 99 Gossip messages! After Redis 7.0: Sharded Pub/Sub (Precision Mailbox) Redis 7.0 made a definition change: Channel IS now a special Key! Command Behavior SSUBSCRIBE news Slot = CRC16(“news”) % 16384, connect to owner node SPUBLISH news msg Route to owner node, deliver locally Trade-off: Clients must be “smart” (like Redisson) - know the topology, connect to correct node Can’t just connect to any random node and subscribe anymore Summary: Old logic (before 7.0): “Village loudspeaker” - convenient but wasteful New logic (after 7.0): “Precision mailbox” - efficient but requires smart clients ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:2:5","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Home of Data: Hash Slot and Routing ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:3:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Traditional Hashing VS Consistent Hashing VS Hash Slot Traditional Hashing The simplest approach: node = hash(key) % N Problem: When N changes (add/remove node), almost ALL keys get remapped! Example: hash(\"user:1001\") = 1000 Before (3 nodes): 1000 % 3 = 1 -\u003e Node 1 After (4 nodes): 1000 % 4 = 0 -\u003e Node 0 (MOVED!) Migration cost: ~(N-1)/N of all keys = ~75% when 3-\u003e4 nodes Consistent Hashing The industry standard for distributed systems (Cassandra, DynamoDB, etc.) Core Idea: Both nodes and keys are mapped to a ring (0 ~ 2^32). Each key goes to the first node found walking clockwise. Benefit: Adding a node only affects ~1/N of keys (the range between new node and its predecessor). But still has problems for Redis: Virtual Nodes Complexity: Need 100-200 virtual nodes per physical node for balance Metadata Overhead: Client must store the entire ring (all virtual nodes) Migration Granularity: Hard to control exactly which data moves Hash Slot Redis’s pragmatic choice: A fixed array of 16384 slots. Two-Level Mapping: Key -\u003e Slot: slot = CRC16(key) % 16384 (fixed, never changes) Slot -\u003e Node: Configurable, stored in cluster metadata No Silver Bullet: When Traditional Hash Wins Beware of “silver bullet” thinking! Consistent Hashing is NOT universally better. Where Traditional Hash beats Consistent Hash: Aspect Traditional Hash Consistent Hash Uniformity Naturally average Naturally not average Time Complexity O(1) - CPU instruction level O(log N) - binary search in TreeMap Distribution Mathematically perfect uniform Uneven without virtual nodes (a “hack”) Implementation 1 line: hash(key) % N ~50 lines: TreeMap + virtual nodes + ring wrap Memory Zero overhead TreeMap for all virtual nodes Best scenarios for Traditional Hash: Database Sharding: user_id % 1024 for fixed table count (rarely changes) HashMap/Dict Internals: Language-level hash tables use modulo, not consistent hashing Any static node count: When you can guarantee N won’t change Best scenarios for Consistent Hash: Load balancers with dynamic backends Distributed cache (Memcached) with frequent node changes Any system where nodes join/leave frequently Engineering Wisdom Use the simplest solution that works. If node count is fixed, traditional hash is faster and simpler. Only use consistent hashing when dynamic scaling is a real requirement. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:3:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Hash Slot Algorithm // Redis source code: cluster.c unsigned int keyHashSlot(char *key, int keylen) { int s, e; /* start-end indexes of { and } */ // Look for hash tag {...} for (s = 0; s \u003c keylen; s++) if (key[s] == '{') break; if (s \u003c keylen) { for (e = s+1; e \u003c keylen; e++) if (key[e] == '}') break; if (e \u003c keylen \u0026\u0026 e != s+1) { // Hash tag found: only hash content within {} return crc16(key+s+1, e-s-1) \u0026 16383; } } // No hash tag: hash the entire key return crc16(key, keylen) \u0026 16383; } The Formula: slot = CRC16(key) mod 16384 Why 16384 (2^14)? This is a hardcore design decision by Antirez - a “bandwidth vs. granularity” trade-off game. 1. The Gossip “Bandwidth Tax” Every Ping/Pong message carries a Slots Bitmap - each bit represents one slot: Slots Count Bitmap Size TCP Packets (MTU=1500) 65536 (2^16) 8 KB 6-7 packets 16384 (2^14) 2 KB 2 packets 8KB per heartbeat = massive bandwidth waste + more TCP fragmentation + higher retransmit probability. Antirez: “Making the message too big would waste a lot of bandwidth.” 2. The 1000-Node Soft Limit Redis Cluster targets medium-scale clusters, not Google Spanner-level global systems. Slots Nodes Slots per Node 65536 1000 ~65 16384 1000 ~16 16 slots per node is enough for rebalancing. 65 slots adds negligible benefit but 4x bandwidth cost. 3. Memory Overhead Each node stores bitmap for ALL other nodes: // cluster.h typedef struct { unsigned char slots[16384/8]; /* 2048 bytes = 2KB */ } clusterNode; Slots 1000 Nodes Memory 65536 1000 x 8KB = 8MB 16384 1000 x 2KB = 2MB Bottom Line: 16384 is the Goldilocks number - not too big (wastes bandwidth), not too small (limits granularity). CRC16 can produce 65536, but CRC16(key) % 16384 gives us just what we need. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:3:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Hash Tags: Forcing Keys to Same Slot Practical Use Cases: # All keys for same user go to same slot SET {user:1001}:name \"John\" SET {user:1001}:email \"john@example.com\" HSET {user:1001}:profile age 25 city \"NYC\" # Now you can do multi-key operations! MGET {user:1001}:name {user:1001}:email # Lua scripts work too EVAL \"return redis.call('GET', KEYS[1]) .. redis.call('GET', KEYS[2])\" 2 {user:1001}:name {user:1001}:email Warning: Don’t overuse hash tags! If too many keys share the same tag, you create a “hot slot” problem. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:3:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Gossip Protocol: How Nodes Talk Without a Boss ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Why Gossip? In centralized systems like Kafka, ZooKeeper maintains cluster state. But Redis Cluster has no ZK. How do nodes know about each other? Answer: Gossip Protocol — nodes exchange information through periodic “chitchat”. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Message Types Message Purpose PING “Hey, I’m alive! Here’s what I know about the cluster” PONG Response to PING with sender’s view of cluster state MEET “Welcome new node, join our cluster” FAIL “Node X is confirmed dead” PUBLISH Pub/Sub message broadcast ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"What’s Inside a Gossip Message? Each PING/PONG contains: // Simplified from cluster.h typedef struct { char sig[4]; // \"RCmb\" signature uint32_t totlen; // Total message length uint16_t type; // PING, PONG, MEET, FAIL... uint16_t count; // Number of gossip entries uint64_t currentEpoch; // Cluster's current epoch uint64_t configEpoch; // Sender's config epoch char sender[40]; // Sender's node ID char myslots[2048]; // Bitmap: which slots I own (16384 bits) char slaveof[40]; // My master's node ID (if I'm slave) uint16_t port; // My port uint16_t flags; // MASTER, SLAVE, PFAIL, FAIL... unsigned char state; // Cluster state (OK/FAIL) // Gossip section: info about OTHER nodes clusterMsgDataGossip gossip[]; } clusterMsg; typedef struct { char nodename[40]; // Node ID uint32_t ping_sent; // When I last pinged this node uint32_t pong_received; // When I last got pong char ip[46]; // IP address uint16_t port; // Port uint16_t flags; // What I think about this node } clusterMsgDataGossip; Key Information Exchanged: My Slots: 2KB bitmap of which slots I own My Epoch: My configuration version (critical for conflict resolution) Gossip About Others: What I know about N random other nodes ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Gossip Frequency and Scale Limits // From cluster.c - How often to gossip void clusterCron(void) { // Every 100ms if (!(iteration % 10)) { // Select a random node to PING // Prefer nodes we haven't heard from recently } // Every second if (!(iteration % 10)) { // Check for nodes that might be failing // Send PING to nodes not contacted recently } } The Communication Storm Problem: With N nodes, full mesh = N × (N-1) connections Nodes Connections Messages/sec (estimated) 10 90 ~100 100 9,900 ~1,000 1,000 999,000 ~10,000 Redis’s Mitigation: Smart Node Selection (not purely random): Each round, randomly pick ~5 nodes from the cluster From these 5, choose the one with oldest PONG time (least recently contacted) This ensures no node gets “forgotten” while avoiding full mesh Fallback Mechanism: If any node hasn’t responded for \u003e cluster-node-timeout / 2 Force send a PING immediately, regardless of random selection Prevents false-positive failure detection Partial Gossip: Each PING only carries info about ~10% of known nodes (not all) Reduces message size while still propagating state eventually Scale Limit: Recommended max: ~1000 nodes Beyond this, Gossip overhead becomes significant ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:4","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Epoch: The Logical Clock ConfigEpoch is crucial for consistency — it’s like Raft’s “term” or Paxos’s “ballot number”. When Epoch Increments: Slave wins election → becomes new master with higher epoch Slot migration completes → new owner gets higher epoch Manual failover → forced epoch bump ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:4:5","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Scaling: Slot Migration Deep Dive ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:5:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"When Do You Need to Scale? Scale Out (Add Nodes): Memory pressure on existing nodes CPU bottleneck Network bandwidth saturation Scale In (Remove Nodes): Over-provisioned cluster Cost optimization ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:5:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Migration State Machine ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:5:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The MIGRATE Command Internals MIGRATE Behavior: Atomic: Key appears on target and disappears from source atomically Blocking: By default, blocks the source node during transfer Timeout: Configurable timeout to prevent stuck migrations The Blocking Problem: // Simplified MIGRATE logic void migrateCommand(client *c) { // This can block! robj *o = lookupKeyRead(c-\u003edb, key); // Serialize object rio payload; createDumpPayload(\u0026payload, o); // Send to target (network I/O!) syncWrite(fd, payload.io.buffer.ptr, sdslen(payload.io.buffer.ptr), timeout); // Wait for OK (more network I/O!) syncReadLine(fd, buf, sizeof(buf), timeout); // Delete from source dbDelete(c-\u003edb, key); } For Large Keys: A single large key (big hash, big list) can block the source node for seconds! Mitigation: Redis 6.0+ supports non-blocking migration for certain data types. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:5:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Request Handling During Migration During migration, Slot 100 is in a transient state — moving from Node A to Node B but not yet complete. Node States: Node State Responsibility Node A MIGRATING Still owns Slot 100, but data is moving out Node B IMPORTING Receiving data, but not officially responsible Client - Slot Map still points Slot 100 → Node A Request Flow: Client → Node A: Client sends request based on cached Slot Map Node A checks local: Key exists → Process and return result Key missing → Return -ASK \u003cNode B\u003e (key already migrated) Client → Node B: Must send ASKING command first, then the original command Node B checks ASKING flag: Flag present → Execute command Flag absent → Return -MOVED \u003cNode A\u003e Why Require ASKING? The ASKING command prevents routing table corruption: Without ASKING: A random client connecting to Node B might incorrectly assume Slot 100 belongs to B Client updates its Slot Map prematurely → All future requests go to B But migration just started → Most keys still on A → Severe cache misses The ASKING flag acts as a one-time authorization token — only clients explicitly redirected by Node A (via -ASK) can access the importing slot. ASK vs MOVED: Aspect MOVED ASK When Migration completed Migration in progress Client action Update Slot Map Do NOT update Slot Map Semantics Permanent redirect Temporary redirect ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:5:4","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Failure Detection and Automatic Failover ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Distributed Voting Problem The Challenge: Without a central authority, how do nodes agree that a node is dead? The Answer: Quorum-based failure detection through gossip. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"PFAIL vs FAIL: The Two-Phase Detection ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Configuration: cluster-node-timeout # redis.conf cluster-node-timeout 15000 # 15 seconds (default) What This Controls: PFAIL Trigger: Node marked PFAIL after timeout with no PONG Failover Speed: Lower = faster detection, but more false positives Network Partition Sensitivity: Too low = frequent unnecessary failovers Rule of Thumb: Production: 15-30 seconds Testing: 5-10 seconds Never below 5 seconds ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Slave Election: Choosing the New Master ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:4","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Manual Failover Sometimes you want to failover deliberately (maintenance, upgrades): # On the slave you want to promote: CLUSTER FAILOVER # Force failover even if master is healthy: CLUSTER FAILOVER FORCE # Takeover without master agreement (dangerous!): CLUSTER FAILOVER TAKEOVER CLUSTER FAILOVER (graceful): Slave tells master “stop accepting writes” Master stops, slave catches up Slave becomes master No data loss! CLUSTER FAILOVER TAKEOVER: Doesn’t need master’s consent May lose recent writes Use only when master is unreachable ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:6:5","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Consistency Trade-offs: What Redis Cluster Sacrifices ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:7:0","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"CAP Theorem Recap ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:7:1","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Asynchronous Replication: The Data Loss Window ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:7:2","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"The Split-Brain Scenario ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:7:3","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Cluster","Redis"],"content":"Mitigation: min-replicas-to-write # redis.conf min-replicas-to-write 1 # At least 1 slave must be connected min-replicas-max-lag 10 # Slave must have replicated within 10 seconds Trade-off: Better consistency, but sacrifices availability. ","date":"2025-12-01","objectID":"/en/2025/12/cluster-2.-redis-cluster/:7:4","tags":["Redis","Cluster"],"title":"[Cluster] - 2. Redis Cluster","uri":"/en/2025/12/cluster-2.-redis-cluster/"},{"categories":["Algorithm"],"content":"Introduction ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:1:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"What is Dynamic Programming? Dynamic Programming (DP) is often considered one of the most challenging topics in computer science algorithms. However, at its core, it is simply an optimization technique. The fundamental idea of DP is “Don’t Repeat Yourself.” If you have already solved a sub-problem, you should save the result (cache it) so that you never have to calculate it again. By trading a little bit of space (to store results) for time (to avoid re-calculation), DP can turn an inefficient exponential algorithm ($O(2^n)$) into a highly efficient linear one ($O(n)$). A Simple Analogy Imagine I ask you to calculate: $$1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = ?$$ You count them up and tell me: “8”. Now, if I add another + 1 to the end of that equation: $$1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 \\quad \\mathbf{+\\ 1} = ?$$ You will immediately answer: “9”. Why? You didn’t recount the first eight 1s. You remembered that the previous result was 8, and you simply added 1 to it. This is Dynamic Programming. State: You remembered “the sum of the first 8 numbers.” Transition: You used the formula Current Sum = Previous Sum + 1. ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:1:1","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Core Pillars ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:2:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"The Three Core Concepts of Dynamic Programming When solving a DP problem, you must strictly follow three steps. Think of this as your “pre-coding checklist.” If you cannot answer these three points clearly, do not start writing code yet. Define the Array (The Semantics) We use an array (often named dp[]) to store our results. The most critical step is defining the physical meaning of this array. You must be able to complete this sentence: “The value of dp[i] represents…” If your definition is vague, your logic will fail. Bad Definition: dp[i] is the answer for $i$. (Too abstract) Good Definition: dp[i] represents the maximum profit we can generate after selling the $i$-th item. Good Definition: dp[i] represents the minimum number of steps required to reach stair $i$. The State Transition Equation (The Logic) The Transition Equation is not random math; it is a formal description of a decision-making process. To write this equation, you must ask: “How does the current state $i$ relate to the previous candidate states?” The specific mathematical operator you use is strictly determined by the Goal of the Problem. You can categorize almost all DP equations into three abstract patterns: Pattern A: The Aggregator (Counting / Sum) Goal: “How many distinct ways are there to reach state $i$?” The Logic: You are not choosing between options; you are combining them. If you can arrive at the current state from “Option A” or “Option B,” the total number of ways is the sum of both histories. The Abstract Equation: $$dp[i] = dp[\\text{Option A}] + dp[\\text{Option B}] + \\dots$$ The Mindset: Accumulation. Every valid path from the past contributes to the present. Pattern B: The Selector (Optimization / Max or Min) Goal: “What is the maximum profit / minimum cost to reach state $i$?” The Logic: You are in a competition. You compare “Option A” (e.g., taking an action) against “Option B” (e.g., skipping an action). You only care about the winner; the loser is discarded. The Abstract Equation: $$dp[i] = \\max(\\text{Value of Option A}, \\quad \\text{Value of Option B})$$ (Or $\\min$ if you are minimizing cost) The Mindset: Survival of the Fittest. Only the best previous state matters. Pattern C: The Validator (Existence / Boolean) Goal: “Is it possible to reach state $i$?” The Logic: You are checking for connectivity. If there is at least one valid path from a previous state to here, then the current state becomes valid. The Abstract Equation: $$dp[i] = dp[\\text{Option A}] \\lor dp[\\text{Option B}] \\dots$$ (Logical OR operation) The Mindset: Propagation. If the signal reached “Option A”, and “Option A” connects to me, then the signal reaches me. Initialization (The Base Case) The State Transition Equation drives the logic, but it needs a starting point. Without initialization, your loop will try to access negative indexes (like dp[-1]) or calculate based on empty data. You must manually set the values for the smallest sub-problems. If your equation relies on i-1, you usually need to initialize dp[0]. If your equation relies on i-2, you usually need to initialize dp[0] and dp[1]. Think of it like Dominos: Step 3 sets up the first domino. Step 2 ensures that if one falls, the next one falls. Step 1 is the floor they stand on. ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:2:1","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Top-Down Approach (Recursion + Memoization) The Top-Down approach uses recursion to break down the problem from the target state to the base cases. We use memoization to cache results and avoid redundant calculations. ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:3:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category A: Sum Problems (Counting Paths) These problems ask “How many ways?” The key insight is that we add all possible paths that lead to the current state. Climbing Stairs (LeetCode 70) The Problem: You are climbing a staircase. It takes n steps to reach the top. Each time you can either climb 1 step or 2 steps. In how many distinct ways can you climb to the top? Input: n = 3 Output: 3 Explanation: There are three ways to climb to the top: 1 step + 1 step + 1 step 1 step + 2 steps 2 steps + 1 step Solution (Top-Down) 1. Array Definition: memo[i] = The number of distinct ways to reach the $i$-th step. 2. Equation: This is a sum problem. You have two choices: Come from 1 step back Come from 2 steps back $$memo[i] = memo[i-1] + memo[i-2]$$ 3. Base Cases: memo[0] = 1 (One way to stay at ground) memo[1] = 1 (One way: take 1 step) public class ClimbingStairs { private int[] memo; public int climbStairs(int n) { memo = new int[n + 1]; Arrays.fill(memo, -1); return climb(n); } private int climb(int n) { // Base cases if (n == 0) return 1; if (n == 1) return 1; // Check memo if (memo[n] != -1) return memo[n]; // Recursively calculate int fromOneStepBack = climb(n - 1); int fromTwoStepsBack = climb(n - 2); // Store and return memo[n] = fromOneStepBack + fromTwoStepsBack; return memo[n]; } } Unique Paths (LeetCode 62) The Problem: There is a robot on an m x n grid. The robot is initially located at the top-left corner (i.e., grid[0][0]). The robot tries to move to the bottom-right corner (i.e., grid[m-1][n-1]). The robot can only move either down or right at any point in time. Given the two integers m and n, return the number of possible unique paths that the robot can take to reach the bottom-right corner. Input: m = 3, n = 7 Output: 28 Note: The answer can also be computed using combinatorics: $$ C_{m+n-2}^{m-1} $$ Solution (Top-Down) 1. Array Definition: memo[i][j] = The number of unique paths to reach cell $(i, j)$. 2. Equation: This is a sum problem. You have two choices: Come from top (i-1, j) Come from left (i, j-1) $$memo[i][j] = memo[i-1][j] + memo[i][j-1]$$ 3. Base Cases: memo[0][j] = 1 (First row: only one way, go straight right) memo[i][0] = 1 (First column: only one way, go straight down) public class UniquePaths { private int[][] memo; public int uniquePaths(int m, int n) { memo = new int[m][n]; for (int[] row : memo) Arrays.fill(row, -1); return paths(m - 1, n - 1); } private int paths(int i, int j) { // Base cases if (i == 0 || j == 0) return 1; // Check memo if (memo[i][j] != -1) return memo[i][j]; // Recursively calculate int fromTop = paths(i - 1, j); int fromLeft = paths(i, j - 1); // Store and return memo[i][j] = fromTop + fromLeft; return memo[i][j]; } } Unique Paths II (LeetCode 63) The Problem: Similar to Unique Paths, but now the grid has obstacles. An obstacle is marked as 1, and empty space is marked as 0. A path that the robot takes cannot include any square that is an obstacle. Input: obstacleGrid = [[0,0,0],[0,1,0],[0,0,0]] Output: 2 Solution (Top-Down) 1. Array Definition: memo[i][j] = Number of paths to reach $(i, j)$. 2. Equation: Same as Unique Paths, but skip cells with obstacles. $$memo[i][j] = memo[i-1][j] + memo[i][j-1] \\quad \\text{if obstacleGrid[i][j] == 0}$$ 3. Base Cases: If obstacleGrid[0][0] == 1, return 0 (blocked at start) First row/column: propagate 1 until hitting an obstacle public class UniquePathsII { private int[][] memo; private int[][] grid; public int uniquePathsWithObstacles(int[][] obstacleGrid) { if (obstacleGrid[0][0] == 1) return 0; this.grid = obstacleGrid; int m = grid.length; int n = grid[0].length; memo = new int[m][n]; for (int[] row : memo) Arrays.fill(row, -1); return paths(m - 1, n - 1); } private int paths(int i, int j) { // Out of bounds if (i \u003c 0 || j \u003c 0) return 0; // Obstacle if (grid[i][j] == 1) return 0; // Start position if (i == 0 \u0026\u0026 j == 0) return 1; // Check memo if (memo[i][j] != -1) return memo[i][j]; // Recursively calculate int fromTop = paths(i - 1, j","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:3:1","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category B: Max/Min Problems (Optimization) These problems ask “What is the best value?” The key insight is that we choose the optimal option among all possibilities using max() or min(). Min Cost Climbing Stairs (LeetCode 746) The Problem: You are given an integer array cost where cost[i] is the cost of the $i$-th step on a staircase. Once you pay the cost, you can climb either 1 or 2 steps. You can either start from the step with index 0, or the step with index 1. Return the minimum cost to reach the top of the floor (which is one step past the last index). Input: cost = [10, 15, 20] Output: 15 Explanation: You will start at index 1. Pay 15 and climb two steps to reach the top. The total cost is 15. Solution (Top-Down) 1. Array Definition: memo[i] = The minimum cost to reach step $i$. 2. Equation: This is a min problem. You have two choices: Come from 1 step back Come from 2 steps back $$memo[i] = \\min(\\text{costFrom1Back}, \\quad \\text{costFrom2Back})$$ 3. Base Cases: memo[0] = 0 (Start at ground floor is free) memo[1] = 0 (Can start at index 0 or 1 for free) public class MinCostClimbingStairs { private int[] memo; private int[] cost; public int minCostClimbingStairs(int[] cost) { this.cost = cost; int n = cost.length; memo = new int[n + 1]; Arrays.fill(memo, -1); return minCost(n); } private int minCost(int step) { // Base cases if (step == 0) return 0; if (step == 1) return 0; // Check memo if (memo[step] != -1) return memo[step]; // Choice 1: from 1 step back int costFrom1Back = minCost(step - 1) + cost[step - 1]; // Choice 2: from 2 steps back int costFrom2Back = minCost(step - 2) + cost[step - 2]; // Store and return min memo[step] = Math.min(costFrom1Back, costFrom2Back); return memo[step]; } } House Robber (LeetCode 198) The Problem: You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed. The only constraint stopping you is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night. Given an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police. Input: nums = [1, 2, 3, 1] Output: 4 Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4. Solution (Top-Down) 1. Array Definition: memo[i] = The max money we can rob from houses 0...i. 2. Equation: This is a max problem. For house $i$, you have two choices: Don’t rob it: Value is same as memo[i-1]. Rob it: Value is current cash + memo[i-2] (skip adjacent house). $$memo[i] = \\max(\\text{skipHouse}, \\quad \\text{robHouse})$$ 3. Base Cases: memo[0] = nums[0] memo[1] = max(nums[0], nums[1]) public class HouseRobber { private int[] memo; private int[] nums; public int rob(int[] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums[0]; this.nums = nums; memo = new int[nums.length]; Arrays.fill(memo, -1); return maxRob(nums.length - 1); } private int maxRob(int i) { // Base cases if (i == 0) return nums[0]; if (i == 1) return Math.max(nums[0], nums[1]); // Check memo if (memo[i] != -1) return memo[i]; // Choice 1: skip this house int skipHouse = maxRob(i - 1); // Choice 2: rob this house int robHouse = nums[i] + maxRob(i - 2); // Store and return max memo[i] = Math.max(skipHouse, robHouse); return memo[i]; } } Coin Change (LeetCode 322) The Problem: You are given an integer array coins representing coins of different denominations and an integer amount representing a total amount of money. Return the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1. Input: coins = [1, 2, 5], amount = 11 Output: 3 Explanation: 11 = 5 + 5 + 1 (3 coins). Solution (Top-Down) 1. Array Definition: memo[amount] = The minimum coins needed to make this amount. 2. Equation: This is a min pro","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:3:2","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category C: Exist Problems (Possibility / Boolean) These problems ask “Is it possible?” The key insight is that we use logical OR - if any path works, the answer is true. Word Break (LeetCode 139) The Problem: Given a string s and a dictionary of strings wordDict, return true if s can be segmented into a space-separated sequence of one or more dictionary words. Input: s = \"leetcode\", wordDict = [\"leet\",\"code\"] Output: true Explanation: Return true because “leetcode” can be segmented as “leet code”. Solution (Top-Down) 1. Array Definition: memo[i] = Whether substring s[i...] can be segmented. 2. Equation: This is an exist problem. Try each word: $$memo[i] = \\bigvee_{word} (\\text{s starts with word AND } memo[i + word.length])$$ 3. Base Cases: memo[s.length()] = true (empty string is valid) public class WordBreak { private Boolean[] memo; private String s; private Set\u003cString\u003e wordSet; public boolean wordBreak(String s, List\u003cString\u003e wordDict) { this.s = s; this.wordSet = new HashSet\u003c\u003e(wordDict); memo = new Boolean[s.length()]; return canBreak(0); } private boolean canBreak(int start) { // Base case if (start == s.length()) return true; // Check memo if (memo[start] != null) return memo[start]; // Try each word for (String word : wordSet) { if (tryWord(start, word)) { memo[start] = true; return true; } } // No word works memo[start] = false; return false; } private boolean tryWord(int start, String word) { int end = start + word.length(); if (end \u003e s.length()) return false; if (!s.substring(start, end).equals(word)) return false; return canBreak(end); } } Partition Equal Subset Sum (LeetCode 416) The Problem: Given an integer array nums, return true if you can partition the array into two subsets such that the sum of the elements in both subsets is equal. Input: nums = [1,5,11,5] Output: true Explanation: The array can be partitioned as [1, 5, 5] and [11]. Solution (Top-Down) 1. Array Definition: memo[i][sum] = Whether we can achieve sum using elements from index i onwards. 2. Equation: This is an exist problem. For each element: Include it, OR Exclude it $$memo[i][sum] = \\text{include}(i, sum) \\lor \\text{exclude}(i, sum)$$ 3. Base Cases: memo[i][0] = true (sum of 0 is always achievable) If i \u003e= nums.length and sum \u003e 0, return false public class PartitionEqualSubsetSum { private Boolean[][] memo; private int[] nums; public boolean canPartition(int[] nums) { int totalSum = 0; for (int num : nums) totalSum += num; // If total is odd, can't partition equally if (totalSum % 2 != 0) return false; this.nums = nums; int target = totalSum / 2; memo = new Boolean[nums.length][target + 1]; return canAchieve(0, target); } private boolean canAchieve(int i, int sum) { // Base cases if (sum == 0) return true; if (i \u003e= nums.length || sum \u003c 0) return false; // Check memo if (memo[i][sum] != null) return memo[i][sum]; // Choice 1: include current number boolean include = includeNum(i, sum); // Choice 2: exclude current number boolean exclude = excludeNum(i, sum); // Store and return memo[i][sum] = include || exclude; return memo[i][sum]; } private boolean includeNum(int i, int sum) { return canAchieve(i + 1, sum - nums[i]); } private boolean excludeNum(int i, int sum) { return canAchieve(i + 1, sum); } } Target Sum (LeetCode 494) The Problem: You are given an integer array nums and an integer target. You want to build an expression by adding '+' or '-' before each integer in nums and then concatenate all the integers. Return the number of different expressions that you can build, which evaluates to target. Input: nums = [1,1,1,1,1], target = 3 Output: 5 Explanation: There are 5 ways: -1+1+1+1+1, +1-1+1+1+1, +1+1-1+1+1, +1+1+1-1+1, +1+1+1+1-1 Solution (Top-Down) 1. Array Definition: memo[i][sum] = Number of ways to achieve sum using elements from index i onwards. 2. Equation: This is a sum problem (counting ways). For each number: Add it, + Subtract it $$memo[i][sum] = \\text{add}(i, sum) + \\text{subtract}(i, sum)$$ 3. Base Cases: If i == nums.lengt","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:3:3","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Bottom-Up Approach (Iteration) ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Why Bottom-Up is Preferred While DP is often introduced using recursion (Top-Down), the Bottom-Up approach has significant advantages: No Stack Overflow: Iterative solutions use constant stack space. Better Performance: No function call overhead. Clearer Flow: The computation order is explicit and easy to trace. Space Optimization: Often easier to reduce space complexity (e.g., rolling array technique). ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:1","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"The Mindset Shift: From Recursion to Iteration Top-Down Thinking: “To solve problem f(n), I need to first solve f(n-1) and f(n-2).” This is goal-oriented: start from the target and work backwards. Bottom-Up Thinking: “I’ll solve the smallest problems first: f(0), f(1), then build up to f(n).” This is foundation-oriented: start from the base and work forwards. ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:2","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Conversion Strategy To convert a Top-Down solution to Bottom-Up: Identify the dependencies: What does dp[i] depend on? If it depends on dp[i-1], loop from 1 to n If it depends on smaller indices, loop forward If it depends on larger indices, loop backward Initialize base cases: Set dp[0], dp[1], etc. directly (no recursion needed) Fill the DP table iteratively: Use loops instead of recursion The loop order matches the dependency direction Apply the same state transition equation Return the final answer: Usually dp[n] or dp[n-1] ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:3","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category A: Sum Problems (Bottom-Up) Climbing Stairs (Bottom-Up) public class ClimbingStairsBottomUp { public int climbStairs(int n) { if (n \u003c= 1) return 1; // 1. Define array int[] dp = new int[n + 1]; // 2. Initialize base cases dp[0] = 1; dp[1] = 1; // 3. Fill table iteratively for (int i = 2; i \u003c= n; i++) { dp[i] = chooseOneStep(dp, i) + chooseTwoSteps(dp, i); } // 4. Return final answer return dp[n]; } private int chooseOneStep(int[] dp, int i) { return dp[i - 1]; } private int chooseTwoSteps(int[] dp, int i) { return dp[i - 2]; } } Space Optimization: Since we only need the last two values, we can use two variables instead of an array: public int climbStairsOptimized(int n) { if (n \u003c= 1) return 1; int prev2 = 1; // dp[i-2] int prev1 = 1; // dp[i-1] for (int i = 2; i \u003c= n; i++) { int current = prev1 + prev2; prev2 = prev1; prev1 = current; } return prev1; } Unique Paths (Bottom-Up) public class UniquePathsBottomUp { public int uniquePaths(int m, int n) { // 1. Define array int[][] dp = new int[m][n]; // 2. Initialize base cases for (int i = 0; i \u003c m; i++) dp[i][0] = 1; // First column for (int j = 0; j \u003c n; j++) dp[0][j] = 1; // First row // 3. Fill table iteratively for (int i = 1; i \u003c m; i++) { for (int j = 1; j \u003c n; j++) { dp[i][j] = fromTop(dp, i, j) + fromLeft(dp, i, j); } } // 4. Return final answer return dp[m - 1][n - 1]; } private int fromTop(int[][] dp, int i, int j) { return dp[i - 1][j]; } private int fromLeft(int[][] dp, int i, int j) { return dp[i][j - 1]; } } Space Optimization: We only need the current row and previous row: public int uniquePathsOptimized(int m, int n) { int[] dp = new int[n]; Arrays.fill(dp, 1); for (int i = 1; i \u003c m; i++) { for (int j = 1; j \u003c n; j++) { dp[j] = dp[j] + dp[j - 1]; // fromTop + fromLeft } } return dp[n - 1]; } Unique Paths II (Bottom-Up) public class UniquePathsIIBottomUp { public int uniquePathsWithObstacles(int[][] obstacleGrid) { if (obstacleGrid[0][0] == 1) return 0; int m = obstacleGrid.length; int n = obstacleGrid[0].length; // 1. Define array int[][] dp = new int[m][n]; // 2. Initialize base cases dp[0][0] = 1; // First column for (int i = 1; i \u003c m; i++) { dp[i][0] = (obstacleGrid[i][0] == 0 \u0026\u0026 dp[i - 1][0] == 1) ? 1 : 0; } // First row for (int j = 1; j \u003c n; j++) { dp[0][j] = (obstacleGrid[0][j] == 0 \u0026\u0026 dp[0][j - 1] == 1) ? 1 : 0; } // 3. Fill table iteratively for (int i = 1; i \u003c m; i++) { for (int j = 1; j \u003c n; j++) { if (obstacleGrid[i][j] == 1) { dp[i][j] = 0; // Obstacle } else { dp[i][j] = fromTop(dp, i, j) + fromLeft(dp, i, j); } } } // 4. Return final answer return dp[m - 1][n - 1]; } private int fromTop(int[][] dp, int i, int j) { return dp[i - 1][j]; } private int fromLeft(int[][] dp, int i, int j) { return dp[i][j - 1]; } } Decode Ways (Bottom-Up) public class DecodeWaysBottomUp { public int numDecodings(String s) { if (s.charAt(0) == '0') return 0; int n = s.length(); // 1. Define array int[] dp = new int[n + 1]; // 2. Initialize base cases dp[0] = 1; // Empty string dp[1] = 1; // First character (already validated non-zero) // 3. Fill table iteratively for (int i = 2; i \u003c= n; i++) { // Choice 1: decode single digit int single = decodeSingle(s, i, dp); // Choice 2: decode two digits int double_ = decodeDouble(s, i, dp); dp[i] = single + double_; } // 4. Return final answer return dp[n]; } private int decodeSingle(String s, int i, int[] dp) { if (s.charAt(i - 1) != '0') { return dp[i - 1]; } return 0; } private int decodeDouble(String s, int i, int[] dp) { int twoDigit = Integer.parseInt(s.substring(i - 2, i)); if (twoDigit \u003e= 10 \u0026\u0026 twoDigit \u003c= 26) { return dp[i - 2]; } return 0; } } Fibonacci Number (Bottom-Up) public class FibonacciBottomUp { public int fib(int n) { if (n \u003c= 1) return n; // 1. Define array int[] dp = new int[n + 1]; // 2. Initialize base cases dp[0] = 0; dp[1] = 1; // 3. Fill table iteratively for (int i = 2; i \u003c= n; i++) { dp[i] = fromPrevOne(dp, i) + fromPrevTwo(dp, i); } // 4. Return final answer return dp[n]; } private int fromPrev","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:4","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category B: Max/Min Problems (Bottom-Up) Min Cost Climbing Stairs (Bottom-Up) public class MinCostClimbingStairsBottomUp { public int minCostClimbingStairs(int[] cost) { int n = cost.length; // 1. Define array int[] dp = new int[n + 1]; // 2. Initialize base cases dp[0] = 0; // Start at ground (free) dp[1] = 0; // Start at first step (free) // 3. Fill table iteratively for (int i = 2; i \u003c= n; i++) { int costFrom1Back = costFromOneBack(dp, cost, i); int costFrom2Back = costFromTwoBack(dp, cost, i); dp[i] = Math.min(costFrom1Back, costFrom2Back); } // 4. Return final answer return dp[n]; } private int costFromOneBack(int[] dp, int[] cost, int i) { return dp[i - 1] + cost[i - 1]; } private int costFromTwoBack(int[] dp, int[] cost, int i) { return dp[i - 2] + cost[i - 2]; } } House Robber (Bottom-Up) public class HouseRobberBottomUp { public int rob(int[] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums[0]; int n = nums.length; // 1. Define array int[] dp = new int[n]; // 2. Initialize base cases dp[0] = nums[0]; dp[1] = Math.max(nums[0], nums[1]); // 3. Fill table iteratively for (int i = 2; i \u003c n; i++) { int skipHouse = skipCurrent(dp, i); int robHouse = robCurrent(dp, nums, i); dp[i] = Math.max(skipHouse, robHouse); } // 4. Return final answer return dp[n - 1]; } private int skipCurrent(int[] dp, int i) { return dp[i - 1]; } private int robCurrent(int[] dp, int[] nums, int i) { return dp[i - 2] + nums[i]; } } Coin Change (Bottom-Up) public class CoinChangeBottomUp { public int coinChange(int[] coins, int amount) { // 1. Define array int[] dp = new int[amount + 1]; // 2. Initialize base cases Arrays.fill(dp, amount + 1); // Infinity placeholder dp[0] = 0; // 0 coins for amount 0 // 3. Fill table iteratively for (int amt = 1; amt \u003c= amount; amt++) { for (int coin : coins) { if (coin \u003c= amt) { dp[amt] = Math.min(dp[amt], useCoin(dp, amt, coin)); } } } // 4. Return final answer return dp[amount] \u003e amount ? -1 : dp[amount]; } private int useCoin(int[] dp, int amt, int coin) { return dp[amt - coin] + 1; } } Longest Increasing Subsequence (Bottom-Up) public class LongestIncreasingSubsequenceBottomUp { public int lengthOfLIS(int[] nums) { int n = nums.length; // 1. Define array int[] dp = new int[n]; // 2. Initialize base cases Arrays.fill(dp, 1); // Each element is a LIS of length 1 // 3. Fill table iteratively for (int i = 1; i \u003c n; i++) { for (int j = 0; j \u003c i; j++) { if (nums[j] \u003c nums[i]) { dp[i] = Math.max(dp[i], extendFrom(dp, j)); } } } // 4. Return final answer (max of all dp values) int maxLen = 0; for (int len : dp) { maxLen = Math.max(maxLen, len); } return maxLen; } private int extendFrom(int[] dp, int j) { return dp[j] + 1; } } Longest Common Subsequence (Bottom-Up) public class LongestCommonSubsequenceBottomUp { public int longestCommonSubsequence(String text1, String text2) { int m = text1.length(); int n = text2.length(); // 1. Define array int[][] dp = new int[m + 1][n + 1]; // 2. Initialize base cases (already 0 by default) // dp[0][j] = 0 and dp[i][0] = 0 // 3. Fill table iteratively for (int i = 1; i \u003c= m; i++) { for (int j = 1; j \u003c= n; j++) { if (text1.charAt(i - 1) == text2.charAt(j - 1)) { dp[i][j] = matchChars(dp, i, j); } else { dp[i][j] = skipChar(dp, i, j); } } } // 4. Return final answer return dp[m][n]; } private int matchChars(int[][] dp, int i, int j) { return 1 + dp[i - 1][j - 1]; } private int skipChar(int[][] dp, int i, int j) { return Math.max(dp[i - 1][j], dp[i][j - 1]); } } Maximum Subarray (Bottom-Up) public class MaximumSubarrayBottomUp { public int maxSubArray(int[] nums) { int n = nums.length; // 1. Define array int[] dp = new int[n]; // 2. Initialize base case dp[0] = nums[0]; // 3. Fill table iteratively int maxSum = dp[0]; for (int i = 1; i \u003c n; i++) { int startFresh = nums[i]; int extendPrev = nums[i] + dp[i - 1]; dp[i] = Math.max(startFresh, extendPrev); maxSum = Math.max(maxSum, dp[i]); } // 4. Return final answer return maxSum; } } Space Optimization: public int ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:5","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Category C: Exist Problems (Bottom-Up) Word Break (Bottom-Up) public class WordBreakBottomUp { public boolean wordBreak(String s, List\u003cString\u003e wordDict) { Set\u003cString\u003e wordSet = new HashSet\u003c\u003e(wordDict); int n = s.length(); // 1. Define array boolean[] dp = new boolean[n + 1]; // 2. Initialize base case dp[0] = true; // Empty string is valid // 3. Fill table iteratively for (int i = 1; i \u003c= n; i++) { for (String word : wordSet) { if (canUseWord(s, dp, i, word)) { dp[i] = true; break; // Found one valid way } } } // 4. Return final answer return dp[n]; } private boolean canUseWord(String s, boolean[] dp, int i, String word) { int len = word.length(); if (len \u003e i) return false; if (!dp[i - len]) return false; return s.substring(i - len, i).equals(word); } } Partition Equal Subset Sum (Bottom-Up) public class PartitionEqualSubsetSumBottomUp { public boolean canPartition(int[] nums) { int totalSum = 0; for (int num : nums) totalSum += num; if (totalSum % 2 != 0) return false; int target = totalSum / 2; // 1. Define array boolean[] dp = new boolean[target + 1]; // 2. Initialize base case dp[0] = true; // Sum of 0 is always achievable // 3. Fill table iteratively for (int num : nums) { // Traverse backwards to avoid using same element twice for (int sum = target; sum \u003e= num; sum--) { if (dp[sum - num]) { dp[sum] = includeNum(dp, sum, num); } } } // 4. Return final answer return dp[target]; } private boolean includeNum(boolean[] dp, int sum, int num) { return dp[sum - num]; // If we can make (sum - num), we can make sum } } Target Sum (Bottom-Up) public class TargetSumBottomUp { public int findTargetSumWays(int[] nums, int target) { int sum = 0; for (int num : nums) sum += num; // Mathematical insight: P - N = target, P + N = sum // Therefore: P = (target + sum) / 2 if (sum \u003c Math.abs(target) || (target + sum) % 2 != 0) return 0; int positiveSum = (target + sum) / 2; // 1. Define array int[] dp = new int[positiveSum + 1]; // 2. Initialize base case dp[0] = 1; // One way to make sum 0 // 3. Fill table iteratively for (int num : nums) { for (int s = positiveSum; s \u003e= num; s--) { dp[s] += includeNum(dp, s, num); } } // 4. Return final answer return dp[positiveSum]; } private int includeNum(int[] dp, int s, int num) { return dp[s - num]; } } Can Jump (Bottom-Up) public class CanJumpBottomUp { public boolean canJump(int[] nums) { int n = nums.length; // 1. Define array boolean[] dp = new boolean[n]; // 2. Initialize base case dp[0] = true; // Starting position is reachable // 3. Fill table iteratively for (int i = 0; i \u003c n; i++) { if (!dp[i]) continue; // Can't reach this position // Mark all reachable positions from here int maxJump = nums[i]; for (int jump = 1; jump \u003c= maxJump \u0026\u0026 i + jump \u003c n; jump++) { dp[i + jump] = tryJump(i, jump); } } // 4. Return final answer return dp[n - 1]; } private boolean tryJump(int pos, int jump) { return true; // Position is reachable } } Greedy Optimization (Better approach): public boolean canJumpGreedy(int[] nums) { int maxReach = 0; for (int i = 0; i \u003c nums.length; i++) { if (i \u003e maxReach) return false; // Can't reach this position maxReach = Math.max(maxReach, i + nums[i]); } return true; } Perfect Squares (Bottom-Up) public class PerfectSquaresBottomUp { public int numSquares(int n) { // 1. Define array int[] dp = new int[n + 1]; // 2. Initialize base cases Arrays.fill(dp, Integer.MAX_VALUE); dp[0] = 0; // 3. Fill table iteratively for (int i = 1; i \u003c= n; i++) { for (int j = 1; j * j \u003c= i; j++) { dp[i] = Math.min(dp[i], useSquare(dp, i, j)); } } // 4. Return final answer return dp[n]; } private int useSquare(int[] dp, int i, int j) { return dp[i - j * j] + 1; } } Stone Game (Bottom-Up) public class StoneGameBottomUp { public boolean stoneGame(int[] piles) { int n = piles.length; // 1. Define array int[][] dp = new int[n][n]; // 2. Initialize base cases for (int i = 0; i \u003c n; i++) { dp[i][i] = piles[i]; // Only one pile } // 3. Fill table iteratively // len is the subarray length for (int len = 2; len ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:4:6","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Summary Table Problem Category Pattern Time Space Climbing Stairs Sum dp[i] = dp[i-1] + dp[i-2] O(n) O(1)* Unique Paths Sum dp[i][j] = dp[i-1][j] + dp[i][j-1] O(mn) O(n)* Unique Paths II Sum Same as above with obstacles O(mn) O(n)* Decode Ways Sum dp[i] = dp[i-1] + dp[i-2] O(n) O(n) Fibonacci Sum dp[i] = dp[i-1] + dp[i-2] O(n) O(1)* Count Vowel Strings Sum dp[n][v] = Σ dp[n-1][v'] O(n) O(n) Min Cost Stairs Min dp[i] = min(dp[i-1], dp[i-2]) + cost O(n) O(1)* House Robber Max dp[i] = max(dp[i-1], dp[i-2] + nums[i]) O(n) O(1)* Coin Change Min dp[i] = min(1 + dp[i-coin]) O(n×m) O(n) Longest Increasing Subsequence Max dp[i] = max(dp[j] + 1) O(n²) O(n) Longest Common Subsequence Max Match: 1 + dp[i-1][j-1] Skip: max(dp[i-1][j], dp[i][j-1]) O(mn) O(n)* Maximum Subarray Max dp[i] = max(nums[i], dp[i-1] + nums[i]) O(n) O(1)* Word Break Exist dp[i] = ∨ dp[i-len(word)] O(n×m×k) O(n) Partition Equal Subset Sum Exist dp[sum] = dp[sum] ∨ dp[sum-num] O(n×sum) O(sum) Target Sum Sum dp[i][sum] = dp[i+1][sum+num] + dp[i+1][sum-num] O(n×sum) O(sum) Can Jump Exist dp[i] = ∨ dp[i+jump] O(n²) O(n) Perfect Squares Min dp[n] = min(1 + dp[n-i²]) O(n√n) O(n) Stone Game Max dp[i][j] = max(piles[i] - dp[i+1][j], piles[j] - dp[i][j-1]) O(n²) O(n²) *Space complexity can be optimized using rolling array or variables. ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:5:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Algorithm"],"content":"Key Takeaways Identify the Pattern: Sum problems → Add all possibilities Max/Min problems → Choose the best option Exist problems → Check if any option works Top-Down vs Bottom-Up: Top-Down: Natural recursion, easier to think, but risk of stack overflow Bottom-Up: Iterative, better performance, easier to optimize space Space Optimization: If dp[i] only depends on dp[i-1] and dp[i-2], use two variables For 2D DP, if dp[i][j] only depends on previous row, use rolling array The Three Steps (Never Skip): Define what dp[i] means Derive the state transition equation Initialize the base cases Practice Makes Perfect: Start with easy problems (Fibonacci, Climbing Stairs) Progress to medium (House Robber, Coin Change) Master hard problems (LCS, Stone Game) ","date":"2025-11-27","objectID":"/en/2025/11/algorithm-1.-dynamic-programming/:6:0","tags":["Algorithm"],"title":"[Algorithm] 1. Dynamic Programming","uri":"/en/2025/11/algorithm-1.-dynamic-programming/"},{"categories":["Java","Spring"],"content":"Preface When handling high-concurrency scenarios in Spring applications, proper use of asynchronous programming and thread pool management is crucial. This article provides an in-depth analysis of Spring’s default thread pool, custom thread pools, and thread reuse mechanisms through practical code examples. ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:1:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Why Use Custom Thread Pools? When a Spring Boot application starts, it automatically configures a global task executor (TaskExecutor) with the default name applicationTaskExecutor. However, using Spring’s default thread pool directly in production environments is not recommended for the following reasons: Lack of Isolation: All asynchronous tasks share the same thread pool, causing tasks from different business modules to interfere with each other Difficult to Monitor: Unable to perform fine-grained thread pool monitoring and tuning for specific business scenarios Single Configuration: Default configuration may not meet the performance needs of all business scenarios Best Practice: Customize thread pools based on business scenarios to achieve task isolation and fine-grained management. ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:2:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Custom Thread Pool Configuration Here’s a typical custom thread pool configuration example: private static final AtomicInteger COUNT = new AtomicInteger(0); private static final Executor EXECUTOR = new ThreadPoolExecutor( 10, // Core pool size 10, // Maximum pool size 10, // Keep-alive time for idle threads TimeUnit.SECONDS, new ArrayBlockingQueue\u003c\u003e(10), // Work queue capacity r -\u003e new Thread(r, String.format(\"customer-t-%s\", COUNT.addAndGet(1))) // Custom thread naming ); ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:3:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Configuration Breakdown: Core Pool Size = Maximum Pool Size = 10: Fixed-size thread pool, avoids frequent thread creation and destruction Queue Capacity = 10: When all 10 threads are working, up to 10 more tasks can be queued Custom Thread Naming: customer-t-{number}, convenient for log tracking and problem diagnosis ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:3:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Async Interface vs Sync Interface Comparison ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:4:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Async Interface Implementation (asyncQuery1) @GetMapping(\"async/query1\") public CompletionStage\u003cString\u003e asyncQuery1() { log.info(\"async query start\"); // Executed by Tomcat thread return CompletableFuture.supplyAsync(() -\u003e { log.info(\"async query sleep start\"); // Executed by customer-t thread ThreadUtils.sleep(10000); // Simulate time-consuming operation log.info(\"async query sleep done\"); return \"done\"; }, EXECUTOR); } Characteristics: Non-blocking: Tomcat thread is immediately released and can handle other requests High Throughput: Suitable for I/O-intensive tasks Thread Switching: Request switches between Tomcat thread and custom thread pool ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:4:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Sync Interface Implementation (syncQuery1) @GetMapping(\"sync/query1\") public String syncQuery1() throws InterruptedException { log.info(\"sync query start\"); // Executed by Tomcat thread final CountDownLatch latch = new CountDownLatch(1); EXECUTOR.execute(() -\u003e { log.info(\"sync query sleep start\"); // Executed by customer-t thread ThreadUtils.sleep(1000); latch.countDown(); }); latch.await(); // Tomcat thread blocks and waits log.info(\"sync query done\"); // Executed by Tomcat thread return \"done\"; } Characteristics: Blocking Wait: Tomcat thread is blocked by CountDownLatch, cannot handle other requests Resource Waste: Occupies both Tomcat thread and Worker thread, two threads doing the work of one Essentially Synchronous: Despite using a custom thread pool, the Tomcat thread waits continuously, completely failing to leverage async advantages Use Cases: Almost none! Better to execute directly in Tomcat thread, which also saves a Worker thread ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:4:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Thread Reuse in Practice Sending 20 concurrent requests via load testing tool to observe thread behavior differences between async and sync interfaces. ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:5:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Async Interface Concurrency Test Sending 20 concurrent requests to /goody/async/query1 (each task takes 10 seconds): // ============ Phase 1: First 10 requests immediately received by Tomcat thread and submitted ============ 09:53:20.896 INFO [io-50012-exec-1] async query start ← Tomcat thread quickly released 09:53:20.899 INFO [customer-t-1] async query sleep start ← Worker thread 1 starts executing 09:53:21.026 INFO [io-50012-exec-1] async query start ← Tomcat thread receives new request again 09:53:21.026 INFO [customer-t-2] async query sleep start ← Worker thread 2 starts executing 09:53:21.186 INFO [io-50012-exec-1] async query start 09:53:21.187 INFO [customer-t-3] async query sleep start ... 09:53:22.261 INFO [io-50012-exec-1] async query start 09:53:22.261 INFO [customer-t-10] async query sleep start ← All 10 threads fully occupied // ============ Phase 2: Requests 11-20 enter queue to wait ============ 09:53:22.411 INFO [io-50012-exec-1] async query start ← 11th request, enters queue 09:53:22.597 INFO [io-50012-exec-1] async query start ← 12th request, enters queue 09:53:22.732 INFO [io-50012-exec-1] async query start ← ...continues to 20th ... 09:53:24.048 INFO [io-50012-exec-1] async query start ← 20th request, queue full // ============ Phase 3: 21st request triggers rejection policy ============ 09:53:24.065 ERROR [io-50012-exec-1] RejectedExecutionException: ThreadPoolExecutor@79a3d00d[Running, pool size = 10, active threads = 10, queued tasks = 10] ↑ Thread pool status: 10 busy threads + 10 queued tasks = full capacity // ============ Phase 4: Thread reuse begins - Key phenomenon! ============ 09:53:30.313 INFO [customer-t-1] async query sleep done ← Thread 1 completes 1st task 09:53:30.313 INFO [customer-t-1] async query done 09:53:30.314 INFO [customer-t-1] async query sleep start ← Thread 1 immediately executes 11th task (reused!) 09:53:31.041 INFO [customer-t-2] async query sleep done ← Thread 2 completes 2nd task 09:53:31.041 INFO [customer-t-2] async query sleep start ← Thread 2 immediately executes 12th task (reused!) 09:53:31.197 INFO [customer-t-3] async query sleep done 09:53:31.197 INFO [customer-t-3] async query sleep start ← Thread 3 reused // ... All 10 threads reused sequentially, processing queued tasks 11-20 // ============ Phase 5: Second round of tasks all completed ============ 09:53:40.320 INFO [customer-t-1] async query sleep done ← Thread 1 completes 11th task 09:53:41.048 INFO [customer-t-2] async query sleep done ← Thread 2 completes 12th task ... Key Observations: Strong Concurrency: Tomcat thread (io-50012-exec-1) received 20 requests in 2 seconds, averaging 100ms per request Fixed Threads: Only customer-t-1 through customer-t-10 Worker threads throughout Thread Reuse: customer-t-1 immediately executes the 11th task after completing the 1st task at 09:53:30 (only 1ms interval) Rejection Policy: When exceeding capacity (10 threads + 10 queue), the 21st request is rejected ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:5:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Sync Interface Serial Execution Sending 20 concurrent requests to /goody/sync/query1 (each task takes 1 second): // ============ Serial Processing: Tomcat thread blocked ============ 09:54:02.401 INFO [io-50012-exec-1] sync query start ← Tomcat thread handles 1st request 09:54:02.401 INFO [customer-t-1] sync query sleep start ← Worker thread executes 09:54:03.407 INFO [customer-t-1] sync query sleep done ← Completes after 1 second 09:54:03.407 INFO [io-50012-exec-1] sync query done ← Tomcat thread then returns 09:54:03.409 INFO [io-50012-exec-1] sync query start ← Handles 2nd request 09:54:03.409 INFO [customer-t-2] sync query sleep start 09:54:04.416 INFO [customer-t-2] sync query sleep done 09:54:04.416 INFO [io-50012-exec-1] sync query done 09:54:04.418 INFO [io-50012-exec-1] sync query start ← Handles 3rd request 09:54:04.418 INFO [customer-t-3] sync query sleep start ... // ============ Thread reuse also exists ============ 09:54:12.490 INFO [io-50012-exec-1] sync query start ← 11th request 09:54:12.490 INFO [customer-t-1] sync query sleep start ← Thread 1 reused 09:54:13.500 INFO [customer-t-1] sync query sleep done 09:54:13.500 INFO [io-50012-exec-1] sync query done Comparative Analysis: Dimension Async Interface Sync Interface Tomcat Thread Quickly released, receives 20 requests in 2 seconds Blocked, takes 20 seconds to process 20 requests Concurrency Can handle 20 simultaneously (10 threads + 10 queue) Can only process serially, one after another Worker Thread Reuse ✅ Exists (customer-t-1 handles 1st and 11th tasks) ✅ Exists (customer-t-1 handles 1st and 11th tasks) Total Time ~20 seconds (10 seconds × 2 rounds) ~20 seconds (1 second × 20) Thread Utilization High (Tomcat idle, Worker busy) Low (Tomcat + Worker both occupied, doing one job) System Throughput High (Tomcat thread can handle other requests) Low (Tomcat thread occupied) Async Nature ✅ Truly async, releases main thread ❌ Fake async, essentially sync waiting (two threads doing one job, even slower) Key Conclusion: Although the sync interface also demonstrates Worker thread reuse, it essentially doesn’t leverage async advantages. Instead, it brings additional overhead: Tomcat thread blocked → Cannot handle other requests Worker thread executes → Occupies thread pool resources Two threads cooperating to complete one task is worse than executing directly in Tomcat thread, which also saves thread switching overhead This approach is an anti-pattern in production environments, used only for comparison to demonstrate async advantages. ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:5:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Core Mechanism of Thread Reuse ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:6:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Producer-Consumer Model Java thread pool’s thread reuse is based on the Producer-Consumer Model: Worker Thread Loop: Worker threads in the thread pool continuously fetch tasks from BlockingQueue Task Queue: New tasks are submitted to the queue, and idle threads immediately retrieve and execute them Reuse Advantages: Avoids overhead of frequent thread creation and destruction (context switching, memory allocation) ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:6:1","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Similarities with IO Multiplexing Core: Async thread pools are essentially “multiplexing” thinking at the application layer. Although implementation mechanisms differ, the approach to solving problems is highly similar to IO multiplexing. Similarities Core Idea: Using Limited Resources to Handle Massive Requests IO Multiplexing: 1 thread monitors N socket connections via epoll/select Async Thread Pool: A small number of Tomcat threads handle N concurrent requests (through quick release) Non-blocking Mode IO Multiplexing: Main thread doesn’t block on a single IO operation, polls waiting for multiple IO events to be ready Async Thread Pool: Tomcat thread doesn’t block on time-consuming tasks, immediately returns to handle next request Event Notification Mechanism IO Multiplexing: epoll notifies which socket is readable/writable Async Thread Pool: CompletableFuture notifies task completion Essential Differences Dimension IO Multiplexing Async Thread Pool Reuse Object Reuse thread (single thread handles multiple IO) Reuse Tomcat thread (quick release) Use Case Network IO-intensive CPU/IO mixed Implementation Level OS level (epoll/select) Application level (thread pool scheduling) Typical Applications Netty, Redis, Nginx Spring WebFlux, Traditional Web Design Pattern Reactor pattern Producer-Consumer pattern Analogy IO Multiplexing: ┌─────────────┐ │ Event Loop │ ──monitor──\u003e [Socket1, Socket2, ..., SocketN] │ (1 thread) │ Handle whichever is ready └─────────────┘ Async Thread Pool: ┌─────────────┐ │ Tomcat Pool │ ──quick release──\u003e [Request1, Request2, ..., RequestN] │ (200 threads) │ Handed to Worker pool for async processing └─────────────┘ Conclusion: Although underlying mechanisms differ, both are solving the core problem of “how to handle high concurrency with limited resources”. Async thread pools can be understood as multiplexing thinking implemented at the application layer. ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:6:2","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["Java","Spring"],"content":"Summary This article reveals the importance of custom thread pools and thread reuse mechanisms by comparing async and sync interface implementations. Key points: ✅ Custom thread pools achieve business isolation and fine-grained management ✅ Async interfaces improve system throughput by releasing Tomcat threads ✅ Thread reuse avoids overhead of frequent thread creation and destruction ✅ Properly configure thread pool parameters to avoid resource waste or task rejection ✅ Thread reuse process can be clearly observed through thread names in logs In actual production environments, it’s also necessary to continuously optimize thread pool configuration by combining monitoring metrics (thread pool activity, queue length, rejection count, etc.). ","date":"2025-10-02","objectID":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/:7:0","tags":["Java","Spring","web","Thread","Efficiency"],"title":"[Spring] 2. Analysis of Custom Thread Pools and Thread Reuse in Spring Async Interfaces","uri":"/en/2025/10/spring-2.-spring-async-threadpool-and-thread-reuse/"},{"categories":["MySQL"],"content":"What is an Index? Index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:1:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Index Types ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:2:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"By Data Structure Hash Index Hash Index is based on a hash table data structure. It uses a hash function to map keys to specific locations in the hash table, allowing for very fast data retrieval. Algorithm Complexity: O(1) Advantages: Very fast for equality searches (e.g., =). Disadvantages: Not suitable for range queries (e.g., \u003c, \u003e, BETWEEN). Hash collisions can occur, leading to performance degradation. why not support order by let us see the algorithm of hash index We see that hash index which is k-v structure. So v is not sorted, so it can not support order by. BTree(B-Tree) Index why not AVL or Red-Black Tree As to Tree Index, we can think about balanced search tree like AVL tree or Red-Black tree Algorithm Complexity: O(log n) Obviously, they are binary balance search tree which is not suitable for database index because: High tree height: Binary trees can become tall, leading to increased search times. Frequent rebalancing: Insertions and deletions often require tree rotations to maintain balance, which can be costly in terms of performance. Poor disk I/O performance: Binary trees do not take advantage of spatial locality, leading to inefficient disk access patterns. Disk I/O cost much more than memory access. Why BTree So, to reduce the tree height and disk I/O, BTree comes out naturally. Firstly, multi-way tree is used to reduce the tree height. Secondly, each node contains entire disk page data to reduce the disk I/O. But we still have some problems: When we insert or delete data, the tree may become unbalanced. It is hard to search in order that we need to use in-order traversal, much cost, to get sorted data. So, B+Tree comes out. B+Tree Index Firstly, B+Tree reduce the tree node size by only storing keys in internal nodes, which allows more keys to fit in memory and reduces tree height. no value means more keys in one node =\u003e less tree height =\u003e less disk I/O Secondly, B+Tree stores all actual data in leaf nodes, which are linked together in a linked list, so that range queries more efficient. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:2:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"By Usage Primary Key Index Primary key index is automatically created for primary key columns. If there is no primary key, MySQL will create one. Ensures uniqueness and fast access to records, because only one action can get the entire record. Primary Key is B+Tree Composite Index We know MySQL support composite index which is index on multiple columns. Let us see the single leaf pages. The leaf pages are sorted by the first column, and then by the second column. {column A, column B} : (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2) So, let us thinking the classical question: (leftmost prefix rule) If we have a composite index on (A, B), can we use it for query where B = 1? No, because the leaf pages are sorted by A first, then B. So we can not find all B = 1 quickly. If we have a composite index on (A, B), can we use it for query where A = 1 AND B \u003e 1? Yes, because we can find A = 1 quickly, then we get (1, 1), (1, 2) which means B is in order, so we can find B \u003e 1 quickly. If we have a composite index on (A, B), can we use it for query where A \u003e 1 AND B \u003e 1? Yes, but only A \u003e 1 is used. Because we will get (2, 1), (2, 2), (3, 1), (3, 2) which means B is not in order. Secondary Index (Non-Clustered Index) Secondary index is also called non-clustered index, which is different from clustered index (primary key index). Key Difference: Secondary index leaf nodes only store index columns + primary key values, not the complete row data. Index Structure: B+Tree structure, but leaf nodes contain references to primary key instead of full row data. Covering Index vs Table Lookup Covering Index: When query fields are all included in the secondary index, no table lookup needed. -- Index: idx_AB (A, B) SELECT A, B FROM table WHERE A = 1; -- ✅ Covering index, no lookup Table Lookup: When query needs fields not in the secondary index, must lookup primary key. -- Index: idx_AB (A, B), but need field C SELECT A, B, C FROM table WHERE A = 1; -- ❌ Need table lookup for field C Table Lookup Process Search Secondary Index: Find matching records, get primary key values Lookup Primary Index: Use primary key to fetch complete row data from clustered index Return Results: Combine data from both indexes This is why SELECT * often requires table lookup, while selecting only indexed columns can avoid it. Full-Text Index Full-Text index is designed to solve finding words in large text problems using inverted index structure. Key Differences from B+Tree Index Feature B+Tree Index Full-Text Index Data Structure Balanced tree Inverted index (word → documents) Storage Complete row data in leaves Word-to-document mappings Query Type Exact match, range queries Text search, relevance ranking Complexity O(log n) Depends on word frequency Inverted Index Mechanism Tokenization: Split text into individual words Word Mapping: Create word → document list mappings Search Process: Find documents containing query words Intersection: Combine results for multi-word queries Supported Data Types CHAR VARCHAR TEXT Create Table Grammar -- Single column full-text index CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), content TEXT, FULLTEXT KEY ft_content (content) ) ENGINE=InnoDB; -- Multi-column full-text index CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), content TEXT, FULLTEXT KEY ft_title_content (title, content) ) ENGINE=InnoDB; Search Modes Natural Language Mode (Default) SELECT * FROM articles WHERE MATCH(title, content) AGAINST('MySQL optimization'); -- With relevance score SELECT *, MATCH(title, content) AGAINST('MySQL optimization') as score FROM articles WHERE MATCH(title, content) AGAINST('MySQL optimization') ORDER BY score DESC; Boolean Mode -- Must contain \"MySQL\", must not contain \"Oracle\" SELECT * FROM articles WHERE MATCH(title, content) AGAINST('+MySQL -Oracle' IN BOOLEAN MODE); -- Exact phrase search SELECT * FROM articles WHERE MATCH(title, content) AGAINST('\"index optimization\"' IN BOOLEAN MODE","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:2:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Advanced Index Optimization Concepts Let’s analyze various SQL queries to understand Covering Index, Index Condition Pushdown (ICP), and Key Lookup (回表) behaviors. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:3:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Table Structure and Index CREATE TABLE articles ( id INT PRIMARY KEY AUTO_INCREMENT, A BIGINT NOT NULL, B BIGINT NOT NULL, C BIGINT NOT NULL, D BIGINT NOT NULL ) ENGINE=InnoDB; -- Composite index on columns A, B, C CREATE INDEX idx_articles_query ON articles (A, B, C); Index Structure: idx_articles_query (A, B, C)(secondary index) + Primary Key id (automatically included in secondary indexes) ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:3:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Covering Index Covering Index: When all query columns are included in the index, eliminating the need for table lookup. So, if the query need a table lookup, it must not be a Covering Index Not Covering Index: table lookup for other columns return SELECT * FROM articles WHERE A = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; Not Covering Index: table lookup for other columns as conditions SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; Covering Index success SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:3:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Index Condition Pushdown ICP: Pushes WHERE conditions that can use the index down to the storage engine level, reducing table lookups. Not ICP : Just one condition SELECT * FROM articles WHERE A = 1; ICP Success even skip one index column SELECT * FROM articles WHERE A = 1 AND C \u003e 1; even has other index column SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; normal SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:3:3","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Key Lookup Secondary index only save id that key lookup will occur if there has other column. Key Lookup is the action we need to avoid. Because it cost more. Key Lookup : return more columns SELECT * FROM articles WHERE A = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; Key Lookup : conditions contain more column SELECT * FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; SELECT A, B, C, D FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1 AND D = 1; Not Key Lookup(Good) SELECT A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; id contains in secondary index leaf SELECT id, A, B, C FROM articles WHERE A = 1 AND B \u003e 1 AND C = 1; ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:3:4","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Index Usage Experience ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:4:0","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"When to Use Hash Index Just search one picture. The column content nearly random or unrelated, such as UUID. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:4:1","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Filter rate is the point for index usage If reusable, index should be less. The index design is point to more filter rate. Classic Question: (id, begin, end) vs (id, end, begin) coupons table user_id begin end ~ ~ ~ 7 1234567890000 1234567890100 7 1234567891000 1234567891100 7 1234567892000 1234567892100 7 1234567893000 1234567893100 7 1234567894000 1234567894100 7 1234567895000 1234567895100 7 1234567896000 1234567896100 7 1234567897000 1234567897100 7 1234567898000 1234567898100 ~ ~ ~ find the actives coupons. Let us analysis if we search the below SQL. SELECT * FROM table WHERE id = 7 AND begin \u003c 1234567894000 AND end \u003e 1234567894000 Index Condition Pushdown can be used for this query. Only first two column index can be used which is (id, begin) in (id, begin, end) or (id, end) in (id, end, begin). But, as to (id, begin) part, begin \u003e 1234567894000 always filter less data, which the time always be now. Because there are always data begin less than now. As to (id, end) part, end \u003e 1234567894000 always filter more data, which time always be now. So the better one is (id, end, begin) Classic Question: types index Assume there are 5 types nearly average like computer types INDEX idx_computer_type(type) In this situation, when we use type = 1, just nearly 20% filter rate. not average like payment status INDEX idx_payment_status(status) In this situation, almost payment will be status = final. But we always query the status != final which just occupy a little data like less than 1% (nearly more than 99% filter rate). ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:4:2","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Data Size VS Index Size People always think that if data is just a little, there is no need to create index. When data is not large, it does not matter if we have many indexes. Because indexes will not use much storage. When data is large, it does not matter if we have many indexes. Because indexes will be important for us to query quickly. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:4:3","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Do not modify index column frequently We know the index is balanced and in order. If we modify the index column value, the B+Tree need to rebalance itself which cost much. ","date":"2025-09-28","objectID":"/en/2025/09/mysql-3.-mysql-index/:4:4","tags":["MySQL","Index"],"title":"[MySQL] 3. MySQL Index","uri":"/en/2025/09/mysql-3.-mysql-index/"},{"categories":["MySQL"],"content":"Introduction In high-concurrency environments, database locking mechanisms are crucial for ensuring data consistency and integrity. MySQL, as a widely-used relational database, provides various lock types and mechanisms to manage concurrent access. However, improper use of locks may lead to performance bottlenecks, deadlocks, and other issues that affect system stability and response time. ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:1:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Basic Lock-Related Concepts Lock Definition: A lock is a mechanism used to control access to shared resources, preventing multiple transactions from modifying the same data simultaneously, thereby ensuring data consistency and integrity. Lock Types: Table-level Lock: Locks the entire table. Shared Lock (S Lock): Allows multiple transactions to read data simultaneously but prevents modification. Exclusive Lock (X Lock): Allows one transaction to modify data while preventing other transactions from reading or modifying. Intention Locks (IS and IX Locks): Used at the table level to indicate that a transaction intends to acquire locks at the row level. Auto-increment Lock (AUTO-INC Lock): Used to handle concurrent inserts on auto-increment columns, preventing conflicts. Gap Lock: Locks the gaps between index records to prevent phantom reads. Next-Key Lock: Combines record locks and gap locks, locking index records and the gaps before them. Record Lock: Locks specific index records. Row-level Lock: Locks specific rows. Optimistic Lock: Implemented through version numbers or timestamps, suitable for read-heavy scenarios. Pessimistic Lock: Implemented through explicit locking, suitable for write-heavy scenarios. Deadlock: Multiple transactions wait for each other to release locks, resulting in inability to continue execution. Lock Compatibility: Compatibility rules exist between different types of locks, determining which locks can coexist. Lock Granularity: The scope of locked resources; finer granularity provides higher system concurrency but increases management overhead. ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:2:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"MySQL Lock Introduction ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:0","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Basic Commands Create test table -- auto-generated definition create table example_single_pk ( id bigint not null comment 'id' primary key, created timestamp default CURRENT_TIMESTAMP not null comment 'create time', updated timestamp default CURRENT_TIMESTAMP not null on update CURRENT_TIMESTAMP comment 'update time' ) comment 'example_single_pk' charset = utf8mb4; Execute commands SELECT id, created, updated FROM example_single_pk; INSERT INTO example_single_pk (id) VALUES (1); SELECT id, created, updated FROM example_single_pk; UPDATE example_single_pk SET id = 6 WHERE id = 1; SELECT id, created, updated FROM example_single_pk; DELETE FROM example_single_pk WHERE id = 1 or id = 6; SELECT id, created, updated FROM example_single_pk; Results mysql\u003e SELECT id, created, updated FROM example_single_pk; Empty set (0.00 sec) mysql\u003e mysql\u003e INSERT INTO example_single_pk (id) VALUES (1); Query OK, 1 row affected (0.00 sec) mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; +----+---------------------+---------------------+ | id | created | updated | +----+---------------------+---------------------+ | 1 | 2025-09-27 11:14:40 | 2025-09-27 11:14:40 | +----+---------------------+---------------------+ 1 row in set (0.00 sec) mysql\u003e mysql\u003e UPDATE example_single_pk SET id = 6 WHERE id = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; +----+---------------------+---------------------+ | id | created | updated | +----+---------------------+---------------------+ | 6 | 2025-09-27 11:14:40 | 2025-09-27 11:14:40 | +----+---------------------+---------------------+ 1 row in set (0.00 sec) mysql\u003e mysql\u003e DELETE FROM example_single_pk WHERE id = 1 or id = 6; Query OK, 1 row affected (0.00 sec) mysql\u003e mysql\u003e SELECT id, created, updated FROM example_single_pk; Empty set (0.00 sec) ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:1","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Classification by Granularity Table-level Lock - READ Locking mysql\u003e LOCK TABLES example_single_pk READ; Query OK, 0 rows affected (0.00 sec) At this point, other sessions can read data but cannot perform write operations. Unlocking mysql\u003e UNLOCK tables; Query OK, 0 rows affected (0.00 sec) As can be seen, other sessions can read normally but cannot write data. Table-level Lock - WRITE Locking mysql\u003e LOCK TABLES example_single_pk WRITE; Query OK, 0 rows affected (0.00 sec) Unlocking mysql\u003e UNLOCK tables; Query OK, 0 rows affected (0.00 sec) As can be seen, other sessions can neither read nor write data. Overall, table-level locks have larger granularity, suitable for scenarios involving bulk operations on entire tables, but they affect concurrency performance. Not recommended for use. Row-level Lock - SELECT … FOR SHARE Locking mysql\u003e start transaction; Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM example_single_pk WHERE id = 1 FOR SHARE; Empty set (0.00 sec) At this point, it affects locks within other sessions. Other sessions can read data but cannot perform write operations. We won’t elaborate further or take screenshots. Unlocking mysql\u003e COMMIT; Query OK, 0 rows affected (0.00 sec) After unlocking, read locks and write locks can be acquired normally. Row-level Lock - SELECT … FOR UPDATE Locking mysql\u003e start transaction; Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM example_single_pk WHERE id = 1 FOR UPDATE; Empty set (0.00 sec) From the data query perspective, there’s no difference from FOR SHARE, as it affects locks within other sessions. FOR UPDATE locks the selected rows, preventing other transactions from reading or modifying these rows. Unlocking mysql\u003e COMMIT; Query OK, 0 rows affected (0.00 sec) After unlocking, read locks and write locks can be acquired normally. ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:2","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Classification by Attributes Shared Locks \u0026 Exclusive Locks No. Lock Name Trigger Method Lock Type Scope Key Features 1 Table-level Shared Lock (S Lock) LOCK TABLES tbl_name READ Server layer lock Entire table Allows other sessions to read, blocks writes; requires manual UNLOCK TABLES. 2 Table-level Exclusive Lock (X Lock) LOCK TABLES tbl_name WRITE Server layer lock Entire table Blocks other sessions from reading/writing; requires manual UNLOCK TABLES. 3 Row-level Shared Lock (S Lock) SELECT … FOR SHARE (within transaction) InnoDB row lock Single row Allows other sessions to read the row, blocks writes; auto-unlocked after transaction commit/rollback. 4 Row-level Exclusive Lock (X Lock) SELECT … FOR UPDATE (within transaction) InnoDB row lock Single row Blocks other sessions from reading/writing the row; auto-unlocked after transaction commit/rollback. Auto-increment Lock (AUTO-INC Lock) Trigger Method: Automatically triggered when INSERT statements operate on AUTO_INCREMENT columns Features: AUTO-INC lock ensures continuity and uniqueness of auto-increment values. Different from row locks, but behavior results are similar. As can be seen, there’s no conflict between auto-increment locks, and both sessions successfully insert data. Obviously, when the ID is determined, the behavior becomes similar to row locks, and the second session fails to insert. After commit, we can see that data insertion failed due to ID conflict. Intention Lock Intention locks are server-level table locks designed by the InnoDB storage engine to coordinate conflicts between “server layer (MySQL main process) table locks” and “storage engine layer row locks”. Their core purpose is to transmit the signal “row locks exist in the storage engine”, allowing the server layer to quickly determine compatibility between table locks and row locks, avoiding high-cost conflict detection. The following explains from the perspectives of hierarchical positioning, design goals, linkage mechanisms, and lightweight reasons: Intention locks serve as “row lock signal lights” at the server layer—using minimal overhead of table-level locks to transmit row lock status from the storage engine, enabling efficient coordination between two-layer lock mechanisms, both correctly and performantly. Hierarchical Positioning: Server Layer’s “Row Lock Signal Officer” • Server Layer: MySQL main process manages server-level locks (such as LOCK TABLES), which is a native lock mechanism independent of storage engines. • Storage Engine Layer: InnoDB manages row-level locks (such as X locks from SELECT … FOR UPDATE), controlling access to specific data rows. • Role of Intention Locks: Intention locks belong to server layer table locks, but they don’t directly control data rows—instead, they act as “translators”, converting row lock status in the storage engine (“transactions are operating on certain rows”) into “signals” understandable by the server layer (table-level IS/IX locks). Core Design Goal: Solving “Information Gap” in Layered Architecture MySQL’s layered architecture (server layer vs storage engine layer) naturally isolates lock status: • When the server layer wants to add table locks, it cannot directly perceive whether there are row locks in the storage engine (e.g., transaction A has locked certain rows); • Row locks in the storage engine also don’t care whether there are table locks at the server layer. Without intention locks, when the server layer adds table locks, it must traverse all rows in the storage engine (O(n) time complexity) to check for row lock conflicts—this would be catastrophic performance loss for large tables. The emergence of intention locks optimizes this “traversal check” to O(1) signal judgment: • When the storage engine adds row locks, it automatically registers corresponding intention locks with the server layer (IS=row read intention, IX=row write intention); • When the server layer adds table locks, it only needs to check table-level intention lock st","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:3","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Classification by Algorithm (InnoDB Engine) 1. Record Lock 2. Gap Lock 3. Next-key Lock Practical Operation Instructions Operation Type Common Scenario Lock Type Lock Range Isolation Level Dependency Main Conflict Objects Notes SELECT Normal query (without FOR UPDATE/SHARE) No lock None None None Read without lock (snapshot read) SELECT FOR UPDATE Equality query (record exists) Record lock (LOCK_REC_NOT_GAP) Specific record (e.g., id=3) RR/RC Record locks and next-key locks on same record Locks target row, blocks modification/deletion SELECT FOR UPDATE Equality query (record doesn’t exist, RR) Gap lock (LOCK_GAP) Gap between adjacent records (e.g., (1,5)) RR Gap locks and insert intention locks in same gap Prevents other transactions from inserting missing records (phantom read) SELECT FOR UPDATE Equality query (record doesn’t exist, RC) No lock None RC None RC has no gap locks, only read without lock SELECT FOR UPDATE Range query (e.g., id\u003e2, RR) Next-key lock (LOCK_NEXT_KEY) Record+predecessor gap (e.g., (3,3], (3,5]) RR Record locks and gap locks on same record Locks all records and gaps in range (prevents phantom read) SELECT FOR UPDATE Range query (e.g., id\u003e2, RC) Record lock Records meeting conditions (e.g., id=3,5) RC Record locks on same record RC has no gap locks, only locks existing records INSERT Insert new record (any scenario) Insert intention lock (LOCK_INSERT_INTENTION) Gap between adjacent records (e.g., (1,5)) None (always added) Normal gap locks and record locks in same gap Coordinates insert exclusion, conflicts with normal locks DELETE Equality deletion (record exists) Record lock (LOCK_REC_NOT_GAP) Specific record (e.g., id=3) RR/RC Record locks and next-key locks on same record Locks target row, blocks modification/insertion DELETE Range deletion (e.g., id\u003e2, RR) Next-key lock (LOCK_NEXT_KEY) Record+predecessor gap (e.g., (3,3], (3,5]) RR Record locks and gap locks on same record Locks all records and gaps in range (prevents phantom read) DELETE Range deletion (e.g., id\u003e2, RC) Record lock Records meeting conditions (e.g., id=3,5) RC Record locks on same record RC has no gap locks, only locks existing records UPDATE Equality update (record exists) Record lock (LOCK_REC_NOT_GAP) Specific record (e.g., id=3) RR/RC Record locks and next-key locks on same record Locks target row, blocks modification/insertion UPDATE Range update (e.g., id\u003e2, RR) Next-key lock (LOCK_NEXT_KEY) Record+predecessor gap (e.g., (3,3], (3,5]) RR Record locks and gap locks on same record Locks all records and gaps in range (prevents phantom read) UPDATE Range update (e.g., id\u003e2, RC) Record lock Records meeting conditions (e.g., id=3,5) RC Record locks on same record RC has no gap locks, only locks existing records INSERT Insert operations use insert intention locks insert into -1 Time SESSION 1 (LEFT) SESSION 2 (RIGHT) 0.0 SESSION 2 START - Waiting for Session1… 0.3 SESSION 1 START - INSERT === Round 0: Testing SELECT \u0026 INSERT === ~ mysql\u003e SELECT * FROM example_single_pk ~ (1, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ (4, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ (5, ‘2025-09-27 11:22:48’, ‘2025-09-27 11:22:48’) ~ 3 rows (0.03s) ~ === Round -1: Testing INSERT ID=-1 === ~ mysql\u003e start transaction ~ Query OK, 0 rows affected ~ mysql\u003e INSERT INTO example_single_pk (id) VALUES (-1) ~ Query OK, 1 row affected (0.03s) Session 1 inserts a record with ID=-1 and keeps the transaction uncommitted, holding an insert intention lock. Time SESSION 1 (LEFT) SESSION 2 (RIGHT) 0.7 mysql\u003e SELECT * FROM example_single_pk WHERE id = -1 FOR UPDATE 3.7 ERROR: SELECT timeout (3.04s) (3.04s) 4.1 mysql\u003e INSERT INTO example_single_pk (id) VALUES (-1) 7.1 ERROR: INSERT timeout (3.03s) (3.03s) Session 2 queries the record with ID=-1, finds that the record exists but is held by Session 1’s insert intention lock, causing the query to block waiting for lock release, ultimately timing out and failing. Session 2 attempts to insert a record with ID=-1, also","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:4","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["MySQL"],"content":"Appendix Python script for data collection #!/usr/bin/env python3 \"\"\" MySQL Lock Testing - Timeline Analysis Version \"\"\" import pymysql import threading import time from datetime import datetime from collections import defaultdict DB_CONFIG = { 'host': 'localhost', 'user': 'haotian', 'password': 'qwe123qwe123', 'database': 'toy', 'autocommit': False } # Test Configuration - Modify as needed TEST_CONFIG = { # Session1 Configuration 'session1_operation': 'select_for_update_range', # 'insert', 'select_for_update', 'select_for_update_range' 'insert_start': -1, # Operation start ID 'insert_end': 7, # Operation end ID (exclusive) 'range_size': 2, # Range size (for select_for_update_range only) # Session2 Test Configuration 'test_ids': [-1, 0, 1, 2, 3, 4, 5, 6, 7], # ID list to test 'lock_timeout': 3 # Lock wait timeout in seconds } # Global log collector timeline_log = [] log_lock = threading.Lock() def log_event(session, event_type, sql, result=None, error=None, duration=None): \"\"\"Record event to timeline\"\"\" with log_lock: timestamp = datetime.now() timeline_log.append({ 'timestamp': timestamp, 'session': session, 'type': event_type, # 'sql', 'info', 'error' 'sql': sql, 'result': result, 'error': error, 'duration': duration }) # Real-time progress display time_str = timestamp.strftime(\"%H:%M:%S.%f\")[:-3] if event_type == 'sql': progress_text = f\"mysql\u003e {sql}\" if duration: progress_text += f\" ({duration:.2f}s)\" elif event_type == 'error': progress_text = f\"ERROR: {error}\" if duration: progress_text += f\" ({duration:.2f}s)\" else: progress_text = result or sql print(f\"[{time_str}] [{session}] {progress_text}\") def session1(): \"\"\"Session1: Loop operations (INSERT or SELECT FOR UPDATE)\"\"\" conn = pymysql.connect(**DB_CONFIG) cursor = conn.cursor() operation_type = TEST_CONFIG['session1_operation'] log_event('S1', 'info', '', f'SESSION 1 START - {operation_type.upper()}') # Check table status start = time.time() cursor.execute(\"SELECT * FROM example_single_pk\") results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', 'SELECT * FROM example_single_pk', results, duration=duration) # Loop operations try: for target_id in range(TEST_CONFIG['insert_start'], TEST_CONFIG['insert_end']): log_event('S1', 'info', '', f'=== Round {target_id}: Testing {operation_type.upper()} ID={target_id} ===') try: # Start transaction log_event('S1', 'sql', 'start transaction', 'Query OK, 0 rows affected') cursor.execute(\"START TRANSACTION\") if operation_type == 'insert': # INSERT operation sql = f\"INSERT INTO example_single_pk (id) VALUES ({target_id})\" start = time.time() try: cursor.execute(sql) duration = time.time() - start log_event('S1', 'sql', sql, 'Query OK, 1 row affected', duration=duration) except pymysql.err.IntegrityError as e: duration = time.time() - start log_event('S1', 'error', sql, None, str(e), duration) elif operation_type == 'select_for_update': # SELECT FOR UPDATE operation sql = f\"SELECT * FROM example_single_pk WHERE id = {target_id} FOR UPDATE\" start = time.time() try: cursor.execute(sql) results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', sql, None) log_event('S1', 'info', '', f'Query returned {len(results)} rows ({duration:.2f}s)') if results: for row in results: formatted_row = [] for item in row: if hasattr(item, 'strftime'): formatted_row.append(item.strftime('%Y-%m-%d %H:%M:%S')) else: formatted_row.append(item) log_event('S1', 'info', '', str(tuple(formatted_row))) except Exception as e: duration = time.time() - start log_event('S1', 'error', sql, None, str(e), duration) elif operation_type == 'select_for_update_range': # SELECT FOR UPDATE range operation range_size = TEST_CONFIG['range_size'] end_id = target_id + range_size sql = f\"SELECT * FROM example_single_pk WHERE id \u003e= {target_id} AND id \u003c {end_id} FOR UPDATE\" start = time.time() try: cursor.execute(sql) results = cursor.fetchall() duration = time.time() - start log_event('S1', 'sql', sql, None) log_event('S1', 'info', ","date":"2025-09-27","objectID":"/en/2025/09/mysql-2.-mysql-lock/:3:5","tags":["MySQL","Lock"],"title":"[MySQL] 2. Lock Mechanism Execution Analysis","uri":"/en/2025/09/mysql-2.-mysql-lock/"},{"categories":["Github","Efficiency"],"content":"Introduction Value-Checker-Java is essentially a customizable AOP pointcut framework. It allows developers to insert custom validation logic before method execution, and this validation logic can be arbitrarily complex business rules. However, if it merely provides an AOP pointcut, that wouldn’t be very meaningful. The core value of Value-Checker-Java lies in its thread-safe context management mechanism. Without this context management, data queried in the first validator cannot be used in subsequent validators, forcing each validator to re-query data, which defeats the purpose of validation chains. It is precisely because of ValueCheckerReentrantThreadLocal, this thread-safe context manager, that multiple validators can share data and form truly meaningful validation chains. ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:1:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Basic Usage ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:2:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Validator Configuration // From TargetService.java @ValueCheckers(checkers = { @ValueCheckers.ValueChecker(method = \"verify\", keys = {\"#id\", \"#name\"}, handler = SampleCheckerHandlerImpl.class), @ValueCheckers.ValueChecker(method = \"verify\", keys = \"#id\", handler = SampleCheckerHandlerImpl.class), @ValueCheckers.ValueChecker(method = \"verify\", keys = \"#name\", handler = SampleCheckerHandlerImpl.class) }) public void checker(Long id, String name) { // Will execute 3 validators in sequence } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:2:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Validator Implementation // From SampleCheckerHandlerImpl.java @Service public class SampleCheckerHandlerImpl implements IValueCheckerHandler { public static final Long CORRECT_ID = 2L; public static final String CORRECT_NAME = \"correctName\"; public void verify(Long id, String name) { if (!CORRECT_ID.equals(id) || !CORRECT_NAME.equals(name)) { throw new ValueIllegalException(\"error\"); } } public void verify(Long id) { if (!CORRECT_ID.equals(id)) { throw new ValueIllegalException(\"error\"); } } public void verify(String name) { if (!CORRECT_NAME.equals(name)) { throw new ValueIllegalException(\"error\"); } } } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:2:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Key Technical Implementation Annotation Design @ValueCheckers adopts a nested annotation design pattern: @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface ValueCheckers { ValueChecker[] checkers(); @interface ValueChecker { Class\u003c? extends IValueCheckerHandler\u003e handler(); String method() default \"verify\"; String[] keys() default \"\"; } } Design highlights: Array configuration: Supports multiple validator combinations Type safety: Handler must implement IValueCheckerHandler interface Flexible method mapping: Can specify any validation method name Parameterized configuration: Pass required parameters through keys array SpEL Expression Engine SeplUtil class provides powerful parameter extraction capabilities: public static Object[] getValue(ProceedingJoinPoint point, String[] keys) { MethodSignature methodSignature = (MethodSignature) point.getSignature(); String[] params = methodSignature.getParameterNames(); Object[] args = point.getArgs(); EvaluationContext context = new StandardEvaluationContext(); for (int len = 0; len \u003c params.length; len++) { context.setVariable(params[len], args[len]); } Object[] values = new Object[keys.length]; for (int i = 0; i \u003c keys.length; i++) { Expression expression = SPEL_PARSER.parseExpression(keys[i]); values[i] = expression.getValue(context, Object.class); } return values; } Technical features: Dynamic parameter mapping: Runtime acquisition of method parameter names Expression parsing: Supports complex SpEL expressions Type safety: Automatic type conversion handling Performance optimization: Reuses SpEL parser instances Intelligent Method Invocation Mechanism The method invocation mechanism in ValueCheckerAspect has the following characteristics: private void methodInvoke(Object instance, String method, Object[] paras) { // Generate method signature cache key final String parasName = objectTypeName(paras); final String objectMethodName = String.format(OBJECT_METHOD_FORMAT, instanceClass.getSimpleName(), method, parasName); // Prioritize cached methods if (OBJECT_METHOD_MAP.containsKey(objectMethodName)) { final Method pointMethod = OBJECT_METHOD_MAP.get(objectMethodName); pointMethod.invoke(instance, paras); return; } // First call: perform method matching and caching for (Method subMethod : instanceClass.getMethods()) { // Method name + parameter length + parameter type matching if (subMethod.getName().equals(method) \u0026\u0026 subMethod.getParameterTypes().length == paras.length \u0026\u0026 parasName.equals(methodTypeName(subMethod.getParameterTypes()))) { OBJECT_METHOD_MAP.put(objectMethodName, subMethod); subMethod.invoke(instance, paras); return; } } } Core advantages: Performance optimization: Method reflection result caching, avoiding repeated lookups Precise matching: Supports accurate identification of method overloading Type safety: Strict parameter type matching Reentrant ThreadLocal Design ValueCheckerReentrantThreadLocal is an innovative design of the framework: public static void init() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter) { VALUE_CHECKER_THREAD_LOCAL.set(new ConcurrentHashMap\u003c\u003e()); VALUE_CHECKER_THREAD_LOCAL_COUNTER.set(new AtomicInteger()); return; } counter.addAndGet(1); } public static void clear() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter || counter.get() \u003c= 0) { VALUE_CHECKER_THREAD_LOCAL.remove(); VALUE_CHECKER_THREAD_LOCAL_COUNTER.remove(); return; } counter.addAndGet(-1); } Design essence: Reference counting: Uses AtomicInteger to implement reentrant counting Thread safety: ConcurrentHashMap ensures concurrent safety Automatic cleanup: Automatically cleans up resources when outermost call ends Nested support: Perfect support for nested validator calls ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:2:3","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"ThreadLocal Context Management This is one of the core capabilities of the framework. ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:3:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Data Storage and Retrieval // Store data to ThreadLocal public void verifyPutThreadValue(String name) { ValueCheckerReentrantThreadLocal.getOrDefault(String.class, name); if (!ValueCheckerReentrantThreadLocal.getOrDefault(String.class, \"\").equals(name)) { throw new ValueIllegalException(\"error\"); } } // Retrieve data from ThreadLocal public void verifyGetRightThreadValue(String name) { if (!ValueCheckerReentrantThreadLocal.getOrDefault(String.class, name).equals(name)) { throw new ValueIllegalException(\"error\"); } } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:3:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Why It’s Important Without ThreadLocal: public void validateUser(Long userId) { User user = userRepository.findById(userId); // 1st query } public void validateUserPermission(Long userId) { User user = userRepository.findById(userId); // 2nd query, duplicate! } With ThreadLocal: public void validateUser(Long userId) { User user = userRepository.findById(userId); // Only query once ValueCheckerReentrantThreadLocal.put(user); } public void validateUserPermission(Long userId) { User user = ValueCheckerReentrantThreadLocal.get(User.class, () -\u003e null); // Direct retrieval } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:3:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Reentrant Support ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:4:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Test Scenario // From TargetService.java @ValueCheckers(checkers = { @ValueCheckers.ValueChecker(method = \"verifyPutThreadValue\", keys = \"#name\", handler = SampleCheckerHandlerImpl.class) }) public void checkerReentrant(String name) { // 1st layer AOP: store data to ThreadLocal this.targetService.checkerGetThreadValue(name); // 2nd layer AOP this.targetService.checkerGetWrongThreadValue(\"\"); // 3rd layer AOP } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:4:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Reentrant Counter // ValueCheckerReentrantThreadLocal.java public static void init() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter) { // First call: initialize ThreadLocal VALUE_CHECKER_THREAD_LOCAL.set(new ConcurrentHashMap\u003c\u003e()); VALUE_CHECKER_THREAD_LOCAL_COUNTER.set(new AtomicInteger()); } else { // Nested call: counter+1 counter.addAndGet(1); } } public static void clear() { final AtomicInteger counter = VALUE_CHECKER_THREAD_LOCAL_COUNTER.get(); if (null == counter || counter.get() \u003c= 0) { // Outermost call: actually clean ThreadLocal VALUE_CHECKER_THREAD_LOCAL.remove(); VALUE_CHECKER_THREAD_LOCAL_COUNTER.remove(); } else { // Inner call: counter-1 counter.addAndGet(-1); } } Example: // ValueCheckerAspectTest.java - Test comments explain the entire flow // aop1 - 1.1 counter = 0 init ThreadLocal // aop1 - 1.2 counter = 0 set RIGHT_VALUE to ThreadLocal // aop2 - 2.1 counter = 1 init ThreadLocal // aop2 - 2.2 counter = 1 try to set RIGHT_VALUE to ThreadLocal (success) // aop2 - 2.3 counter = 0 clear ThreadLocal (if not reentrant, RIGHT_VALUE will be clear) // aop3 - 3.1 counter = 1 init ThreadLocal // aop3 - 3.2 counter = 1 try to set WRONG_VALUE to ThreadLocal (fail) // aop3 - 3.3 counter = 0 clear ThreadLocal // aop1 - 1.3 counter = null clear ThreadLocal Without reentrant support, the second and third layer AOP calls would clear ThreadLocal data stored by the first layer, causing validation failure. ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:4:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Core Architecture ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:5:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"AOP Aspect try-finally Structure // ValueCheckerAspect.java - Key try-finally implementation @Around(\"handleValueCheckerPoint() \u0026\u0026 @annotation(valueCheckers)\") public Object around(ProceedingJoinPoint point, ValueCheckers valueCheckers) throws Throwable { try { // init ThreadLocal, if init in sub ValueChecker, ThreadLocal will counter++ ValueCheckerReentrantThreadLocal.init(); for (ValueCheckers.ValueChecker checker : valueCheckers.checkers()) { valueCheck(checker, point); } return point.proceed(); } finally { // clear ThreadLocal, if clear in sub ValueChecker, ThreadLocal will counter-- ValueCheckerReentrantThreadLocal.clear(); } } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:5:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Execution Flow @ValueCheckers annotated method ↓ ValueCheckerAspect intercepts ↓ try { init ThreadLocal (reentrant counting) ↓ iterate checkers array ↓ SpEL parameter extraction → reflection call Handler ↓ validation fails throw exception / validation succeeds continue ↓ all validations pass execute original method } finally { clear ThreadLocal (reentrant counting) } Key roles of try-finally: Guaranteed resource cleanup: ThreadLocal is cleaned regardless of validation success or failure Reentrant count management: Ensures proper ThreadLocal management in nested calls through counter Memory leak prevention: Ensures ThreadLocal is properly cleaned when method ends ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:5:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Performance Optimization Design ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:6:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Method Caching Mechanism // ValueCheckerAspect.java - Method caching based on performance considerations private static final ConcurrentHashMap\u003cString, Method\u003e OBJECT_METHOD_MAP = new ConcurrentHashMap\u003c\u003e(); private void methodInvoke(Object instance, String method, Object[] paras) { // Generate cache key final String objectMethodName = String.format(OBJECT_METHOD_FORMAT, instanceClass.getSimpleName(), method, objectTypeName(paras)); // Prioritize cache usage if (OBJECT_METHOD_MAP.containsKey(objectMethodName)) { final Method pointMethod = OBJECT_METHOD_MAP.get(objectMethodName); pointMethod.invoke(instance, paras); return; } // First call: traverse methods and cache for (Method subMethod : instanceClass.getMethods()) { if (subMethod.getName().equals(method) \u0026\u0026 subMethod.getParameterTypes().length == paras.length \u0026\u0026 objectTypeName(paras).equals(methodTypeName(subMethod.getParameterTypes()))) { OBJECT_METHOD_MAP.put(objectMethodName, subMethod); // Cache result subMethod.invoke(instance, paras); return; } } } ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:6:1","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Performance Considerations Reflection overhead optimization: First call traverses all methods for matching Subsequent calls directly get Method object from ConcurrentHashMap Avoids repeated reflection lookup operations SpEL expression performance: Reuses SpEL parser instance: private static final ExpressionParser SPEL_PARSER Runtime parameter mapping, supports complex expressions but has performance cost ThreadLocal overhead: ThreadLocal operations themselves are lightweight Reentrant counter uses AtomicInteger, thread-safe and efficient Validation chain execution: Multiple validators execute serially Total time = sum of individual validator times Reduces duplicate queries through data sharing ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:6:2","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Java 8 vs Java 17 Core difference: SpEL parameter name acquisition Java 17 requires additional configuration: \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cparameters\u003etrue\u003c/parameters\u003e \u003c!-- Preserve parameter names --\u003e \u003c/configuration\u003e \u003c/plugin\u003e Dependency versions: Java 8: Spring Boot 2.5.13 Java 17: Spring Boot 3.5.5 ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:7:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Summary Value-Checker-Java solves the core problem: data sharing between validators. Essence: Customizable AOP + ThreadLocal context Value: Avoid duplicate queries, make validation chains meaningful Key: Reentrant ThreadLocal supports nested calls Scenario: Multi-step validation that needs to share query results ","date":"2025-09-26","objectID":"/en/2025/09/github-4.-value-checker/:8:0","tags":["Github","Efficiency"],"title":"[Github] 4. Value-Checker-Java: Customizable AOP Validation Framework","uri":"/en/2025/09/github-4.-value-checker/"},{"categories":["Github","Efficiency"],"content":"Introduction Parameter validation is a common and crucial requirement in daily Java development. Traditional parameter validation typically requires writing extensive if-else conditional code in each method, which is not only redundant and tedious but also prone to omissions. Basic-Check-Java was born to solve this pain point as a lightweight parameter validation framework. This article will provide an in-depth introduction to Basic-Check-Java’s design philosophy, core features, and practical applications, helping developers quickly master this practical tool. ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:1:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Github basic-check-java-8 basic-check-java-17 ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:2:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Design Philosophy and Core Features ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:3:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Design Philosophy Basic-Check-Java is designed based on the following core principles: Simplicity: Reduce boilerplate code through annotation-driven declarative programming Flexibility: Support multiple return strategies to adapt to different business scenarios Non-intrusive: Based on AOP implementation with zero intrusion on business code Extensibility: Support custom validation rules and processing logic ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:3:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Core Features Rich Parameter Validation Annotations Basic-Check-Java provides six commonly used parameter validation annotations: @CheckNull: Validates that parameter is not null @CheckString: Validates that string parameter is not blank @CheckLong: Validates that Long parameter is greater than -1 @CheckCollection: Validates that collection parameter is not empty @CheckMap: Validates that Map parameter is not empty @CheckObject: Validates object parameters using Bean Validation Flexible Return Strategies Through the returnType attribute of the @BasicCheck annotation, three handling strategies are supported when validation fails: EXCEPTION (default): Throws IllegalArgumentException EMPTY: Automatically returns empty values based on method return type (empty collections, empty Map, Optional.empty(), etc.) NULL: Returns null directly Non-intrusive Implementation Based on Spring AOP The framework uses AspectJ annotations and Spring AOP technology to perform parameter validation before method execution through aspect-oriented programming, with complete non-intrusion on business code. ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:3:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"In-depth Technical Architecture Analysis ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:4:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Core Architecture Diagram @BasicCheck annotated method ↓ NotNullAndPositiveAspect intercepts ↓ Iterate through method parameters and their annotations ↓ Execute corresponding validation logic based on annotation type ↓ Validation fails → Return corresponding result based on returnType Validation succeeds → Continue executing original method ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:4:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Key Technical Implementation Annotation Design Using @BasicCheck as an example, demonstrating elegant annotation design: @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface BasicCheck { ReturnType returnType() default ReturnType.EXCEPTION; enum ReturnType { EMPTY, // Return empty collection/map/optional NULL, // Return null EXCEPTION // Throw exception } } Key design points: @Target(ElementType.METHOD): Can only be applied at method level @Retention(RetentionPolicy.RUNTIME): Retain annotation information at runtime Provides enum-type configuration options ensuring type safety AOP Aspect Implementation NotNullAndPositiveAspect is the core component of the framework, handling the main parameter validation logic: @Around(\"handleBasicCheckPoint() \u0026\u0026 @annotation(basicCheck)\") public Object around(ProceedingJoinPoint point, BasicCheck basicCheck) throws Throwable { final Object[] args = point.getArgs(); final MethodSignature signature = (MethodSignature) point.getSignature(); final Method method = signature.getMethod(); final Parameter[] parameters = method.getParameters(); // Iterate through all parameters for validation for (int i = 0; i \u003c parameters.length; i++) { // Execute corresponding validation logic based on different annotation types if (parameters[i].isAnnotationPresent(CheckNull.class) \u0026\u0026 null == args[i]) { return this.getReturnObj(basicCheck, method); } // ... other validation logic } return point.proceed(); } Implementation highlights: Parameter-Annotation Mapping: Obtain method parameters and corresponding annotation information through reflection Decoupled Validation Logic: Each validation type is handled independently, facilitating maintenance and extension Intelligent Return Handling: Intelligently generate return values based on method return type and configuration strategy Intelligent Return Value Generation A clever design of the framework is automatically generating appropriate empty values based on method return type: private Object getReturnObj(BasicCheck annotation, Method method) { if (annotation.returnType() == BasicCheck.ReturnType.EMPTY) { Class\u003c?\u003e returnType = method.getReturnType(); if (returnType == List.class) return Collections.emptyList(); if (returnType == Set.class) return Collections.emptySet(); if (returnType == Map.class) return Collections.emptyMap(); if (returnType == Optional.class) return Optional.empty(); } // ... other processing logic } This design avoids the complexity of developers manually handling different return types. ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:4:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Practical Application Examples ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:5:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Basic Usage Examples Simple Parameter Validation @Service public class UserService { @BasicCheck public void createUser(@CheckNull String username, @CheckString String email, @CheckLong Long age) { // Business logic, no need for manual parameter validation userRepository.save(new User(username, email, age)); } } Collection Parameter Validation @BasicCheck public void batchCreateUsers(@CheckCollection List\u003cUser\u003e users, @CheckMap Map\u003cString, Object\u003e config) { // Ensure both users collection and config map are not empty users.forEach(user -\u003e userRepository.save(user)); } Empty Return Strategy @BasicCheck(returnType = BasicCheck.ReturnType.EMPTY) public List\u003cUser\u003e searchUsers(@CheckString String keyword) { // If keyword is blank string, automatically return empty List return userRepository.findByKeyword(keyword); } ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:5:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Advanced Application Scenarios Complex Object Validation // Define validation DTO @Data @Builder public class UserCreateRequest { @NotNull @Min(1) private Integer id; @NotBlank private String name; @Valid @NotEmpty private List\u003c@Valid ContactInfo\u003e contacts; } // Use @CheckObject to validate complex objects @BasicCheck public void createUserWithDetails(@CheckObject UserCreateRequest request) { // Framework automatically uses Bean Validation to validate entire object tree userService.createUser(request); } Mixed Validation Strategies @BasicCheck(returnType = BasicCheck.ReturnType.NULL) public void processOrder(@CheckLong Long orderId, @CheckString String customerId, @CheckCollection List\u003cOrderItem\u003e items, @CheckObject OrderConfig config) { // Return null if any parameter validation fails, suitable for void methods orderProcessor.process(orderId, customerId, items, config); } ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:5:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Performance Considerations and Best Practices ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:6:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Performance Analysis Reflection Overhead: Framework uses reflection to obtain method parameter information, performance testing recommended for high-concurrency scenarios AOP Overhead: Spring AOP is based on proxy pattern with slight performance overhead Bean Validation: Object validation has certain performance cost but is generally acceptable ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:6:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Best Practices Reasonable Validation Strategy Selection // Recommended: Query methods use EMPTY strategy @BasicCheck(returnType = BasicCheck.ReturnType.EMPTY) public List\u003cUser\u003e findUsers(@CheckString String keyword) { return userRepository.findByKeyword(keyword); } // Recommended: Create/update methods use EXCEPTION strategy @BasicCheck public void updateUser(@CheckLong Long id, @CheckObject UserUpdateRequest request) { userRepository.update(id, request); } Combined Use of Validation Annotations // Combine multiple validation annotations @BasicCheck public void processUserData(@CheckNull @CheckString String username, @CheckNull @CheckLong Long userId, @CheckNull @CheckObject UserData data) { // Check both null and business rules } Custom Validation Object Design @Data public class ProductRequest { @NotNull(message = \"Product ID cannot be null\") @Min(value = 1, message = \"Product ID must be greater than 0\") private Long productId; @NotBlank(message = \"Product name cannot be blank\") @Size(max = 100, message = \"Product name length cannot exceed 100\") private String productName; @Valid // Enable nested validation @NotNull(message = \"Product configuration cannot be null\") private ProductConfig config; } ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:6:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Java 8 vs Java 17 Version Differences ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:7:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Core Difference: Validation API Namespace Change This is the most important difference between the two versions: Java 8 version uses traditional javax namespace: import javax.validation.Validation; import javax.validation.Validator; Java 17 version uses new jakarta namespace: import jakarta.validation.Validation; import jakarta.validation.Validator; ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:7:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Complete Dependency Comparison Java 8 Version Dependencies: Spring Boot 2.4.4 Lombok 1.18.18 javax.validation:validation-api:2.0.1.Final org.hibernate:hibernate-validator:6.0.1.Final org.glassfish:javax.el:3.0.1-b09 Java 17 Version Dependencies: Spring Boot 3.5.5 Lombok 1.18.38 org.hibernate.validator:hibernate-validator:8.0.1.Final org.glassfish:jakarta.el:4.0.2 ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:7:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Extension and Customization ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:8:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Custom Validation Annotations The framework adopts an open design, supporting addition of custom validation annotations: // 1. Define custom validation annotation @Target(ElementType.PARAMETER) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface CheckEmail { } // 2. Add validation logic in NotNullAndPositiveAspect if (parameters[i].isAnnotationPresent(CheckEmail.class) \u0026\u0026 !isValidEmail((String) args[i])) { return this.getReturnObj(basicCheck, method); } ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:8:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Custom Return Strategies ReturnType enum can be extended to add more return strategies: public enum ReturnType { EMPTY, NULL, EXCEPTION, CUSTOM_DEFAULT // Custom default value strategy } ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:8:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Conclusion Basic-Check-Java is an elegantly designed and practically useful parameter validation framework. It solves the pain points of parameter validation in Java development through the following characteristics: ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:9:0","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Core Advantages High Development Efficiency: Reduces 80% of parameter validation boilerplate code Simple to Use: Only need to add annotations to enjoy complete validation functionality Comprehensive Features: Supports multi-level validation of basic types, collections, and complex objects Flexible Configuration: Multiple return strategies adapt to different business scenarios Non-intrusive: Based on AOP implementation with zero impact on existing code ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:9:1","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Github","Efficiency"],"content":"Applicable Scenarios Web Applications: Controller layer parameter validation Service Layer: Service method parameter validation Utility Classes: Parameter checking for general utility methods API Interfaces: Parameter pre-checking before third-party interface calls Basic-Check-Java embodies excellent framework design principles: simple to use, feature-complete, reliable performance, and easy to extend. It not only significantly improves development efficiency but also helps developers write more robust and elegant code. Whether for new projects or existing projects, Basic-Check-Java can be quickly integrated and deliver value, making it an indispensable tool in the Java developer’s toolkit. ","date":"2025-09-26","objectID":"/en/2025/09/github-3.-basic-check/:9:2","tags":["Github","Efficiency"],"title":"[Github] 3. Basic-Check: Validation Framework","uri":"/en/2025/09/github-3.-basic-check/"},{"categories":["Cluster","Algorithm"],"content":"Introduction RAFT (Raft Consensus Algorithm) is a distributed consensus algorithm designed to solve the problem of achieving data state agreement among multiple nodes in distributed systems. Compared to the renowned Paxos algorithm, RAFT’s design philosophy emphasizes “understandability” through clear role separation and straightforward state transitions, making it easier for developers to comprehend and implement. This article demonstrates the complete evolution of the RAFT algorithm from single-node to multi-node clusters through 11 detailed diagrams, covering key scenarios including normal operation, failure handling, network partitions, and conflict resolution. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:1:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Core RAFT Concepts Before diving into the analysis, let’s familiarize ourselves with several core concepts of RAFT: Node States: Leader: Handles client requests and replicates log entries to other nodes Follower: Passively receives log replication requests from the Leader Candidate: Temporary state during the Leader election process Key Data Structures: Log: Ordered sequence storing operation commands Term: Monotonically increasing logical clock used to detect stale information CommitIndex: Index of the highest log entry known to be committed ApplyIndex/LastApplied: Index of the highest log entry applied to the state machine State: Actual business data state ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:2:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 1: Single Node Startup (Diagram 1) Node1 starts as Leader with initial state: - Log=[] (empty log) - Term=0 (initial term) - CommitIndex=0, ApplyIndex=0 (no committed/applied entries) - State={} (empty state machine) In a single-node cluster, the node automatically becomes the Leader as it constitutes a “majority” (1 \u003e 1/2). This illustrates a crucial property of the RAFT algorithm: at most one Leader can exist at any given moment. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:3:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 2: Handling Client Requests (Diagram 2) Client request: x=1 Processing workflow: 1. Leader appends \"x=1\" to the log (Log=[x=1]) 2. Updates Term=1 (in some implementations, term updates upon receiving client requests) 3. Single node commits immediately (CommitIndex=1) 4. Applies to state machine (ApplyIndex=1, State={x:1}) This demonstrates RAFT’s fundamental workflow: log append → replicate → commit → apply. In single-node scenarios, this process completes instantaneously. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:4:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 3: Nodes Joining the Cluster (Diagram 3) Node2 and Node3 join as Followers: - New nodes' initial state: Term=0, empty log, empty state machine - Discover existing Leader through \"join\" operation New nodes join in Follower state and require synchronization of historical data from the Leader. This demonstrates RAFT’s dynamic membership change capability. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:5:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 4: Log Synchronization (Diagram 4) Leader synchronization process: 1. Node1 sends AppendEntries RPC to Node2 and Node3 2. Contains historical log entry \"x=1\" 3. Followers update their state: - Term=1 (synchronize Leader's term) - Log=[x=1] (replicate log entry) - CommitIndex=1, ApplyIndex=1 (commit and apply) - State={x:1} (state machine synchronization) This stage showcases RAFT’s core mechanism: log replication. The Leader ensures all Followers maintain log consistency with itself. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:6:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 5: Cluster Expansion (Diagram 5) Continue adding Node4 and Node5: - New nodes complete joining and synchronization in one step via \"join \u0026 rpc sync\" - Eventually forms a 5-node cluster with all nodes in consistent state Clusters can expand dynamically, with new nodes automatically completing historical data synchronization upon joining. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:7:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 6: Batch Operations and Failures (Diagram 6 \u0026 6-Result) Diagram 6 illustrates the problem scenario: Leader processes multiple client requests: - x=2, y=1, y=2, update term - Node1 state: Term=2, CommitIndex=4, State={x:2, y:2} Diagram 6-Result shows recovery state: Leader continues operation: - Node1 successfully synchronizes to Node2, Node4, Node5 - Node3 fails, maintaining old state (x=1) - Cluster continues providing service with majority nodes functioning This demonstrates RAFT’s fault tolerance: the cluster continues operating as long as a majority of nodes remain functional. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:8:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 7: Election Failure Scenario (Diagram 7) Network partition occurs: - Node1 becomes isolated (separate partition) - Node3 attempts election but fails: - Updates Term=2, becomes Candidate - Cannot obtain majority votes (log too stale) Partition state: - Partition 1: Node1 (isolated old Leader) - Partition 2: Node2, Node3, Node4, Node5 (no new Leader) This illustrates RAFT’s network partition handling mechanism: nodes with newer logs are more likely to become the new Leader. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:9:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 8: Election Within Partition (Diagram 8) Node2 initiates election: - Term=3, becomes Candidate - Sends RequestVote RPC to Node4, Node5, Node3 - Even though Node3's log is behind, it can still vote for Node2 - Meanwhile, isolated Node1 continues receiving client request \"x=9\" The partitioned old Leader still processes requests, but these operations will not be committed due to inability to obtain majority confirmation. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:10:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 9: New Leader Established with Existing Partition (Diagram 9) Node2 becomes the new Leader: - Obtains sufficient votes, Term=3 - Begins synchronizing to other nodes - Node1 and Node3 remain out of sync: - Node1: Has uncommitted \"x=9\", Term=2 - Node3: Still contains old data, Term=1 This proves RAFT’s partition tolerance: even with some unreachable nodes, the majority can continue functioning normally. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:11:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 10: Conflict Detection (Diagram 10) Node2 as the new Leader begins operation: - Synchronizes to Node4 and Node5 (successful) - Node1 and Node3 recovery - Cluster continues running on available majority nodes This state demonstrates the split-brain problem: the system contains both new and old Leader states. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:12:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Stage 11: Partition Healing and Conflict Resolution (Diagram 11) Final state after network partition repair: - All nodes reconnect - Node2 remains Leader, Term=3 - Conflict resolution process: 1. Node1 discovers higher term, steps down to Follower 2. Node1's uncommitted operation \"x=9\" is discarded (log truncation) 3. Node3 synchronizes to latest state 4. All nodes eventually consistent: State={x:2, y:2} Data loss: x=9 permanently lost as it was never majority-confirmed This represents the classic log conflict resolution scenario in RAFT: Higher term Leaders are authoritative Operations not confirmed by majority are discarded All nodes eventually achieve strong consistency ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:13:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Key Characteristics Analysis of RAFT Algorithm Through these 11 diagrams, we can summarize the important characteristics of RAFT: 1. Strong Consistency All nodes eventually reach identical states Data reliability ensured through majority commit 2. Partition Tolerance During network partitions, majority partition continues service Minority partition cannot handle write operations 3. Leader Election At most one Leader at any given time Latest nodes elected through term and log comparison 4. Log Replication Leader replicates logs to all Followers Ensures log ordering and consistency 5. Failure Recovery Nodes can automatically synchronize after rejoining from failure Conflicting logs are correctly overwritten ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:14:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Practical Application Considerations Performance Characteristics: Write operations require majority confirmation, resulting in higher latency Read operations can be performed from Leader or Followers Network partitions affect availability Suitable Use Cases: Configuration management systems (such as etcd) Distributed databases (such as TiKV) Distributed lock services Important Considerations: Odd-numbered node clusters are preferable (avoid split-brain) Network quality significantly impacts performance Need to consider safety of membership changes ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:15:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Cluster","Algorithm"],"content":"Conclusion The RAFT algorithm elegantly solves distributed consensus problems through clear role separation and simple rules. This article comprehensively demonstrates the progression from single-node to complex failure scenarios through 11 progressive diagrams. Understanding these scenarios is crucial for designing and implementing reliable distributed systems. RAFT’s success lies in its understandability: compared to Paxos’s complex proofs, RAFT employs intuitive concepts (terms, elections, log replication) that enable developers to truly comprehend and correctly implement distributed consistency systems. ","date":"2025-09-26","objectID":"/en/2025/09/cluster-1.-understanding_raft_algorithm/:16:0","tags":["Cluster","RAFT","Distributed","Algorithm"],"title":"[Cluster] 1. RAFT Algorithm: A Complete Evolution from Single Node to Distributed Consensus","uri":"/en/2025/09/cluster-1.-understanding_raft_algorithm/"},{"categories":["Github","MyBatis","plugins"],"content":" 🔗 Project Repository: mybatis-generator-custome-plugins A powerful collection of MyBatis Generator custom plugins designed to enhance code generation capabilities with MySQL-specific features, DTO layer generation, and automatic Service layer generation. ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:0:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🚀 Features Overview This plugin collection includes 6 custom plugins: Plugin Name Description Key Features InsertIgnoreIntoPlugin MySQL INSERT IGNORE statement support Batch insert ignoring duplicate records InsertOnDuplicateKeyPlugin MySQL ON DUPLICATE KEY UPDATE support Auto update on insert conflicts ReplaceIntoPlugin MySQL REPLACE INTO statement support Replace insert operations DtoGeneratorPlugin DTO layer code generation Lombok annotations, Entity conversion methods ServiceGeneratorPlugin Service layer code generation Interface + Implementation, Complete CRUD operations CustomerMapperPlugin Custom Mapper generation Extended native Mapper functionality ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:1:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"📦 Dependency Analysis ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:2:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Core Dependencies \u003c!-- MyBatis Generator Core Dependency --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.generator\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-core\u003c/artifactId\u003e \u003cversion\u003e1.4.2\u003c/version\u003e \u003c/dependency\u003e \u003c!-- MyBatis Dynamic SQL Support --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.dynamic-sql\u003c/groupId\u003e \u003cartifactId\u003emybatis-dynamic-sql\u003c/artifactId\u003e \u003cversion\u003e1.5.2\u003c/version\u003e \u003c/dependency\u003e \u003c!-- MyBatis Spring Boot Integration --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.spring.boot\u003c/groupId\u003e \u003cartifactId\u003emybatis-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e3.0.5\u003c/version\u003e \u003c/dependency\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:2:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Maven Plugin Configuration \u003cplugin\u003e \u003cgroupId\u003eorg.mybatis.generator\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e1.4.2\u003c/version\u003e \u003cconfiguration\u003e \u003cverbose\u003etrue\u003c/verbose\u003e \u003coverwrite\u003etrue\u003c/overwrite\u003e \u003c/configuration\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.goody.utils\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-custome-plugins\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/plugin\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:2:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🔧 Plugin Details ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:3:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"MySQL Extension Plugins InsertIgnoreIntoPlugin Function: Adds INSERT IGNORE statement support to Mapper Generated Methods: insertIgnoreCustom(), insertIgnoreBatchCustom() Use Cases: Ignore primary key conflicts during batch inserts InsertOnDuplicateKeyPlugin Function: Adds ON DUPLICATE KEY UPDATE statement support to Mapper Generated Methods: insertOnDuplicateKeyCustom(), insertOnDuplicateKeyBatchCustom() Use Cases: Auto update records on duplicate key conflicts during insert ReplaceIntoPlugin Function: Adds REPLACE INTO statement support to Mapper Generated Methods: replaceIntoCustom(), replaceIntoBatchCustom() Use Cases: Replace existing records or insert new ones ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:3:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"DTO Layer Generation Plugin DtoGeneratorPlugin Function: Automatically generates DTO classes Features: Lombok annotation support (@Data, @Builder, @AllArgsConstructor, @NoArgsConstructor) Auto-generated fromEntity() and toEntity() conversion methods Package structure: *.model.dto ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:3:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Service Layer Generation Plugin ServiceGeneratorPlugin Function: Automatically generates Service interfaces and implementation classes Features: Complete CRUD operation methods Support for single and composite primary keys Spring annotation support (@Service, @Autowired) Package structure: *.service.interfaces and *.service.impl ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:3:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Custom Mapper Plugin CustomerMapperPlugin Function: Generates extended Mapper interfaces Package structure: *.dao.customer ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:3:4","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"💻 Usage Guide ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:4:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Step 1: Add Dependencies Add the plugin to your project: \u003cdependency\u003e \u003cgroupId\u003ecom.goody.utils\u003c/groupId\u003e \u003cartifactId\u003emybatis-generator-custome-plugins\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:4:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Step 2: Configure generatorConfig.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"https://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"\u003e \u003cgeneratorConfiguration\u003e \u003cclassPathEntry location=\"${user.home}/.m2/repository/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar\"/\u003e \u003ccontext id=\"dao\" targetRuntime=\"MyBatis3DynamicSql\"\u003e \u003cproperty name=\"autoDelimitKeywords\" value=\"true\"/\u003e \u003cproperty name=\"beginningDelimiter\" value=\"`\"/\u003e \u003cproperty name=\"endingDelimiter\" value=\"`\"/\u003e \u003c!-- Standard Plugins --\u003e \u003cplugin type=\"org.mybatis.generator.plugins.SerializablePlugin\"/\u003e \u003cplugin type=\"org.mybatis.generator.plugins.EqualsHashCodePlugin\"/\u003e \u003cplugin type=\"org.mybatis.generator.plugins.ToStringPlugin\"/\u003e \u003cplugin type=\"org.mybatis.generator.plugins.FluentBuilderMethodsPlugin\"/\u003e \u003c!-- Custom Plugins --\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.InsertIgnoreIntoPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.InsertOnDuplicateKeyPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.ReplaceIntoPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.DtoGeneratorPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.ServiceGeneratorPlugin\"/\u003e \u003cplugin type=\"com.goody.utils.mybatis.plugin.CustomerMapperPlugin\"/\u003e \u003ccommentGenerator\u003e \u003cproperty name=\"addRemarkComments\" value=\"true\"/\u003e \u003cproperty name=\"suppressDate\" value=\"true\"/\u003e \u003c/commentGenerator\u003e \u003cjdbcConnection driverClass=\"com.mysql.cj.jdbc.Driver\" connectionURL=\"jdbc:mysql://127.0.0.1:3306/your_database\" userId=\"your_username\" password=\"your_password\"\u003e \u003cproperty name=\"useSSL\" value=\"false\"/\u003e \u003cproperty name=\"serverTimezone\" value=\"Asia/Shanghai\"/\u003e \u003cproperty name=\"nullCatalogMeansCurrent\" value=\"true\"/\u003e \u003c/jdbcConnection\u003e \u003cjavaTypeResolver\u003e \u003cproperty name=\"forceBigDecimals\" value=\"true\"/\u003e \u003cproperty name=\"useJSR310Types\" value=\"true\"/\u003e \u003c/javaTypeResolver\u003e \u003cjavaModelGenerator targetPackage=\"com.yourpackage.model.entity\" targetProject=\"src/main/java\"\u003e \u003cproperty name=\"enableSubPackages\" value=\"true\"/\u003e \u003cproperty name=\"trimStrings\" value=\"true\"/\u003e \u003c/javaModelGenerator\u003e \u003cjavaClientGenerator type=\"ANNOTATEDMAPPER\" targetPackage=\"com.yourpackage.model.dao\" targetProject=\"src/main/java\"\u003e \u003cproperty name=\"enableSubPackages\" value=\"true\"/\u003e \u003c/javaClientGenerator\u003e \u003c!-- Configure Tables to Generate --\u003e \u003ctable schema=\"your_schema\" tableName=\"your_table\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e \u003c/context\u003e \u003c/generatorConfiguration\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:4:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Step 3: Execute Code Generation mvn mybatis-generator:generate ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:4:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"📝 Configuration Examples ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:5:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Single Primary Key Table Configuration \u003ctable schema=\"toy\" tableName=\"example_single_pk\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:5:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Composite Primary Key Table Configuration \u003ctable schema=\"toy\" tableName=\"example_double_pk\"\u003e \u003cproperty name=\"useActualColumnNames\" value=\"false\"/\u003e \u003c/table\u003e ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:5:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🏗️ Generated Code Analysis The plugins generate a complete code structure for each table: ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:6:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"File Structure src/main/java/ ├── com/yourpackage/model/ │ ├── entity/ # Entity Classes │ │ ├── Example.java │ │ └── ExampleDoublePk.java │ ├── dao/ # Standard Mapper Interfaces │ │ ├── ExampleMapper.java │ │ ├── ExampleDynamicSqlSupport.java │ │ └── customer/ # Custom Mapper Interfaces │ │ └── CustomerExampleMapper.java │ └── dto/ # DTO Classes │ ├── ExampleDTO.java │ └── ExampleDoublePkDTO.java ├── service/ │ ├── interfaces/ # Service Interfaces │ │ ├── IExampleService.java │ │ └── IExampleDoublePkService.java │ └── impl/ # Service Implementations │ ├── ExampleServiceImpl.java │ └── ExampleDoublePkServiceImpl.java ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:6:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Core Code Snippet Analysis MySQL Extension Methods (Mapper Layer) @Mapper public interface ExampleMapper { // Standard generated methods... // INSERT IGNORE Support @Insert({\"\u003cscript\u003e\" + \" INSERT IGNORE INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \"\u003c/script\u003e\"}) void insertIgnoreCustom(@Param(\"item\") Example record); // Batch INSERT IGNORE @Insert({\"\u003cscript\u003e\" + \" INSERT IGNORE INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" \u003cforeach collection='items' item='item' separator=','\u003e\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \" \u003c/foreach\u003e\" + \"\u003c/script\u003e\"}) void insertIgnoreBatchCustom(@Param(\"items\") Collection\u003cExample\u003e records); // ON DUPLICATE KEY UPDATE Support @Insert({\"\u003cscript\u003e\" + \" INSERT INTO example\" + \" (`id`, `name`, `created`, `updated`)\" + \" VALUES\" + \" (#{item.id}, #{item.name}, #{item.created}, #{item.updated})\" + \" AS r\" + \" ON DUPLICATE KEY UPDATE\" + \" name = r.name, updated = r.updated\" + \"\u003c/script\u003e\"}) void insertOnDuplicateKeyCustom(@Param(\"item\") Example record); } DTO Classes (Data Transfer Layer) @Data @AllArgsConstructor @NoArgsConstructor @Builder public class ExampleDTO { private Long id; private String name; private LocalDateTime created; private LocalDateTime updated; // Entity to DTO conversion public static ExampleDTO fromEntity(Example entity) { if (entity == null) return null; return ExampleDTO.builder() .id(entity.getId()) .name(entity.getName()) .created(entity.getCreated()) .updated(entity.getUpdated()) .build(); } // DTO to Entity conversion public Example toEntity() { return new Example() .withId(this.id) .withName(this.name) .withCreated(this.created) .withUpdated(this.updated); } } Service Interfaces (Service Layer Interface) Single Primary Key Service Interface: public interface IExampleService { int save(ExampleDTO dto); int saveBatch(List\u003cExampleDTO\u003e dtoList); int update(ExampleDTO dto); int deleteById(Long id); // Single parameter ExampleDTO findById(Long id); // Single parameter List\u003cExampleDTO\u003e findAll(); } Composite Primary Key Service Interface (Plugin Fixed): public interface IExampleDoublePkService { int save(ExampleDoublePkDTO dto); int saveBatch(List\u003cExampleDoublePkDTO\u003e dtoList); int update(ExampleDoublePkDTO dto); int deleteById(Long id, Long id2); // Multi-parameter support ExampleDoublePkDTO findById(Long id, Long id2); // Multi-parameter support List\u003cExampleDoublePkDTO\u003e findAll(); } Service Implementation (Service Layer Implementation) Core Improvement for Composite Primary Key Handling: @Service public class ExampleDoublePkServiceImpl implements IExampleDoublePkService { @Autowired private ExampleDoublePkMapper exampleDoublePkMapper; @Override public int deleteById(Long id, Long id2) { if (id == null || id2 == null) { return 0; } // Correctly calls composite primary key method return exampleDoublePkMapper.deleteByPrimaryKey(id, id2); } @Override public ExampleDoublePkDTO findById(Long id, Long id2) { if (id == null || id2 == null) { return null; } // Correctly calls composite primary key method ExampleDoublePk entity = exampleDoublePkMapper .selectByPrimaryKey(id, id2).orElse(null); return entity != null ? ExampleDoublePkDTO.fromEntity(entity) : null; } } ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:6:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Key Feature Analysis Composite Primary Key Support Issue: Previous version incorrectly attempted to use single primary key type Solution: Auto-detects primary key column count, generates multi-parameter methods for composite keys Implementation: List\u003cIntrospectedColumn\u003e primaryKeyColumns MySQL-Specific Features INSERT IGNORE: Ignores duplicate key errors, continues inserting other records ON DUPLICATE KEY UPDATE: Auto updates specified fields on insert conflicts REPLACE INTO: MySQL’s replace insert operation Automation Level Dependency Injection: Auto-generates Spring @Autowired annotations Null Checks: Auto-generates parameter null validation Conversion Methods: Automatic conversion between DTO and Entity ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:6:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🏗️ Project Structure mybatis-generator-custome-plugins/ ├── src/main/java/com/goody/utils/mybatis/plugin/ │ ├── CustomerMapperPlugin.java │ ├── DtoGeneratorPlugin.java │ ├── InsertIgnoreIntoPlugin.java │ ├── InsertOnDuplicateKeyPlugin.java │ ├── ReplaceIntoPlugin.java │ └── ServiceGeneratorPlugin.java ├── src/main/resources/ │ └── generatorConfig.xml ├── src/test/ # Generated code examples └── pom.xml ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:7:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🎯 Usage Recommendations Development Workflow: Design database table structure first, then use plugins to generate complete layered code Composite Primary Keys: Plugin perfectly supports composite primary keys, no manual modification of generated code needed Extensibility: Can inherit generated Service interfaces to add complex business logic MySQL Optimization: Properly use INSERT IGNORE and ON DUPLICATE KEY features to improve performance ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:8:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"🔍 Source Code Analysis This section provides deep insights into the implementation details of each plugin for developers interested in understanding or extending the functionality. ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:0","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Plugin Architecture Overview All plugins extend PluginAdapter and follow MyBatis Generator’s plugin lifecycle: public abstract class PluginAdapter implements Plugin { // Validation phase public boolean validate(List\u003cString\u003e warnings); // Code generation hooks public boolean clientGenerated(Interface interfaze, IntrospectedTable introspectedTable); public List\u003cGeneratedJavaFile\u003e contextGenerateAdditionalJavaFiles(IntrospectedTable introspectedTable); } ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:1","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"InsertIgnoreIntoPlugin Implementation Analysis Core Method Generation Strategy The plugin uses a sophisticated approach to generate SQL templates dynamically: private Method insertIgnoreIntoOne(Interface interfaze, IntrospectedTable introspectedTable) { // Dynamic column mapping using Stream API final String columnNames = introspectedTable.getAllColumns() .stream() .map(column -\u003e String.format(\"`%s`\", column.getActualColumnName())) .collect(Collectors.joining(\", \")); // Parameter binding with MyBatis syntax final String columnValueNames = introspectedTable.getAllColumns() .stream() .map(column -\u003e String.format(\"#{item.%s}\", column.getJavaProperty())) .collect(Collectors.joining(\", \")); // Template-based annotation generation String insertIgnore = String.format(\"@Insert({\" + \"\\\"\u003cscript\u003e\\\" +\\n\" + \" \\\" INSERT IGNORE INTO %s\\\" +\\n\" + \" \\\" (%s)\\\" +\\n\" + \" \\\" VALUES\\\" +\\n\" + \" \\\"(%s)\\\" +\\n\" + \" \\\"\u003c/script\u003e\\\"\" + \"})\", tableName, columnNames, columnValueNames); } Key Technical Innovations Stream-based Field Transformation: Efficiently converts database schema to Java method parameters Dynamic SQL Template Generation: Creates parameterized SQL with proper escaping Batch Processing Support: Automatic \u003cforeach\u003e loop generation for batch operations ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:2","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"DtoGeneratorPlugin Implementation Analysis Package Resolution Algorithm private TopLevelClass generateDtoClass(IntrospectedTable introspectedTable) { // Intelligent package transformation String entityFullType = introspectedTable.getBaseRecordType(); String entityPackage = entityFullType.substring(0, entityFullType.lastIndexOf('.')); String dtoFullPackage = entityPackage.replace(\".entity\", \".\" + DTO_PACKAGE); // Type-safe class instantiation FullyQualifiedJavaType dtoType = new FullyQualifiedJavaType(dtoFullPackage + \".\" + dtoClassName); TopLevelClass dtoClass = new TopLevelClass(dtoType); } Field Generation with Metadata Preservation // Column-to-field conversion with comment preservation for (IntrospectedColumn column : allColumns) { Field field = new Field(column.getJavaProperty(), column.getFullyQualifiedJavaType()); field.setVisibility(JavaVisibility.PRIVATE); // Preserve database column comments if (column.getRemarks() != null \u0026\u0026 !column.getRemarks().trim().isEmpty()) { field.addJavaDocLine(\"/**\"); field.addJavaDocLine(\" * \" + column.getRemarks()); field.addJavaDocLine(\" */\"); } dtoClass.addField(field); } ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:3","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"ServiceGeneratorPlugin Implementation Analysis Composite Primary Key Resolution The most sophisticated feature is intelligent composite primary key handling: // Dynamic primary key analysis List\u003cIntrospectedColumn\u003e primaryKeyColumns = introspectedTable.getPrimaryKeyColumns(); // Adaptive method signature generation for (IntrospectedColumn column : primaryKeyColumns) { String paramName = column.getJavaProperty(); method.addParameter(new Parameter(column.getFullyQualifiedJavaType(), paramName)); } // Dynamic method call construction StringBuilder methodCall = new StringBuilder(\"return \" + mapperFieldName + \".deleteByPrimaryKey(\"); for (int i = 0; i \u003c primaryKeyColumns.size(); i++) { if (i \u003e 0) methodCall.append(\", \"); methodCall.append(primaryKeyColumns.get(i).getJavaProperty()); } methodCall.append(\");\"); Service Implementation Pattern // Null-safe parameter validation for (IntrospectedColumn column : primaryKeyColumns) { method.addBodyLine(\"if (\" + column.getJavaProperty() + \" == null) {\"); method.addBodyLine(\" return 0;\"); method.addBodyLine(\"}\"); } ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:4","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Advanced Patterns and Techniques Type Safety Enforcement // Generic type preservation FullyQualifiedJavaType listType = new FullyQualifiedJavaType(\"List\u003c\" + dtoClassName + \"\u003e\"); Parameter batchParameter = new Parameter(listType, \"dtoList\"); // Import resolution serviceInterface.addImportedType(new FullyQualifiedJavaType(\"java.util.List\")); SQL Injection Prevention All generated SQL uses parameterized queries: // Safe: Parameterized query \"INSERT IGNORE INTO \" + tableName + \" VALUES (#{item.id}, #{item.name})\" // Unsafe: String concatenation (never used) \"INSERT IGNORE INTO \" + tableName + \" VALUES (\" + item.getId() + \", '\" + item.getName() + \"')\" Memory-Efficient Generation // Lazy evaluation pattern for large codebases public List\u003cGeneratedJavaFile\u003e contextGenerateAdditionalJavaFiles(IntrospectedTable introspectedTable) { List\u003cGeneratedJavaFile\u003e files = new ArrayList\u003c\u003e(); // Generate only when needed if (shouldGenerateService(introspectedTable)) { files.add(createServiceInterface(introspectedTable)); files.add(createServiceImplementation(introspectedTable)); } return files; } ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:5","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Github","MyBatis","plugins"],"content":"Extension Points for Custom Development Custom Plugin Template public class CustomPlugin extends PluginAdapter { @Override public boolean validate(List\u003cString\u003e warnings) { // Plugin validation logic return true; } @Override public boolean clientGenerated(Interface interfaze, IntrospectedTable introspectedTable) { // Add methods to existing Mapper interfaces interfaze.addMethod(createCustomMethod(introspectedTable)); return super.clientGenerated(interfaze, introspectedTable); } private Method createCustomMethod(IntrospectedTable table) { // Custom method generation logic Method method = new Method(\"customMethod\"); method.setVisibility(JavaVisibility.PUBLIC); method.setAbstract(true); return method; } } Configuration-Driven Behavior @Override public boolean validate(List\u003cString\u003e warnings) { // Read plugin properties String enableFeature = getProperties().getProperty(\"enableCustomFeature\"); if (\"false\".equals(enableFeature)) { // Skip plugin execution return false; } return true; } This deep source code analysis reveals the sophisticated engineering behind each plugin, showcasing advanced Java code generation techniques and MyBatis Generator’s extensibility framework. This technical deep dive was created to provide comprehensive insights into the MyBatis Generator Custom Plugins architecture and implementation. ","date":"2025-09-24","objectID":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/:9:6","tags":["Github","MyBatis","plugins"],"title":"[Github] 2. MyBatis Generator Custom Plugins","uri":"/en/2025/09/github-2.-mybatis-generator-custom-plugins/"},{"categories":["Customs","IDE"],"content":"Introduction As one of the most powerful Java IDEs available, IntelliJ IDEA can be significantly enhanced through proper configuration and carefully selected plugins. This guide presents a curated collection of essential plugins and configuration skills that will transform your development experience. The goal is to provide ready-to-use configurations and plugins that immediately improve productivity, code quality, and development workflow. ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:1:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"Essential Plugins ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🔧 Development Tools CamelCase (3.0.12) Purpose: Quick text case conversion for variable names and strings. Features: Convert between camelCase, PascalCase, snake_case, and SCREAMING_SNAKE_CASE Keyboard shortcut: Shift + Alt + U Essential for refactoring and code consistency Supports multiple selection for batch conversions Use Cases: Converting database column names to Java field names Adapting naming conventions between different coding standards Quick text formatting during code reviews Maven Helper (4.23.222.2964.0) Purpose: Enhanced Maven project management and dependency analysis. Key Features: Visual dependency tree with conflict resolution Easy exclusion of transitive dependencies Quick Maven goal execution Dependency analyzer with search functionality Benefits: Resolve dependency conflicts quickly Understand project dependency structure Optimize build performance by identifying unused dependencies RestfulToolkit-fix (2.0.8) Purpose: RESTful API development assistance. Features: Navigate to REST endpoints quickly Generate HTTP requests from controller methods API documentation integration Request/response testing within IDE Workflow Enhancement: Jump to controller methods from URLs Test API endpoints without external tools Maintain API documentation alongside code ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:1","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🎨 Visual Enhancement Atom Material Icons Purpose: Beautiful file and folder icons for better visual organization. Features: Modern, colorful icon set Language-specific file icons Framework and library recognition Customizable icon themes Pokemon Progress Purpose: Fun Pokemon-themed progress bars. Features: Replace boring progress bars with Pokemon characters Multiple Pokemon themes available ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:2","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🛠️ Productivity Tools Grep Console Purpose: Advanced console output filtering and highlighting. Features: Real-time log filtering with regex patterns Color-coded log levels and patterns Save and reuse filter configurations Multiple console tabs with different filters ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:3","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🌐 API Development Apipost Purpose: API testing and documentation within IntelliJ IDEA. Features: Create and execute HTTP requests Tips: Only use for temporary testing Still recommend Postman for complex API testing ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:4","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"📊 Diagram \u0026 Design Tools PlantUML Purpose: Professional UML diagram creation using text-based syntax. Key Features: Real-time diagram preview with instant updates Comprehensive diagram types: sequence, class, activity, use case, component, deployment, state, and more Export to multiple formats (PNG, SVG, PDF, LaTeX) Integration with code documentation and comments Version control friendly (text-based source) Excalidraw Purpose: Hand-drawn style diagrams for brainstorming and creative design. Key Features: Intuitive drag-and-drop interface Hand-drawn aesthetic for approachable diagrams Real-time collaborative whiteboarding Extensive shape and element library Export to various formats (PNG, SVG, JSON) ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:2:5","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"Essential IDE Skills \u0026 Configurations ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:3:0","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🚀 Automation Settings Commit-Time Code Formatting Configuration: Enable automatic code formatting on commit Setup Path: VCS → Git → Enable \"Reformat code\" and \"Optimize imports\" Local Variable Final Enhancement Configuration: Automatic final modifier for local variables Setup: Editor → Inspections → Java → Code Style → Local variable or parameter can be final ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:3:1","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"⌨️ Productivity Shortcuts Advanced Cursor Operations Essential Shortcuts: Alt + Click: Add cursor at click position Alt + Shift + Click: Create rectangular selection Ctrl + Alt + Shift + J: Select all occurrences Alt + J: Select next occurrence Workflow Enhancement: Multi-line editing efficiency Bulk text replacements Simultaneous code modifications Custom Live Templates Popular Custom Templates: .str → String.valueOf($VAR$) .not → !$VAR$ .nn → if ($VAR$ != null) .null → if ($VAR$ == null) Setup Path: Editor → Live Templates → Create new template group Productivity Benefits: Faster common code patterns Reduced typing and syntax errors Consistent coding patterns across team ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:3:2","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Customs","IDE"],"content":"🎯 Code Quality Settings Import Optimization Configuration: Prevent wildcard imports Setup Path: Editor → Code Style → Java → Imports Set “Class count to use import with ‘*’” to 999 Set “Names count to use static import with ‘*’” to 999 Benefits: Explicit import declarations Avoid naming conflicts Smaller JAR file sizes Better IDE performance File Header Templates Configuration: Automatic file headers with author and date Template Example: /** * TODO: Add class description * * @author ${USER} * @version 1.0, ${DATE} * @since 1.0.0 */ Setup Path: Editor → File and Code Templates → Includes Line Ending Consistency Configuration: Ensure files end with line breaks Setup Path: Editor → General → On Save → \"Ensure every saved file ends with a line break\" Benefits: Consistent file endings across operating systems Better compatibility with command-line tools Cleaner git diffs ","date":"2025-09-21","objectID":"/en/2025/09/custom-1.-customs-idea/:3:3","tags":["Customs","idea","plugins"],"title":"[Customs] 1. IntelliJ IDEA Configuration \u0026 Recommended Plugins","uri":"/en/2025/09/custom-1.-customs-idea/"},{"categories":["Java"],"content":"Mockito Basic Usage In unit testing, many tests (except Util classes) need to mock some services to ensure only the current logic being tested is actually tested. Specifically, you need to first mock an object, then mock the methods of this object, and then you can use the mocked methods to test the logic you want to test. ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:1:0","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Mock Objects First, you need to declare the interfaces/implementation classes that need to be mocked in the Test class. For example: @MockBean private IOssService ossService; Sometimes, you also need to manually mock something directly, for example, when you need to mock Redis operations, you can: RSet\u003cLong\u003e redisSet = Mockito.mock(RSet.class); Note: Don’t mock primitive types like int, long, etc. There’s another way using @SpyBean. This will be skipped for now and introduced in later sections. ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:1:1","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Mock Methods Assume there’s an interface: public interface IUserService { Long add(UserDTO dto); void remove(Long userId); Optional\u003cUserDTO\u003e find(String username); Optional\u003cUserDTO\u003e find(Long userId); } With userService already mocked: @MockBean private IUserService userService; We can mock the add method so that any parameter passed will directly return 100. Mockito.doReturn(100L).when(userService).add(any()); You can also do it this way: Mockito.when(userService.add(any())).thenReturn(100L); However, for the same service, don’t mix these two approaches, otherwise sometimes the second mock may become ineffective. When you find that your mocked methods are ineffective, or you encounter some mysterious errors, please use one mocking approach consistently. If it still doesn’t work, try switching to the other approach. When you need to mock a void method, you can: Mockito.doNothing().when(userService).remove(anyLong()); If you need to simulate error conditions, you can: Mockito.doThrow(...).when(userService).remove(anyLong()); When you need to mock specific data, like userId = 1 has a user, userId = 2 has no user, you can mock like this: Mockito.doReturn(Optional.of(UserDTO.builder().build())).when(userService).find(1L); Mockito.doReturn(Optional.empty()).when(userService).find(2L); Mock has more general approaches. If you want to return data when userId \u003c 10, and not return data in other cases: Mockito.doAnswer(invocation -\u003e { Long userId = (Long) invocation.getArguments()[0]; if (userId \u003c 10L) { return UserDTO.builder().id(userId).build(); } return null; }).when(userService).find(anyLong()); Or: Mockito.when(userService.find(anyLong())).thenAnswer(invocation -\u003e { Long userId = (Long) invocation.getArguments()[0]; if (userId \u003c 10L) { return UserDTO.builder().build(); } return null; }); Note that doAnswer can also mock void return values. Suppose I now want to mock Redis operations: // Mock Redis get and set operations. RAtomicLong mockValue = Mockito.mock(RAtomicLong.class); doAnswer(invocation -\u003e { Long newValue = (Long) (invocation.getArguments()[0]); doReturn(true).when(mockValue).isExists(); doReturn(newValue).when(mockValue).get(); return null; }).when(mockValue).set(anyLong()); In this example, when calling Redis set, we also mock get to return the value that was just set. This simulates Redis operations. When you mock overloaded methods, you may encounter errors, for example: Mockito.when(userService.find(any())).thenReturn(Optional.empty()); At this point, find(any()) can match both methods, which will cause problems. The solution is: Mockito.when(userService.find(anyLong())).thenReturn(Optional.empty()); Mockito.when(userService.find(anyString())).thenReturn(Optional.empty()); When using any(), be careful: Don’t use any() to represent long, int values, otherwise you’ll get NPE. You can specifically use any(class) for specific inputs, like any(LocalDateTime.class) can represent any LocalDateTime parameter. When using mock, you can also use doCallRealMethod() or thenCallRealMethod() to call the original implementation, but I personally don’t recommend this approach as it generally causes various problems. Specific solutions will be covered later. ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:1:2","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Testing Results Generally, after you’ve mocked all called methods and used assert tools to verify the output, the program is basically running as expected. public interface IUserBizService { UserDTO reg(UserDTO dto); } For example, if you’re testing this reg method, Mockito tools can verify whether userService.add() method was actually called once: Mockito.verify(userService, times(1)).add(any()); However, sometimes you need to check other things, like the call parameters of your mocked methods. This is when you need Mockito tools: ArgumentCaptor\u003cUserDTO\u003e captor = ArgumentCaptor.forClass(UserDTO.class); Mockito.verify(userService, times(1)).add(captor.capture()); List\u003cUserDTO\u003e users = captor.getAllValues(); // Here you can verify the content of users assertEquals(......); There’s also a simpler approach: Mockito.verify(userService, times(1)).add((UserDTO) argThat(u -\u003e { assertEquals(userId, t.getId()); ... return true; })); ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:1:3","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"SpyBean Example In the above examples, we used MockBean, which is the most widely used mocking approach. But sometimes, tests need to go deep into implementation classes, modify some logic, and then test. This is when SpyBean is needed. MockBean is equivalent to completely mocking a class, where all methods in this class are mocked and cannot be called directly without first mocking them. SpyBean is equivalent to taking a real implementation and then only mocking part of its methods. For example, suppose in the IUserService implementation, there’s this code: public class UserServiceImpl implements IUserService { @Override public void remove(Long userId) { Optional\u003cUserDTO\u003e dto = this.find(userId); // Note this line if (!dto.isPresent()) { throw new Error(404); } ... } } At this point, the remove method calls the find method. If you want to test this remove method, mocking the entire implementation class is obviously not feasible. If you don’t mock, the return value of this find method is uncontrollable. If you directly put data in the database, that’s not a complete unit test because you’re using external services. This is when SpyBean should be used. You can mock only the find method, then directly call remove to execute the test. @SpyBean private IUserService userService; Similarly, there are some logic that’s not easy to test, like: public void doJob() { while(true) { if (xxx) { break; } doWork(); sleep(); } } public void sleep() { try { Thread.sleep(2000); } catch (...) { ... } } You need to test that this while loop runs according to your logic, but this sleep method will actually sleep, making the entire test extremely troublesome. Using SpyBean solves this well. doNothing().when(xxxService).sleep(); xxxService.doJob(); verify(xxxService, times(1)).sleep(); verify(xxxService, times(1)).doWork(); Similarly, when you need to test some time-related operations, this logic is critical but closely related to current time, SpyBean is also needed. public void doJob() { LocalDateTime now = getNow(); ... } public LocalDateTime getNow() { return DateUtil.now(); } Mock operation: doReturn(LocalDateTime.of(2021, 10, 1)).when(xxxService).getNow(); xxxService.doJob(); ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:1:4","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Test Case Specifications ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:0","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Test Case Naming Rules For method startTask under test, there might be the following test cases: startTask_noPerm startTask_banned startTask_succeed startTask_limited … javaguide.html#s5.2.3-method-names ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:1","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Test Case Comment Rules 1 Test scenario description 2 Expected results, actual results ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:2","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Test Scope Rules Use Mockito to isolate test boundaries Use Postman for integration testing Controller layer should also have test cases ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:3","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Code Coverage Maximize code coverage ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:4","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java"],"content":"Test Case Specifications Build test data Use test data to build mock methods Execute methods Verify mock results Key Points Unit test cases should embody the concept of unit. When building test data, attention should be paid to sufficient unit isolation between build data Simply put, unit test code also needs to be elegant enough with high extensibility, so that when business modifications occur later, testing can be better extended ","date":"2025-09-21","objectID":"/en/2025/09/java-2.-unit-test/:2:5","tags":["Java","Unit Test"],"title":"[Java] 2. Unit Test Basic Usage","uri":"/en/2025/09/java-2.-unit-test/"},{"categories":["Java","Spring"],"content":"Introduction Spring-web provides excellent support for asynchronous operations, which can be used for many optimizations through asynchronous return forms: Improve throughput Fine-tune execution thread pools for various business operations ","date":"2025-09-21","objectID":"/en/2025/09/spring-1.-spring-web-completionstage/:1:0","tags":["Java","Spring","web"],"title":"[Spring] 1. Spring Web CompletionStage Overview","uri":"/en/2025/09/spring-1.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"Sample Code /** * async interface controller * * @author Goody * @version 1.0, 2024/9/19 */ @RestController @RequestMapping(\"/goody\") @RequiredArgsConstructor @Slf4j public class GoodyAsyncController { private static final AtomicInteger COUNT = new AtomicInteger(0); private static final Executor EXECUTOR = new ThreadPoolExecutor( 10, 10, 10, TimeUnit.SECONDS, new ArrayBlockingQueue\u003c\u003e(10), r -\u003e new Thread(r, String.format(\"customer-t-%s\", COUNT.addAndGet(1))) ); @GetMapping(\"async/query1\") public CompletionStage\u003cString\u003e asyncQuery1() { log.info(\"async query start\"); return CompletableFuture.supplyAsync(() -\u003e { log.info(\"async query sleep start\"); ThreadUtils.sleep(1000); log.info(\"async query sleep done\"); log.info(\"async query done\"); return \"done\"; }, EXECUTOR); } @GetMapping(\"sync/query1\") public String syncQuery1() throws InterruptedException { log.info(\"sync query start\"); final CountDownLatch latch = new CountDownLatch(1); EXECUTOR.execute(() -\u003e { log.info(\"sync query sleep start\"); ThreadUtils.sleep(1000); log.info(\"sync query sleep done\"); latch.countDown(); }); latch.await(); log.info(\"sync query done\"); return \"done\"; } } Defined a custom thread pool for use in asynchronous scenarios Here’s one synchronous and one asynchronous endpoint, let’s look at the specific request scenarios ","date":"2025-09-21","objectID":"/en/2025/09/spring-1.-spring-web-completionstage/:2:0","tags":["Java","Spring","web"],"title":"[Spring] 1. Spring Web CompletionStage Overview","uri":"/en/2025/09/spring-1.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"Single Request Request Async Interface curl –location ‘127.0.0.1:50012/goody/async/query1’ 2024-09-19 15:56:43.408 INFO 24912 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 15:56:43.411 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 15:56:44.417 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 15:56:44.417 INFO 24912 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query done Request Sync Interface curl –location ‘127.0.0.1:50012/goody/sync/query1’ 2024-09-19 16:03:00.916 INFO 25780 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : sync query start 2024-09-19 16:03:00.917 INFO 25780 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : sync query sleep start 2024-09-19 16:03:01.924 INFO 25780 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : sync query sleep done 2024-09-19 16:03:01.924 INFO 25780 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : sync query done Analysis Actually, from a single request example, the difference isn’t that significant. But let’s analyze step by step. From the async interface, we can see that after CompletableFuture takes over, everything is handled by the custom thread pool, and all unpacking is handled by the spring-web framework. From the sync interface, we can see that after CompletableFuture takes over, the spring-web thread waits. We can actually infer that this is synchronous waiting. ","date":"2025-09-21","objectID":"/en/2025/09/spring-1.-spring-web-completionstage/:2:1","tags":["Java","Spring","web"],"title":"[Spring] 1. Spring Web CompletionStage Overview","uri":"/en/2025/09/spring-1.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"Concurrent Requests The Java program has been set to web threads=1, custom business threads=10 Request Script import threading import requests import datetime def get_current_time(): current_time = datetime.datetime.now() return current_time.strftime(\"%Y-%m-%d %H:%M:%S\") url = \"http://127.0.0.1:50012/goody/async/query1\" num_threads = 10 def send_request(): response = requests.get(url) print(f\"{get_current_time()} Request finished with status code: {response.status_code}\") threads = [] for _ in range(num_threads): thread = threading.Thread(target=send_request) threads.append(thread) thread.start() # Wait for all threads to complete for t in threads: t.join() Request Async Interface Java Output 2024-09-19 16:11:19.983 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.986 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.991 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.992 INFO 11712 --- [ customer-t-2] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.992 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.993 INFO 11712 --- [ customer-t-3] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.993 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.994 INFO 11712 --- [ customer-t-4] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.994 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.995 INFO 11712 --- [ customer-t-5] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.995 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.996 INFO 11712 --- [ customer-t-6] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.997 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.997 INFO 11712 --- [ customer-t-7] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.997 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.998 INFO 11712 --- [ customer-t-8] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.998 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:19.999 INFO 11712 --- [ customer-t-9] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:19.999 INFO 11712 --- [io-50012-exec-1] c.g.u.j.controller.GoodyAsyncController : async query start 2024-09-19 16:11:20.000 INFO 11712 --- [ customer-t-10] c.g.u.j.controller.GoodyAsyncController : async query sleep start 2024-09-19 16:11:20.989 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:20.989 INFO 11712 --- [ customer-t-1] c.g.u.j.controller.GoodyAsyncController : async query done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-2] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-8] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-6] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-10] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-9] c.g.u.j.controller.GoodyAsyncController : async query sleep done 2024-09-19 16:11:21.004 INFO 11712 --- [ customer-t-7] c.g.u.j.controller.GoodyAsy","date":"2025-09-21","objectID":"/en/2025/09/spring-1.-spring-web-completionstage/:2:2","tags":["Java","Spring","web"],"title":"[Spring] 1. Spring Web CompletionStage Overview","uri":"/en/2025/09/spring-1.-spring-web-completionstage/"},{"categories":["Java","Spring"],"content":"Source Code Analysis protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // =========================================== // Determine handler for the current request. // =========================================== mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { noHandlerFound(processedRequest, response); return; } // =========================================== // Determine handler adapter for the current request. // =========================================== HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) \u0026\u0026 isGet) { return; } } // =========================================== // Pre-processing // =========================================== if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // =========================================== // Actually invoke the handler. // asyncManager.isConcurrentHandlingStarted() = false // org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter#handleInternal // org.springframework.web.servlet.mvc.method.annotation.DeferredResultMethodReturnValueHandler // =========================================== mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); // =========================================== // asyncManager.isConcurrentHandlingStarted() = true // =========================================== // =========================================== // If it's an async return, this will be true, and subsequent post-processing won't execute // =========================================== if (asyncManager.isConcurrentHandlingStarted()) { return; } applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion if (mappedHandler != null) { // =========================================== // What's actually added is the following post-processing to restore context after awakening // org.springframework.boot.actuate.metrics.web.servlet.LongTaskTimingHandlerInterceptor // org.springframework.web.servlet.handler.ConversionServiceExposingInterceptor // org.springframework.web.servlet.resource.ResourceUrlProviderExposingInterceptor // =========================================== mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } } This is very Spring-style code: Pre-load various processors. Execute pre-processors Star","date":"2025-09-21","objectID":"/en/2025/09/spring-1.-spring-web-completionstage/:3:0","tags":["Java","Spring","web"],"title":"[Spring] 1. Spring Web CompletionStage Overview","uri":"/en/2025/09/spring-1.-spring-web-completionstage/"},{"categories":["Java"],"content":"Introduction Lombok is a Java library that can automatically generate repetitive code for Java classes, such as getter, setter, equals, and hashCode methods. ","date":"2025-09-21","objectID":"/en/2025/09/java-1.-lombok/:1:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/en/2025/09/java-1.-lombok/"},{"categories":["Java"],"content":"Principles Lombok’s working principle is based on Annotation Processors and the Java Compiler API. When Lombok is discovered by the compiler, it uses annotation processors to modify Java code. During this process, Lombok checks for specific annotations in classes and generates corresponding code based on these annotations, such as getter and setter methods. Specifically, Lombok uses the Apache BCEL (Bytecode Engineering Library) to directly manipulate Java bytecode, rather than through reflection or runtime operations. This way, Lombok can make modifications during compilation, thus improving performance and efficiency. ","date":"2025-09-21","objectID":"/en/2025/09/java-1.-lombok/:2:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/en/2025/09/java-1.-lombok/"},{"categories":["Java"],"content":"Annotations Define a clean Node class, after compilation it looks like: public class Node { private Long item1; private String item2; } public class Node { private Long item1; private String item2; public Node() { } } To explain, the Java compiler automatically provides a no-argument constructor because any class cannot be without a constructor. ","date":"2025-09-21","objectID":"/en/2025/09/java-1.-lombok/:3:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/en/2025/09/java-1.-lombok/"},{"categories":["Java"],"content":"Entity Classes @Data \u0026 @EqualsAndHashCode \u0026 @Getter \u0026 @Setter \u0026 @ToString @Data public class Node { private Long item1; private String item2; } Our most commonly used @Data is actually a collection of basic annotations, such as @Getter, @Setter, @EqualsAndHashCode, @ToString. public class Node { private Long item1; private String item2; public Node() { } public Long getItem1() { return this.item1; } public String getItem2() { return this.item2; } public void setItem1(Long item1) { this.item1 = item1; } public void setItem2(String item2) { this.item2 = item2; } public boolean equals(Object o) { if (o == this) { return true; } else if (!(o instanceof Node)) { return false; } else { Node other = (Node)o; if (!other.canEqual(this)) { return false; } else { Object this$item1 = this.getItem1(); Object other$item1 = other.getItem1(); if (this$item1 == null) { if (other$item1 != null) { return false; } } else if (!this$item1.equals(other$item1)) { return false; } Object this$item2 = this.getItem2(); Object other$item2 = other.getItem2(); if (this$item2 == null) { if (other$item2 != null) { return false; } } else if (!this$item2.equals(other$item2)) { return false; } return true; } } } protected boolean canEqual(Object other) { return other instanceof Node; } public int hashCode() { int PRIME = true; int result = 1; Object $item1 = this.getItem1(); result = result * 59 + ($item1 == null ? 43 : $item1.hashCode()); Object $item2 = this.getItem2(); result = result * 59 + ($item2 == null ? 43 : $item2.hashCode()); return result; } public String toString() { return \"Node(item1=\" + this.getItem1() + \", item2=\" + this.getItem2() + \")\"; } } Here are some basic advanced usages: @Getter @Setter public class Node { @Getter(AccessLevel.PRIVATE) private Long item1; @Setter(AccessLevel.PRIVATE) private String item2; } You can see that setting the corresponding PRIVATE permission methods become private. This helps with centralized permission management. public class Node { private Long item1; private String item2; public Node() { } public String getItem2() { return this.item2; } public void setItem1(Long item1) { this.item1 = item1; } private Long getItem1() { return this.item1; } private void setItem2(String item2) { this.item2 = item2; } } @AllArgsConstructor \u0026 @NoArgsConstructor \u0026 @RequiredArgsConstructor @NoArgsConstructor @AllArgsConstructor public class Node { private Long item1; private String item2; } Obviously, this will generate a no-argument constructor and an all-arguments constructor. The required arguments constructor would conflict with the no-argument constructor, so it’s shown separately. public class Node { private Long item1; private String item2; public Node() { } public Node(Long item1, String item2) { this.item1 = item1; this.item2 = item2; } } Required arguments constructor @RequiredArgsConstructor @AllArgsConstructor public class Node { private final Long item1; private String item2; } public class Node { private final Long item1; private String item2; public Node(Long item1) { this.item1 = item1; } public Node(Long item1, String item2) { this.item1 = item1; this.item2 = item2; } } Here’s the most classic combination: @Service @Slf4j @RequiredArgsConstructor public class Node { private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; } Output: @Service public class Node { private static final Logger log = LoggerFactory.getLogger(Node.class); private final Long item1; private final Long item2; private final Long item3; @Autowired @Lazy private String lazyBean; public Node(Long item1, Long item2, Long item3) { this.item1 = item1; this.item2 = item2; this.item3 = item3; } } You can see that beans that could originally be initialized can still be initialized, and those with conflicts or needing lazy loading will still follow lazy loading. Advanced usage: @Service @Slf4j @RequiredArgsConstructor(onConstructor_ = @JsonCreator) public class Node { private final Long item1","date":"2025-09-21","objectID":"/en/2025/09/java-1.-lombok/:3:1","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/en/2025/09/java-1.-lombok/"},{"categories":["Java"],"content":"How to Customize Lombok’s principle is to use Java’s built-in compilation API for enhancement, so we can follow this approach for our own enhancements. The core is to inherit and implement the javax.annotation.processing.AbstractProcessor class. Then through this class’s process method, modify the internal JavacTrees related variables. I’ve written a common base class for graphs, let me briefly introduce it: public abstract class BaseProcessor\u003cT extends Annotation\u003e extends AbstractProcessor { /** annotation type */ protected Class\u003cT\u003e clazz; /** javac trees */ protected JavacTrees trees; /** AST */ protected TreeMaker treeMaker; /** mark name */ protected Names names; /** log */ protected Messager messager; /** filer */ protected Filer filer; /** the jcTrees generated by annotation to add */ protected List\u003cJCTree\u003e annotationJCTrees; @Override public synchronized void init(ProcessingEnvironment processingEnv) { super.init(processingEnv); // transfer type T to Class final Type superclass = getClass().getGenericSuperclass(); if (superclass instanceof ParameterizedType) { this.clazz = (Class\u003cT\u003e) ((ParameterizedType) superclass).getActualTypeArguments()[0]; } else { this.clazz = null; } this.trees = JavacTrees.instance(processingEnv); this.messager = processingEnv.getMessager(); this.filer = processingEnv.getFiler(); final Context context = ((JavacProcessingEnvironment) processingEnv).getContext(); this.treeMaker = TreeMaker.instance(context); this.names = Names.instance(context); // init list annotationJCTrees = List.nil(); } /** * {@inheritDoc} */ @Override public final boolean process(Set\u003c? extends TypeElement\u003e annotations, RoundEnvironment roundEnv) { roundEnv.getElementsAnnotatedWith(this.clazz) .stream() .map(element -\u003e trees.getTree(element)) // NOTE(goody): 2022/5/5 // tree is the class input. Modify the `JCTree` to modify the method or argus // `visitClassDef` runs after than `visitAnnotation`, so method `visitAnnotation` can add `annotationJCTrees` to // `annotationJCTrees`. `visitClassDef` will prepend all .forEach(tree -\u003e tree.accept(new TreeTranslator() { @Override public void visitClassDef(JCTree.JCClassDecl jcClassDecl) { // NOTE(goody): 2022/5/4 https://stackoverflow.com/questions/46874126/java-lang-assertionerror-thrown-by-compiler-when-adding-generated-method-with-pa // setMethod var is a new Object from jcVariable, the pos should be reset to jcClass treeMaker.at(jcClassDecl.pos); // generate the new method or variable or something else final List\u003cJCTree\u003e jcTrees = generate(jcClassDecl); jcClassDecl.defs = jcClassDecl.defs.prependList(jcTrees); // add all elements in `annotationJCTrees` jcClassDecl.defs = jcClassDecl.defs.prependList(annotationJCTrees); super.visitClassDef(jcClassDecl); } @Override public void visitMethodDef(JCTree.JCMethodDecl jcMethodDecl) { if (isModify(jcMethodDecl)) { super.visitMethodDef(modifyDecl(jcMethodDecl)); } else { super.visitMethodDef(jcMethodDecl); } } @Override public void visitVarDef(JCTree.JCVariableDecl jcVariableDecl) { if (isModify(jcVariableDecl)) { super.visitVarDef(modifyDecl(jcVariableDecl)); } else { super.visitVarDef(jcVariableDecl); } } @Override public void visitAnnotation(JCTree.JCAnnotation jcAnnotation) { super.visitAnnotation(jcAnnotation); final JCTree.JCAssign[] jcAssigns = jcAnnotation.getArguments() .stream() .filter(argu -\u003e argu.getKind().equals(Tree.Kind.ASSIGNMENT)) .map(argu -\u003e (JCTree.JCAssign) argu) .toArray(JCTree.JCAssign[]::new); if (jcAssigns.length \u003e 0) { annotationGenerateJCTree(handleJCAssign(List.from(jcAssigns))); } } })); return true; } /** * subclass should implement this method to add method or variable or others * * @param jcClassDecl jcClassDecl * @return new JCTree list */ private List\u003cJCTree\u003e generate(JCTree.JCClassDecl jcClassDecl) { final JCTree[] trees = generate() .toArray(JCTree[]::new); // method Trees final JCTree[] methodTrees = jcClassDecl.defs .stream() .filter(k -\u003e k.getKind().equals(Tree.Kind.METHOD)) .map(tree -\u003e (JCTree.JCMet","date":"2025-09-21","objectID":"/en/2025/09/java-1.-lombok/:4:0","tags":["Java","Lombok"],"title":"[Java] 1. Lombok","uri":"/en/2025/09/java-1.-lombok/"},{"categories":["MySQL"],"content":"Preface Let’s start with some of the most common optimization measures we use in daily learning and business scenarios Do more things in the same unit of time QuickSort uses the binary search idea to sort multiple arrays within a single loop Query less information in total KMP algorithm preprocesses the main string to reduce the number of matches. This is the power of logic Search trees use binary search ideas to filter half of the remaining data each time to improve efficiency. This is the power of data structures For MySQL fast queries, the most critical core is to query less data - the less, the faster. This entire article revolves around this statement. ","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:1:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"Does Not Having an Index Always Mean Slow Queries? Define data insertion function DROP TABLE IF EXISTS user; CREATE TABLE user ( id bigint(20) NOT NULL COMMENT 'User ID', biz_id bigint(20) NOT NULL COMMENT 'Business ID', message text COMMENT 'Business information', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT 'User' CHARSET = utf8mb4; DELIMITER $$ CREATE PROCEDURE insertUserData(IN start_id int, IN end_id int, IN bizId int) BEGIN DECLARE i int DEFAULT start_id; WHILE i \u003c= end_id DO INSERT INTO user (id, biz_id, message) VALUES (i, bizId, SUBSTRING(MD5(RAND()), 1, 8)); SET i = i + 1; END WHILE; END $$ DELIMITER ; Prepare some data, where bizId = 1 has only two records, the first and last data -- Insert first business id 1 data CALL insertUserData(1, 1, 1); -- Insert one million other data CALL insertUserData(2, 1000000, 2); -- Insert second business id 1 data CALL insertUserData(1000001, 1000001, 1); Looking at the data distribution, I think everyone already knows what I want to express. Even without using indexes, we have scenarios for fast queries here. Query limit 1 EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ALL null null null null 997030 10 Using where SELECT * FROM user WHERE biz_id = 1 LIMIT 1; [2023-08-27 10:53:43] 1 row retrieved starting from 1 in 97 ms (execution: 5 ms, fetching: 92 ms) SELECT * FROM user WHERE biz_id = 1 LIMIT 2; [2023-08-27 10:53:48] 2 rows retrieved starting from 1 in 1 s 199 ms (execution: 1 s 172 ms, fetching: 27 ms) EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ALL null null null null 997030 10 Using where From a practical perspective, we can see that the execution time difference between limit 1 and limit 2 is huge. Here’s a tip: we can use ChatGPT to efficiently organize and analyze explain results. (Wenxin Yiyan) This query plan shows a full scan of the user table without using indexes, and the WHERE clause is used to filter results. If the table is large or query performance is poor, this may cause performance issues. Optimization may need to consider using appropriate indexes to improve query performance. We can see that the execution plans for limit 1 and limit 2 are almost identical. This confirms that although the time difference is huge, the execution logic is the same for MySQL. So why is there such a huge difference? We need to analyze this specifically. In the limit 1 scenario, based on a full table scan, it returns immediately when the first data is found. This is equivalent to querying only one record. In the limit 2 scenario, the second data is after one million records, so according to the execution plan, it must scan the entire table to completely find the data. In summary, does not having an index always mean slow queries? Not necessarily. Indexes only reduce the amount of data queried, but when the data volume is already extremely small, it won’t be slower. ","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:2:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"Indexes The explain shows “Using where”, and according to ChatGPT’s response and our own analysis, we can naturally think of using indexes to optimize queries. Let’s try using B+ tree indexes. ","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:3:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"B+ Tree Indexes https://www.cs.usfca.edu/~galles/visualization/Algorithms.html CREATE INDEX idx_user_biz_id ON user (biz_id); DROP INDEX idx_user_biz_id ON user; EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 2 100 null SELECT * FROM user WHERE biz_id = 1 LIMIT 1 [2023-08-27 21:09:37] 1 row retrieved starting from 1 in 48 ms (execution: 4 ms, fetching: 44 ms) EXPLAIN SELECT * FROM user WHERE biz_id = 1 LIMIT 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 2 100 null SELECT * FROM user WHERE biz_id = 1 LIMIT 2 [2023-08-27 21:09:04] 2 rows retrieved starting from 1 in 48 ms (execution: 4 ms, fetching: 44 ms) As you can see, after adding an index, query time was greatly optimized. The execution plan evolved from “Using where” to using “const” (Wenxin Yiyan) In your example, this query operates on a table named \"user\" using an index named \"idx_user_biz_id\" with an index length of 8 bytes the comparison column is constant (const), the system estimates it needs to scan 2 rows of data, returning 100% of the rows, with no additional information. Why B+ Trees Balanced Binary Tree In a balanced binary tree, we can think about the query process. Suppose we want to query all data from 5 to 16. We need to query six nodes, and the order retrieved is not naturally sorted, requiring in-order traversal algorithm implementation. So we can see that balanced trees are natural data structures for finding single nodes, but support for range queries is relatively poor. The first optimization point, if you want to understand in detail, requires deep understanding of disk pre-read optimization. I think it’s not very necessary to understand, but can be simply understood as too many IO operations In balanced trees, logically close places may be physically distant, leading to longer IO times during disk rotation operations The second optimization point is that there’s too much useless data when reading IO data, because the data read back must be larger than the size of a single node’s data. Additionally, balanced binary trees require rotation to maintain balance, which is difficult to implement efficiently in memory operations B Tree To address the above optimization points, we can naturally optimize from binary to multi-way, thus optimizing the problem of too much useless data in single nodes. Because when single nodes are larger, the number of IO operations also decreases. This naturally leads to the idea of B trees. By expanding nodes to form multi-way structures. But actually thinking carefully, in-order traversal algorithm implementation is still needed. For range queries, optimization is only at the single node level. When the range span is large, there’s still room for performance optimization. B+ Tree B+ trees first further optimize single node storage efficiency (no Values). This way single nodes can support more branches. At the same time, the bottom leaf nodes are combined into a linked list through pointers, thus optimizing traversal logic. Note: The bottom layer of B+ trees contains all nodes from upper layers. ","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:3:1","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"B+ Tree Composite Indexes We’ve understood the B+ tree part, which can be simply understood as clustered indexes with Values at the bottom leaf nodes. Now let’s understand composite indexes. Leftmost Matching Principle The specific principle doesn’t need explanation, just understanding. Why are B+ Tree Indexes Sorted? In the above diagram, our first-level index is ordered, which is easy to understand. When the first-level index is fixed, its second-level index underneath is also ordered. My understanding is that B+ trees first search the first-level index. When the first-level index is determined, the B+ tree converts to a second-level index B+ tree. And so on. Note: Composite indexes are on one tree. Why Do B+ Tree Indexes Fail? First, we can understand that the second-level index order when first-level index = 1 and the second-level index order when first-level index = 2 have no necessary sequential relationship. They just both have the same monotonicity. Following my previous direction of index tree changes, when the first-level index becomes a range, there’s more than one tree for second-level indexes. We can’t perform the same range single query on multiple trees. We can only combine multiple times. What is Table Lookup? Assuming everyone understands clustered indexes and secondary indexes. Table lookup, simply put, means after the secondary index is finished, it can’t continue, like in the previous table. Note: Type represents the optimization order of access types, from best to worst: system-\u003econst-\u003eeq_ref-\u003eref-\u003eref_or_null-\u003eindex_merge-\u003eunique_subquery-\u003eindex_subquery-\u003erange-\u003eindex-\u003eall. Generally, we hope to reach ref and eq_ref levels, and range queries need to reach range level. DROP TABLE IF EXISTS user; CREATE TABLE user ( id bigint(20) NOT NULL COMMENT 'User ID', biz_id bigint(20) NOT NULL COMMENT 'Business ID', message text COMMENT 'Business information', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT 'User' CHARSET = utf8mb4; CREATE INDEX idx_user_biz_id ON user (biz_id); Using index completely EXPLAIN SELECT id FROM `user` WHERE biz_id = 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 100 Using index (ChatGPT) The query involves the \"user\" table. The table access method is \"ref\", indicating that an index was used for the query. The index used is \"idx_user_biz_id\", and the query uses this index. The index length used is 8 bytes. The query reference value is \"const\", indicating the query uses a constant value for filtering. The query returns 498380 rows with a filtering rate of 100%. \"Using index\" indicates the query uses a covering index, meaning query results can be obtained directly from the index without accessing table data. Table lookup once EXPLAIN SELECT * FROM `user` WHERE biz_id = 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 100 null (ChatGPT) This is a simple SELECT query with no subqueries or joins. The query references a table named user. The query uses an index named idx_user_biz_id. The index length is 8. The query uses a constant value for reference. The query scanned 498380 rows and filtered 100 rows based on the WHERE condition. The query has no other special conditions. Table lookup once (more table lookup data) EXPLAIN SELECT * FROM `user` WHERE biz_id = 2 and message = ''; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null ref idx_user_biz_id idx_user_biz_id 8 const 498380 10 Using where (ChatGPT) The given query is a simple SELECT query involving a table named \"user\". The query uses an index named \"idx_user_biz_id\" and uses a WHERE clause. The query uses regular secondary index for eq","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:3:2","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"IN or EXISTS? DROP TABLE IF EXISTS task; CREATE TABLE task ( id bigint(20) NOT NULL COMMENT 'User ID', biz_id bigint(20) NOT NULL COMMENT 'Business ID', message text COMMENT 'Business information', created timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL, updated timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (id) ) ENGINE = InnoDB COMMENT 'Order' CHARSET = utf8mb4; CREATE INDEX idx_task_biz_id ON task(biz_id); DELIMITER $$ CREATE PROCEDURE insertTaskData(IN start_id int, IN end_id int, IN bizId int) BEGIN DECLARE i int DEFAULT start_id; WHILE i \u003c= end_id DO INSERT INTO task (id, biz_id, message) VALUES (i, bizId, SUBSTRING(MD5(RAND()), 1, 8)); SET i = i + 1; END WHILE; END $$ DELIMITER ; -- Insert first business id 1 data CALL insertTaskData(1, 1, 1); -- Insert one million other data CALL insertTaskData(2, 10, 2); -- Insert second business id 1 data CALL insertTaskData(11, 11, 1); CALL insertTaskData(12, 12, 3); CALL insertTaskData(13, 13, 4); CALL insertTaskData(14, 14, 5); CALL insertTaskData(15, 15, 6); CALL insertTaskData(16, 16, 7); ","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:4:0","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["MySQL"],"content":"Analysis Principle of small table driving large table EXPLAIN SELECT COUNT(*) FROM task LEFT JOIN user u ON task.biz_id = u.biz_id; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index null idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE u null ref idx_user_biz_id_msg idx_user_biz_id_msg 8 test.task.biz_id 996761 100 Using index SELECT COUNT(*) FROM task LEFT JOIN user u ON task.biz_id = u.biz_id; -- [2023-08-30 10:04:52] 1 row retrieved starting from 1 in 2 s 784 ms (execution: 2 s 766 ms, fetching: 18 ms) EXPLAIN SELECT COUNT(*) FROM user u LEFT JOIN task t ON u.biz_id = t.biz_id; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u null index null idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE t null ref idx_task_biz_id idx_task_biz_id 8 test.u.biz_id 5 100 Using index SELECT COUNT(*) FROM user u LEFT JOIN task t ON u.biz_id = t.biz_id; -- [2023-08-30 10:04:57] 1 row retrieved starting from 1 in 4 s 527 ms (execution: 4 s 504 ms, fetching: 23 ms) EXPLAIN SELECT COUNT(*) FROM task WHERE biz_id IN ( SELECT biz_id FROM user); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.task.biz_id 1 100 null 2 MATERIALIZED user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index SELECT COUNT(*) FROM task WHERE biz_id IN (SELECT biz_id FROM user); -- [2023-08-30 10:04:35] 1 row retrieved starting from 1 in 443 ms (execution: 399 ms, fetching: 44 ms) EXPLAIN SELECT COUNT(*) FROM user WHERE biz_id IN (SELECT biz_id FROM task); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.user.biz_id 1 100 null 2 MATERIALIZED task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index SELECT COUNT(*) FROM user WHERE biz_id IN (SELECT biz_id FROM task); -- [2023-08-30 10:04:58] 1 row retrieved starting from 1 in 545 ms (execution: 507 ms, fetching: 38 ms) EXPLAIN SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE task null index idx_task_biz_id idx_task_biz_id 8 null 16 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.task.biz_id 1 100 null 2 MATERIALIZED user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index EXPLAIN format=tree SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); -- -\u003e Aggregate: count(0) (cost=3189638.65 rows=1) -- -\u003e Nested loop inner join (cost=1594821.05 rows=15948176) -- -\u003e Index scan on task using idx_task_biz_id (cost=1.85 rows=16) -- -\u003e Single-row index lookup on \u003csubquery2\u003e using \u003cauto_distinct_key\u003e (biz_id=task.biz_id) -- -\u003e Materialize with deduplication (cost=202826.45..202826.45 rows=996761) -- -\u003e Index scan on user using idx_user_biz_id_msg (cost=103150.35 rows=996761) SELECT COUNT(*) FROM task WHERE EXISTS(SELECT biz_id FROM user WHERE user.biz_id = task.biz_id); -- [2023-08-30 10:04:59] 1 row retrieved starting from 1 in 459 ms (execution: 421 ms, fetching: 38 ms) EXPLAIN SELECT COUNT(*) FROM user WHERE EXISTS(SELECT biz_id FROM task WHERE user.biz_id = task.biz_id); id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user null index idx_user_biz_id_msg idx_user_biz_id_msg 43 null 996761 100 Using index 1 SIMPLE \u003csubquery2\u003e null eq_ref \u003cauto_distinct_key\u003e \u003cauto_distinct_key\u003e 8 test.user.biz_id 1 100 null 2 MATERIALIZED task null index idx_task_biz_id i","date":"2025-09-21","objectID":"/en/2025/09/mysql-1.-mysql-fast-query-insights/:4:1","tags":["MySQL","index","database","optimization"],"title":"[MySQL] 1. MySQL Fast Query Insights","uri":"/en/2025/09/mysql-1.-mysql-fast-query-insights/"},{"categories":["network"],"content":"Background (Sensitive Data Masked) Due to various requirements, we need to upload data to overseas OSS for storage. So we developed a proxy service to maintain data and perform encryption operations. During this process, we discovered that data upload and download were very slow. After a series of investigations, we finally located the root cause of the problem and provided a solution. We’re now sharing the troubleshooting process. Of course, one prerequisite is internal network connectivity through dedicated line network access to achieve theoretical physical limits. Using complex and lengthy public networks is neither suitable for file security nor for large file long-term transmission. ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:1:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Service-Level Issues Initially, we suspected it was due to data writing to disk being too slow. Because uploads must be written to disk to prevent files from being too large. Downloads use direct streaming transmission, which is very reasonable. The only improvement would be to perform streaming encryption and transmission for uploads, but the current issue is not significant. ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:2:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Phenomenon Using our written script to upload 1M of encrypted data took nearly 2 seconds import requests requests.post(f\"{url}/upload/files\", files={ \"data\": ('', upload_data, \"application/json\"), \"file\": transfer_data }) $ python oss.py --file_input=./1M.data --region=us --model=3 --range=5 encrypted_upload upload ./1M.data, encrypt cost 4.714599609375, upload cost 1788.95849609375 upload ./1M.data, encrypt cost 10.140625, upload cost 1945.90087890625 upload ./1M.data, encrypt cost 9.924560546875, upload cost 1756.984130859375 upload ./1M.data, encrypt cost 8.694580078125, upload cost 1930.31201171875 upload ./1M.data, encrypt cost 8.279296875, upload cost 1739.38623046875 ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:3:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Packet Capture After communicating with operations, they suspected it was a network issue and performed packet capture to investigate. ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:4:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Packet Capture Demonstration Ping Packets $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:21:19.255718 IP public2.alidns.com.domain \u003e domain1.36590: 43190 1/0/1 A x.x.x.x1 (88) 16:21:19.256404 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 1, length 64 16:21:19.456754 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 1, length 64 16:21:20.257688 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 2, length 64 16:21:20.458076 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 2, length 64 16:21:21.259088 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 3, length 64 16:21:21.459506 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 3, length 64 16:21:22.260538 IP domain1 \u003e x.x.x.x1: ICMP echo request, id 32590, seq 4, length 64 16:21:22.460976 IP x.x.x.x1 \u003e domain1: ICMP echo reply, id 32590, seq 4, length 64 $ ping domain1 PING domain1 (x.x.x.x1) 56(84) bytes of data. 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=1 ttl=58 time=200 ms 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=2 ttl=58 time=200 ms 64 bytes from x.x.x.x1 (x.x.x.x1): icmp_seq=3 ttl=58 time=200 ms ^C --- domain1 ping statistics --- 4 packets transmitted, 3 received, 25% packet loss, time 3004ms rtt min/avg/max/mdev = 200.395/200.419/200.456/0.517 ms Three-Way Handshake 16:54:06.286416 IP domain1.33666 \u003e x.x.x.x1.http: Flags [S], seq 2682796272, win 64240, options [mss 1460,sackOK,TS val 2595135963 ecr 0,nop,wscale 7], length 0 16:54:06.486797 IP x.x.x.x1.http \u003e domain1.33666: Flags [S.], seq 2198055866, ack 2682796273, win 62643, options [mss 1460,sackOK,TS val 2062390218 ecr 2595135963,nop,wscale 7], length 0 16:54:06.486840 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 2595136163 ecr 2062390218], length 0 Four-Way Handshake 16:54:28.356723 IP domain1.54028 \u003e x.x.x.x1.http: Flags [F.], seq 1746, ack 215, win 501, options [nop,nop,TS val 2595158034 ecr 2062412087], length 0 16:54:28.557169 IP x.x.x.x1.http \u003e domain1.54028: Flags [F.], seq 215, ack 1747, win 477, options [nop,nop,TS val 2062412289 ecr 2595158034], length 0 16:54:28.557222 IP domain1.54028 \u003e x.x.x.x1.http: Flags [.], ack 216, win 501, options [nop,nop,TS val 2595158234 ecr 2062412289], length 0 tcpdump Flags Tcpdump flags are flags that indicate TCP connection status or actions. They are usually represented in square brackets in tcpdump output. There are various flags in tcpdump output, and the output may also contain combinations of multiple TCP flags. Some common flags include: S (SYN): This flag is used to establish a connection between two hosts. It is set in the first packet of the three-way handshake. . (No flag): This means no flag is set in the packet. It is usually used for data transmission or acknowledgment packets. P (PUSH): This flag is used to indicate that the sender wants to send data as soon as possible without waiting for the buffer to fill. F (FIN): This flag is used to terminate the connection between two hosts. It is set in the last packet of the four-way handshake. R (RST): This flag is used to reset connections that are in an invalid state or encounter errors. It is also used to reject unwanted connection attempts. W (ECN CWR): This flag is used to indicate that the sender has reduced its congestion window size according to the network’s Explicit Congestion Notification (ECN). E (ECN-Echo): This flag is used to indicate that the receiver has received a packet with the ECN bit, meaning there is congestion in the network. For example, a packet with flags [S.] means it is a SYN packet, the first step in establishing a TCP connection. A packet with flags [P.] means it is a PUSH packet containing data that the sender wants to transmit quickly. A packet with flags [F.] means it is a FIN packet, the last step in closing a TCP connection. Why tcpdump Four-Way Handshake Only Ha","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:4:1","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Actual Data $ python oss.py --file_input=./1K.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1K.data, encrypt cost 1.530029296875, upload cost 408.5546875 $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:54:06.286416 IP domain1.33666 \u003e x.x.x.x1.http: Flags [S], seq 2682796272, win 64240, options [mss 1460,sackOK,TS val 2595135963 ecr 0,nop,wscale 7], length 0 16:54:06.486797 IP x.x.x.x1.http \u003e domain1.33666: Flags [S.], seq 2198055866, ack 2682796273, win 62643, options [mss 1460,sackOK,TS val 2062390218 ecr 2595135963,nop,wscale 7], length 0 16:54:06.486840 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 2595136163 ecr 2062390218], length 0 16:54:06.486930 IP domain1.33666 \u003e x.x.x.x1.http: Flags [P.], seq 1:292, ack 1, win 502, options [nop,nop,TS val 2595136164 ecr 2062390218], length 291: HTTP: POST /upload/files HTTP/1.1 16:54:06.486960 IP domain1.33666 \u003e x.x.x.x1.http: Flags [P.], seq 292:1746, ack 1, win 502, options [nop,nop,TS val 2595136164 ecr 2062390218], length 1454: HTTP 16:54:06.687234 IP x.x.x.x1.http \u003e domain1.33666: Flags [.], ack 292, win 488, options [nop,nop,TS val 2062390419 ecr 2595136164], length 0 16:54:06.687279 IP x.x.x.x1.http \u003e domain1.33666: Flags [.], ack 1746, win 477, options [nop,nop,TS val 2062390419 ecr 2595136164], length 0 16:54:06.690277 IP x.x.x.x1.http \u003e domain1.33666: Flags [P.], seq 1:215, ack 1746, win 477, options [nop,nop,TS val 2062390422 ecr 2595136164], length 214: HTTP: HTTP/1.1 200 OK 16:54:06.690314 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 215, win 501, options [nop,nop,TS val 2595136367 ecr 2062390422], length 0 16:54:06.692023 IP domain1.33666 \u003e x.x.x.x1.http: Flags [F.], seq 1746, ack 215, win 501, options [nop,nop,TS val 2595136369 ecr 2062390422], length 0 16:54:06.892401 IP x.x.x.x1.http \u003e domain1.33666: Flags [F.], seq 215, ack 1747, win 477, options [nop,nop,TS val 2062390624 ecr 2595136369], length 0 16:54:06.892448 IP domain1.33666 \u003e x.x.x.x1.http: Flags [.], ack 216, win 501, options [nop,nop,TS val 2595136569 ecr 2062390624], length 0 Actually uploading 1M of data for analysis, simplified here. Since all time jumps occur in packets returned from the server side, the problem is now very clear. Due to the actual physical distance between Shenzhen and the US East Coast, the 200ms round trip has reached its limit. So it’s actually reasonable. ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:4:2","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Soul-Searching Question At this point, a soul-searching question arises: why was it faster when using the public network before? After communicating with colleagues from sister departments and simulating their code, we tested using AWS SDK import boto3 from boto3.s3.transfer import TransferConfig def download(): s3_client = client(access_key, access_secret, host) GB = 1024 ** 3 config = TransferConfig(multipart_threshold=2 * GB, max_concurrency=10, use_threads=True) s3_client.download_file(Bucket=\"bucket\", Key=\"name-100.jpg\", Filename=\"name-100.jpg\", Config=config) if __name__ == '__main__': download() # ... download() Results 2.359457492828369 2.34989070892334 2.4120875199635825 2.3953704833984375 2.382766008377075 2.3793430725733438 2.3801622731345042 2.374732166528702 2.393121269014147 2.387941288948059 2.3849898034876045 2.3809364239374795 2.382789208338811 2.379830701010568 2.3768802642822267 2.3746740520000458 2.374574675279505 2.3716080056296454 As you can see, using aws-sdk was actually slower. This was even stranger - why couldn’t we reproduce the results from the sister department? First, the max_concurrency=10 in the config gave me some confusion. The client definitely supports thread pools, but it didn’t seem to be used because a new client was initialized each time. Optimization So we tested with client reuse def download(): s3_client = client(access_key, access_secret, host) GB = 1024 ** 3 config = TransferConfig(multipart_threshold=2 * GB, max_concurrency=10, use_threads=True) now = time.time() count = 0 while count \u003c 20: s3_client.download_file(Bucket=\"bucket\", Key=\"name-100.jpg\", Filename=\"name-100.jpg\", Config=config) count += 1 print((time.time() - now) / count) Results download 2.465491533279419 1.5669758319854736 1.2221351464589436 1.0315884947776794 0.9212518692016601 0.8434466520945231 0.7922392232077462 0.7573718726634979 0.7251839107937283 0.6981703996658325 0.6772929538380016 0.6588474710782369 0.6429501130030706 0.6297299180712018 0.6190152009328206 0.6086597740650177 0.5995960656334373 0.5917102760738797 0.585765048077232 0.5791293740272522 Based on the results, we found that it was just assumed that the default framework reuses all connections. After modifying to reuse connections, the effect was excellent. And it’s approaching the theoretical limit of 200ms (infinite bandwidth, one interaction) ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:5:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Initial Conclusion First, regarding how to accelerate transmission, we already have the most direct conclusion. Reuse connections, and below is a comparison for 1M files ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:6:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Comparison Between Connection Reuse and Non-Reuse $ python oss.py --file_input=./1M.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1M.data, encrypt cost 4.924560546875, upload cost 1919.100341796875 http://domain1 upload ./1M.data, encrypt cost 4.593017578125, upload cost 1715.593994140625 http://domain1 upload ./1M.data, encrypt cost 10.076171875, upload cost 2253.67333984375 http://domain1 upload ./1M.data, encrypt cost 12.694091796875, upload cost 1714.197021484375 http://domain1 upload ./1M.data, encrypt cost 12.3076171875, upload cost 2152.773193359375 ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:6:1","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Continued Questioning Originally, everything should have ended here. Reuse connections and efficiency improves dramatically. It’s unrelated to the server, only related to the client. The client was handed over to the sister department for modification. Until the sister department reported a problem - why does the server disconnect connections? Through online research and checking source code default values, I found: The server supports 8192 connections by default Default client connection timeout is 30 minutes or never Obviously, neither matches the facts Then the sister department started monitoring connection status and found that connections quickly entered CLOSE_WAIT state. Obviously, the server received FIN packets. To explain this situation, through packet capture, we actually proved that the sister department’s code issue caused FIN packets to be sent. Since I highly value using packet capture and monitoring connection status to find problems, I planned to reproduce the previous situation to introduce packet capture tools and methods. But during the reproduction process, I started soul-searching again ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:7:0","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"Why Is the Efficiency Optimization So Significant? According to naive thinking, reusing connections should save the three-way handshake and four-way handshake, which according to the above understanding, should only optimize about 400ms. However, the reality is not like this - it’s second-level optimization. Why is this? # Naive thinking of non-reused connection packet transmission 1. [S][P][P][P][P][F] 2. [S][P][P][P][P][F] 3. [S][P][P][P][P][F] # Naive thinking of reused connection packet transmission 1. [S][P][P][P][P] 2. [P][P][P][P] 3. [P][P][P][P][F] $ python oss-muti.py --file_input=./1M.data --file_output=./download-1M.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./1M.data, encrypt cost 5.02880859375, upload cost 2589.014892578125 http://domain1 upload ./1M.data, encrypt cost 10.720947265625, upload cost 562.706787109375 http://domain1 upload ./1M.data, encrypt cost 11.202392578125, upload cost 370.651611328125 http://domain1 upload ./1M.data, encrypt cost 10.948486328125, upload cost 372.409423828125 http://domain1 upload ./1M.data, encrypt cost 11.99560546875, upload cost 371.28759765625 So we captured packets again for deeper investigation ","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:7:1","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["network"],"content":"100K Data Comparison (Return packets omitted) $ python oss.py --file_input=./100K.data --file_output=./100K.data --region=us --model=3 --range=5 encrypted_upload http://domain1 upload ./100K.data, encrypt cost 1.81884765625, upload cost 1017.35791015625 http://domain1 upload ./100K.data, encrypt cost 1.159912109375, upload cost 1021.509521484375 http://domain1 upload ./100K.data, encrypt cost 1.11669921875, upload cost 1016.612548828125 http://domain1 upload ./100K.data, encrypt cost 1.128662109375, upload cost 1016.171875 http://domain1 upload ./100K.data, encrypt cost 0.9912109375, upload cost 1016.228759765625 $ sudo tcpdump -i bond0 | grep x.x.x.x1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes 17:16:03.069540 IP domain1.53580 \u003e x.x.x.x1.http: Flags [S], seq 4211566581, win 64240, options [mss 1460,sackOK,TS val 2596452757 ecr 0,nop,wscale 7], length 0 17:16:03.270682 IP x.x.x.x1.http \u003e domain1.53580: Flags [S.], seq 1741768869, ack 4211566582, win 62643, options [mss 1460,sackOK,TS val 2063707002 ecr 2596452757,nop,wscale 7], length 0 17:16:03.270850 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 1:294, ack 1, win 502, options [nop,nop,TS val 2596452958 ecr 2063707002], length 293: HTTP: POST /upload/files HTTP/1.1 17:16:03.680467 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 72694:74142, ack 1, win 502, options [nop,nop,TS val 2596453367 ecr 2063707405], length 1448: HTTP 17:16:03.874400 IP domain1.53580 \u003e x.x.x.x1.http: Flags [P.], seq 101758:103124, ack 1, win 502, options [nop,nop,TS val 2596453561 ecr 2063707606], length 1366: HTTP 17:16:04.082005 IP x.x.x.x1.http \u003e domain1.53580: Flags [P.], seq 1:215, ack 103124, win 442, options [nop,nop,TS val 2063707813 ecr 2596453561], length 214: HTTP: HTTP/1.1 200 OK 17:16:04.083769 IP domain1.53580 \u003e x.x.x.x1.http: Flags [F.], seq 103124, ack 215, win 501, options [nop,nop,TS val 2596453771 ecr 2063707813], length 0 17:16:04.090059 IP domain1.44338 \u003e x.x.x.x1.http: Flags [S], seq 3876376673, win 64240, options [mss 1460,sackOK,TS val 2596453777 ecr 0,nop,wscale 7], length 0 17:16:04.284937 IP x.x.x.x1.http \u003e domain1.53580: Flags [F.], seq 215, ack 103125, win 442, options [nop,nop,TS val 2063708016 ecr 2596453771], length 0 17:16:04.291110 IP x.x.x.x1.http \u003e domain1.44338: Flags [S.], seq 27078140, ack 3876376674, win 62643, options [mss 1460,sackOK,TS val 2063708023 ecr 2596453777,nop,wscale 7], length 0 17:16:04.291270 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 1:294, ack 1, win 502, options [nop,nop,TS val 2596453978 ecr 2063708023], length 293: HTTP: POST /upload/files HTTP/1.1 17:16:04.693394 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 42286:43734, ack 1, win 502, options [nop,nop,TS val 2596454380 ecr 2063708425], length 1448: HTTP 17:16:04.720945 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 72694:74142, ack 1, win 502, options [nop,nop,TS val 2596454408 ecr 2063708425], length 1448: HTTP 17:16:04.894505 IP domain1.44338 \u003e x.x.x.x1.http: Flags [P.], seq 101838:103124, ack 1, win 502, options [nop,nop,TS val 2596454582 ecr 2063708626], length 1286: HTTP 17:16:05.105003 IP x.x.x.x1.http \u003e domain1.44338: Flags [P.], seq 1:215, ack 103124, win 442, options [nop,nop,TS val 2063708837 ecr 2596454582], length 214: HTTP: HTTP/1.1 200 OK 17:16:05.106641 IP domain1.44338 \u003e x.x.x.x1.http: Flags [F.], seq 103124, ack 215, win 501, options [nop,nop,TS val 2596454794 ecr 2063708837], length 0 17:16:05.112610 IP domain1.44340 \u003e x.x.x.x1.http: Flags [S], seq 1962726172, win 64240, options [mss 1460,sackOK,TS val 2596454800 ecr 0,nop,wscale 7], length 0 17:16:05.307713 IP x.x.x.x1.http \u003e domain1.44338: Flags [F.], seq 215, ack 103125, win 442, options [nop,nop,TS val 2063709039 ecr 2596454794], length 0 17:16:05.313623 IP x.x.x.x1.http \u003e domain1.44340: Flags [S.], seq 2582074627, ack 1962726173, win 62643, options [mss 1460,sackOK,TS val 2063709045 ecr 2596454","date":"2025-09-21","objectID":"/en/2025/09/network-1.-file-transfer-optimization/:7:2","tags":["network","tcpdump"],"title":"[Network] 1. File Transfer Optimization Sharing","uri":"/en/2025/09/network-1.-file-transfer-optimization/"},{"categories":["github","deploy"],"content":"intro GitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website. ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:1:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"pre-work ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:2:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"create a repository Create a repository named your_github_username.github.io, where your_github_username is your GitHub username. For example, if your GitHub username is octocat, the repository name should be octocat.github.io. ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:2:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"hugo install Download the latest version of Hugo from the official Hugo releases page ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:2:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"create a blog site ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:3:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"hugo init site # create directory mkdir your_github_username.github.io # cd to directory cd your_github_username.github.io # init site hugo new site . # git init, make sure it's a git repository git init ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:3:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"add a theme # add a theme, here we use LoveIt theme. git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt # now the git is main branch which is not stable, we need to checkout to the latest stable version. cd themes/LoveIt git checkout v0.3.0 cd ../.. # now, there should be a .gitmodules file in your directory. if not, you need to run `git init` first. # copy the exampleSite config file to the root directory cp themes/LoveIt/exampleSite/hugo.toml . ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:3:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"modify the config file modify the config file hugo.toml bashURL baseURL = \"https://gooddayday.github.io\" themes directory # themes directory # 主题目录 themesDir = \"./themes\" website title # website title # 网站标题 title = \"GoodyHao's Blog\" website images # website images for Open Graph and Twitter Cards # 网站图片, 用于 Open Graph 和 Twitter Cards images = [\"/logo.jpg\"] website icon put icon file in the static directory gitRepo modify the gitRepo to your public git repo url # public git repo url only then enableGitInfo is true # 公共 git 仓库路径，仅在 enableGitInfo 设为 true 时有效 gitRepo = \"https://github.com/GOODDAYDAY/GOODDAYDAY.github.io\" ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:3:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"github deploy ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:4:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"create a workflow file create a file .github/workflows/deploy.yaml, and add the following content: name: Deploy Hugo to GitHub Pages on: push: # Trigger condition: push code to master branch branches: - master jobs: build-and-deploy: runs-on: ubuntu-latest # Use Ubuntu environment steps: # 1. Check out repository code (recursively pull theme submodule) - uses: actions/checkout@v4 with: submodules: true # 2. Install Hugo (use extended version, supports SASS) - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' # Or specify version (e.g., '0.147.2') extended: true # 3. Cache dependencies (speed up subsequent builds) - uses: actions/cache@v3 with: path: | resources/_gen public key: ${{ runner.os }}-hugo-${{ hashFiles('**/go.sum') }} restore-keys: | ${{ runner.os }}-hugo- # 4. Build Hugo site (enable compression) - name: Build Hugo site run: hugo --minify # 5. Deploy to GitHub Pages (automatically push public directory to gh-pages branch) - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v4 with: github_token: ${{ secrets.GITHUB_TOKEN }} # GitHub automatically provided Token (no manual creation needed) publish_dir: ./public # Point to Hugo generated static files directory force_orphan: true # Force create new commit (avoid branch history confusion) github repository settings -\u003e pages -\u003e source -\u003e select gh-pages branch and / (root) folder -\u003e save if gh-pages branch not exist, need to push code to github first need to set token generate new token with repo and workflow permissions add token to github secrets with name TOKEN_GITHUB ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:4:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"push code to github # add all files git add . # commit git commit -m \"first commit\" # push to github git push -u origin master ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:4:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"check the workflow check the workflow in github actions ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:4:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"access the blog site access the blog site with https://your_github_username.github.io, for example, https://gooddayday.github.io ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:5:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"others ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:0","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"add new post # create a new post hugo new posts/first-post.md # edit the post vim content/posts/first-post.md # after edit, need to set the post as published # set draft = false # then commit and push to github git add . git commit -m \"add first post\" git push if you want to add images to the post, need to put the images in the static directory, for example, static/images/first-post-image.png, then you can access the image with /images/first-post-image.png in the post. ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:1","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"gitignore create a .gitignore file in the root directory, and add the following content: public/* we don’t need to push the public directory to github, because it will be generated by hugo in the workflow. ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:2","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"tag \u0026 category generate tag and category will be generated automatically by hugo, no need to create them manually. But if no index.html shown below, you need to add templates. just copy the themes/LoveIt/layouts/taxonomy/list.html to the different path and rename it to layouts/taxonomy/tag.html and layouts/taxonomy/category.html and then, run hugo server to check if the result has index.html like the picture below: ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:3","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"setup comment system with giscus By default, the LoveIt theme uses Valine comment system, but we recommend using Giscus which is based on GitHub Discussions. Giscus is free, stable, and stores comment data in your own GitHub repository. disable other comment systems First, make sure other comment systems are disabled in hugo.toml: # Disable Valine [params.page.comment.valine] enable = false # Disable Disqus [params.page.comment.disqus] enable = false # Disable Gitalk [params.page.comment.gitalk] enable = false enable GitHub Discussions Go to your GitHub repository settings: https://github.com/your-username/your-username.github.io/settings Navigate to Features section Check the Discussions checkbox to enable it configure giscus Visit giscus.app to generate configuration Fill in the repository field: your-username/your-username.github.io Click The giscus app is installed, otherwise visitors will not be able to comment and react. and install giscus to your repository. update hugo.toml Add the giscus configuration to your hugo.toml: [params.page.comment.giscus] enable = true repo = \"your-username/your-username.github.io\" repoId = \"your-repo-id-from-giscus\" category = \"General\" # or your chosen category categoryId = \"your-category-id-from-giscus\" lang = \"\" # empty for auto-detection mapping = \"pathname\" reactionsEnabled = \"1\" emitMetadata = \"0\" inputPosition = \"bottom\" lazyLoading = false lightTheme = \"light\" darkTheme = \"dark\" data like repoId and categoryId can be found in the giscus configuration you generated earlier. ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:4","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"language switch settings With the AI development, it is easy to translate the blog content to different languages. Here we use English and Chinese as an example. Firstly, we need to create two directories in the content directory: en and zh, then put the corresponding language content in the respective directory. The name of different language file should be the same, for example, content/en/posts/1.deploy-github-blog-site.md and content/zh/posts/1.deploy-github-blog-site.md if not, the hugo will treat them as different posts, and show them in the different language list. Then, we need to modify the hugo.toml file to enable multi-language support: # determines default content language [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] # 设置默认的语言 [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] defaultContentLanguage = \"en\" # whether to include default language in URL path # 是否在URL路径中包含默认语言 (设为true让所有语言都有前缀，设为false让默认语言无前缀) defaultContentLanguageInSubdir = true .... # 是否包括中日韩文字 hasCJKLanguage = false hasCJKLanguage = true ... # Multilingual # 多语言 [languages] [languages.en] weight = 1 languageCode = \"en\" languageName = \"English\" hasCJKLanguage = false copyright = \"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\" contentDir = \"content\" contentDir = \"content/en\" ... [languages.zh-cn] weight = 2 languageCode = \"zh-CN\" languageName = \"简体中文\" hasCJKLanguage = true copyright = \"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\" contentDir = \"content/zh\" ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:5","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"},{"categories":["github","deploy"],"content":"language switch display customization By default, the language switch button is displayed in the top right corner of the website. If you want to switch the language, you need to click twice and people may not notice there has another language. So, as to me, I want to display the language switch button in the header menu, so that people can easily find it and switch the language. To achieve this, we need to copy the themes/LoveIt/layouts/partials/header.html file to layouts/partials/header.html. Then, we need to modify the layouts/partials/header.html file to add the language switch button in the header menu. before {{- if hugo.IsMultilingual -}} \u003ca href=\"javascript:void(0);\" class=\"menu-item language\" title=\"{{ T \"selectLanguage\" }}\"\u003e \u003ci class=\"fa fa-globe fa-fw\" aria-hidden=\"true\"\u003e\u003c/i\u003e \u003cselect class=\"language-select\" id=\"language-select-desktop\" onchange=\"location = this.value;\"\u003e {{- if eq .Kind \"404\" -}} {{- /* https://github.com/dillonzq/LoveIt/issues/378 */ -}} {{- range .Sites -}} {{- $link := printf \"%v/404.html\" .LanguagePrefix -}} \u003coption value=\"{{ $link }}\"{{ if eq . $.Site }} selected{{ end }}\u003e {{- .Language.LanguageName -}} \u003c/option\u003e {{- end -}} {{- else -}} {{- range .AllTranslations -}} \u003coption value=\"{{ .RelPermalink }}\"{{ if eq .Lang $.Lang }} selected{{ end }}\u003e {{- .Language.LanguageName -}} \u003c/option\u003e {{- end -}} {{- end -}} \u003c/select\u003e \u003c/a\u003e {{- end -}} after {{- /* 直接切换语言按钮 */ -}} {{- if hugo.IsMultilingual -}} {{- if eq .Kind \"404\" -}} {{- /* https://github.com/dillonzq/LoveIt/issues/378 */ -}} {{- range .Sites -}} {{- if ne . $.Site -}} \u003ca class=\"menu-item\" href=\"{{ printf \"%v/404.html\" .LanguagePrefix }}\" title=\"{{ .Language.LanguageName }}\"\u003e {{- if eq .Language.LanguageCode \"zh-CN\" -}} 中文 {{- else -}} English {{- end -}} \u003c/a\u003e {{- end -}} {{- end -}} {{- else -}} {{- range .AllTranslations -}} {{- if ne .Lang $.Lang -}} \u003ca class=\"menu-item\" href=\"{{ .RelPermalink }}\" title=\"{{ .Language.LanguageName }}\"\u003e {{- if eq .Language.LanguageCode \"zh-CN\" -}} 中文 {{- else -}} English {{- end -}} \u003c/a\u003e {{- end -}} {{- end -}} {{- end -}} {{- end -}} result display as below: ","date":"2025-09-10","objectID":"/en/2025/09/github-1.-deploy-github-blog-site/:6:6","tags":["github","deploy","hugo","blog","site"],"title":"[Github] 1. Deploy GitHub Blog Site","uri":"/en/2025/09/github-1.-deploy-github-blog-site/"}]